,ID,title,task,objective,projectObjective,domain,module,evaluation,description,objectiveDescription,formula,unit,id,domains,BENCHMARK_GROUP_ID,BENCHMARK_GROUP_NAME,BENCHMARK_GROUP_DESCRIPTION,BENCHMARK_ID,BENCHMARK_NAME,BENCHMARK_DESCRIPTION,BENCHMARK_FIELD_ID,BENCHMARK_FIELD_NAME,BENCHMARK_AGG,TEST_ID,TEST_NAME,TEST_DESCRIPTION,TEST_FIELD_ID,TEST_FIELD_NAME,TEST_AGG,SCENARIO_ID,SCENARIO_NAME,SCENARIO_DESCRIPTION,SCENARIO_FIELD_ID,SCENARIO_FIELD_NAME
0,KPI-AS-001,Ability to anticipate,Task 4.3,Human user experience,O3,Railway,['(non applicable)'],special evaluation setup,"“The ability to anticipate. Knowing what to expect or being able to anticipate developments further into the future, such as potential disruptions, novel demands or constraints, new opportunities, or changing operating conditions” (Hollnagel, 2015, p. 4). 
The human operator’s ability to anticipate further into the future can be measured by calculating the ratio of (proactively) prevented deviations to actual deviations. In addition, the extent to which the anticipatory sensemaking process of the human operator is supported by AI-based assistants can be measured using the Rigor-Metric for Sensemaking (Zelik et al., 2018) or similar. The instrument needs to be further developed and adapted to the AI context. ","This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,Lickert-Scale or similar ,1,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-AS-001,KPI-AS-001: Ability to anticipate (Railway),"“The ability to anticipate. Knowing what to expect or being able to anticipate developments further into the future, such as potential disruptions, novel demands or constraints, new opportunities, or changing operating conditions” (Hollnagel, 2015, p. 4). 
The human operator’s ability to anticipate further into the future can be measured by calculating the ratio of (proactively) prevented deviations to actual deviations. In addition, the extent to which the anticipatory sensemaking process of the human operator is supported by AI-based assistants can be measured using the Rigor-Metric for Sensemaking (Zelik et al., 2018) or similar. The instrument needs to be further developed and adapted to the AI context. ",2e30dc5b-d7d0-437b-aef3-2bd78939d712,Test score (SUM of scenario scores),SUM,e969a487-4ed8-4fc8-a417-cd4eee4a4d70,"Scenario 1 - “The ability to anticipate. Knowing what to expect or being able to anticipate developments further into the future, such as potential disruptions, novel demands or constraints, new opportunities, or changing operating conditions” (Hollnagel, 2015, p. 4). 
The human operator’s ability to anticipate further into the future can be measured by calculating the ratio of (proactively) prevented deviations to actual deviations. In addition, the extent to which the anticipatory sensemaking process of the human operator is supported by AI-based assistants can be measured using the Rigor-Metric for Sensemaking (Zelik et al., 2018) or similar. The instrument needs to be further developed and adapted to the AI context. ","This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",db639315-cb3e-427a-93d5-100aac247739,Scenario score (raw values)
1,KPI-AS-001,Ability to anticipate,Task 4.3,Human user experience,O3,Power Grid,['(non applicable)'],special evaluation setup,"“The ability to anticipate. Knowing what to expect or being able to anticipate developments further into the future, such as potential disruptions, novel demands or constraints, new opportunities, or changing operating conditions” (Hollnagel, 2015, p. 4). 
The human operator’s ability to anticipate further into the future can be measured by calculating the ratio of (proactively) prevented deviations to actual deviations. In addition, the extent to which the anticipatory sensemaking process of the human operator is supported by AI-based assistants can be measured using the Rigor-Metric for Sensemaking (Zelik et al., 2018) or similar. The instrument needs to be further developed and adapted to the AI context. ","This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,Lickert-Scale or similar ,1,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-AS-001,KPI-AS-001: Ability to anticipate (Power Grid),"“The ability to anticipate. Knowing what to expect or being able to anticipate developments further into the future, such as potential disruptions, novel demands or constraints, new opportunities, or changing operating conditions” (Hollnagel, 2015, p. 4). 
The human operator’s ability to anticipate further into the future can be measured by calculating the ratio of (proactively) prevented deviations to actual deviations. In addition, the extent to which the anticipatory sensemaking process of the human operator is supported by AI-based assistants can be measured using the Rigor-Metric for Sensemaking (Zelik et al., 2018) or similar. The instrument needs to be further developed and adapted to the AI context. ",2e30dc5b-d7d0-437b-aef3-2bd78939d712,Test score (SUM of scenario scores),SUM,190d03e9-e1db-4e65-bb9f-d171a7ce6937,"Scenario 1 - “The ability to anticipate. Knowing what to expect or being able to anticipate developments further into the future, such as potential disruptions, novel demands or constraints, new opportunities, or changing operating conditions” (Hollnagel, 2015, p. 4). 
The human operator’s ability to anticipate further into the future can be measured by calculating the ratio of (proactively) prevented deviations to actual deviations. In addition, the extent to which the anticipatory sensemaking process of the human operator is supported by AI-based assistants can be measured using the Rigor-Metric for Sensemaking (Zelik et al., 2018) or similar. The instrument needs to be further developed and adapted to the AI context. ","This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",b7278b09-5412-4c81-aa0f-bec960f5b2fd,Scenario score (raw values)
2,KPI-AS-001,Ability to anticipate,Task 4.3,Human user experience,O3,ATM,['(non applicable)'],special evaluation setup,"“The ability to anticipate. Knowing what to expect or being able to anticipate developments further into the future, such as potential disruptions, novel demands or constraints, new opportunities, or changing operating conditions” (Hollnagel, 2015, p. 4). 
The human operator’s ability to anticipate further into the future can be measured by calculating the ratio of (proactively) prevented deviations to actual deviations. In addition, the extent to which the anticipatory sensemaking process of the human operator is supported by AI-based assistants can be measured using the Rigor-Metric for Sensemaking (Zelik et al., 2018) or similar. The instrument needs to be further developed and adapted to the AI context. ","This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,Lickert-Scale or similar ,1,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-AS-001,KPI-AS-001: Ability to anticipate (ATM),"“The ability to anticipate. Knowing what to expect or being able to anticipate developments further into the future, such as potential disruptions, novel demands or constraints, new opportunities, or changing operating conditions” (Hollnagel, 2015, p. 4). 
The human operator’s ability to anticipate further into the future can be measured by calculating the ratio of (proactively) prevented deviations to actual deviations. In addition, the extent to which the anticipatory sensemaking process of the human operator is supported by AI-based assistants can be measured using the Rigor-Metric for Sensemaking (Zelik et al., 2018) or similar. The instrument needs to be further developed and adapted to the AI context. ",2e30dc5b-d7d0-437b-aef3-2bd78939d712,Test score (SUM of scenario scores),SUM,2f0e93d0-4a97-4194-b183-172ddb0cb5c0,"Scenario 1 - “The ability to anticipate. Knowing what to expect or being able to anticipate developments further into the future, such as potential disruptions, novel demands or constraints, new opportunities, or changing operating conditions” (Hollnagel, 2015, p. 4). 
The human operator’s ability to anticipate further into the future can be measured by calculating the ratio of (proactively) prevented deviations to actual deviations. In addition, the extent to which the anticipatory sensemaking process of the human operator is supported by AI-based assistants can be measured using the Rigor-Metric for Sensemaking (Zelik et al., 2018) or similar. The instrument needs to be further developed and adapted to the AI context. ","This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",39553d06-7771-4d33-92ff-442c93648106,Scenario score (raw values)
3,KPI-AS-002,Acceptance,Task 4.3,"AI acceptability, trust and trustworthiness",O2,Railway,['(non applicable)'],special evaluation setup,Acceptance of the system by a human user.,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ","Using a TAM model (technology acceptance model) or similar eg. the AI-Acceptance model (KIAM) (Scheuer, 2020) ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,2,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3767d0c2-279b-4c59-92d9-5734312b22fb,"AI acceptability, trust and trustworthiness","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ",f16b1221-ed01-4e64-8d5c-7d673fc20829,Benchmark score (SUM of test scores),SUM,KPI-AS-002,KPI-AS-002: Acceptance (Railway),Acceptance of the system by a human user.,aadc70e2-c3d5-4cdc-9b7d-0377e5978218,Test score (SUM of scenario scores),SUM,ca11cae8-58de-4240-b31f-a3acfa75a9af,Scenario 1 - Acceptance of the system by a human user.,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ",954f1f78-dd4d-4f84-9989-22272b6735f9,Scenario score (raw values)
4,KPI-AS-002,Acceptance,Task 4.3,"AI acceptability, trust and trustworthiness",O2,Power Grid,['(non applicable)'],special evaluation setup,Acceptance of the system by a human user.,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ","Using a TAM model (technology acceptance model) or similar eg. the AI-Acceptance model (KIAM) (Scheuer, 2020) ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,2,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3767d0c2-279b-4c59-92d9-5734312b22fb,"AI acceptability, trust and trustworthiness","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ",f16b1221-ed01-4e64-8d5c-7d673fc20829,Benchmark score (SUM of test scores),SUM,KPI-AS-002,KPI-AS-002: Acceptance (Power Grid),Acceptance of the system by a human user.,aadc70e2-c3d5-4cdc-9b7d-0377e5978218,Test score (SUM of scenario scores),SUM,277db4a0-640b-4752-8272-c09f4f25eb68,Scenario 1 - Acceptance of the system by a human user.,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ",a9298319-e48e-42ec-9497-9e48f83645b5,Scenario score (raw values)
5,KPI-AS-002,Acceptance,Task 4.3,"AI acceptability, trust and trustworthiness",O2,ATM,['(non applicable)'],special evaluation setup,Acceptance of the system by a human user.,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ","Using a TAM model (technology acceptance model) or similar eg. the AI-Acceptance model (KIAM) (Scheuer, 2020) ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,2,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3767d0c2-279b-4c59-92d9-5734312b22fb,"AI acceptability, trust and trustworthiness","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ",f16b1221-ed01-4e64-8d5c-7d673fc20829,Benchmark score (SUM of test scores),SUM,KPI-AS-002,KPI-AS-002: Acceptance (ATM),Acceptance of the system by a human user.,aadc70e2-c3d5-4cdc-9b7d-0377e5978218,Test score (SUM of scenario scores),SUM,6edd680a-81fa-45ed-bbab-4bd492303467,Scenario 1 - Acceptance of the system by a human user.,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ",a45338a1-b525-4324-a074-ca76c4dd3aae,Scenario score (raw values)
6,KPI-HS-003,Human intervention frequency,Task 4.3,Social-technical decision quality,O3,Railway,['Recommendation module'],semi-automated evaluation,"The Human Intervention Frequency KPI measures the proportion of instances in which a human operator intervenes in an automated decision-making process. While this KPI was initially developed for railway traffic control scenarios, it has been generalized to assess the reliability and autonomy of any AI-assisted system. It reflects the trust placed in the AI by quantifying how often human corrections are required during routine operations. ","This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective: 
- To evaluate the effectiveness of the AI system in operating autonomously. 
- To provide a performance benchmark for minimizing human interventions across various domains. 
- To identify areas where the AI may require additional refinement or support. ",(Number of human interventions / Total AI decision instances) x 100,Percentage (%) of AI decisions requiring human intervention. ,3,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3b1aff0c-5c94-445c-a106-c94ab1f780ca,Social-technical decision quality,"This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective: 
- To evaluate the effectiveness of the AI system in operating autonomously. 
- To provide a performance benchmark for minimizing human interventions across various domains. 
- To identify areas where the AI may require additional refinement or support. ",bbacedca-d4da-4c9b-afba-590bc1d9919e,Benchmark score (SUM of test scores),SUM,KPI-HS-003,KPI-HS-003: Human intervention frequency (Railway),"The Human Intervention Frequency KPI measures the proportion of instances in which a human operator intervenes in an automated decision-making process. While this KPI was initially developed for railway traffic control scenarios, it has been generalized to assess the reliability and autonomy of any AI-assisted system. It reflects the trust placed in the AI by quantifying how often human corrections are required during routine operations. ",76092ae5-1e53-4fe9-b071-9810f3e97c94,Test score (SUM of scenario scores),SUM,66addcbf-dddf-475d-b8c3-ff6111707229,"Scenario 1 - The Human Intervention Frequency KPI measures the proportion of instances in which a human operator intervenes in an automated decision-making process. While this KPI was initially developed for railway traffic control scenarios, it has been generalized to assess the reliability and autonomy of any AI-assisted system. It reflects the trust placed in the AI by quantifying how often human corrections are required during routine operations. ","This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective: 
- To evaluate the effectiveness of the AI system in operating autonomously. 
- To provide a performance benchmark for minimizing human interventions across various domains. 
- To identify areas where the AI may require additional refinement or support. ",e36ad968-3de6-40ee-81e8-500eeace4f4a,Scenario score (raw values)
7,KPI-HS-003,Human intervention frequency,Task 4.3,Social-technical decision quality,O3,Power Grid,['Recommendation module'],semi-automated evaluation,"The Human Intervention Frequency KPI measures the proportion of instances in which a human operator intervenes in an automated decision-making process. While this KPI was initially developed for railway traffic control scenarios, it has been generalized to assess the reliability and autonomy of any AI-assisted system. It reflects the trust placed in the AI by quantifying how often human corrections are required during routine operations. ","This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective: 
- To evaluate the effectiveness of the AI system in operating autonomously. 
- To provide a performance benchmark for minimizing human interventions across various domains. 
- To identify areas where the AI may require additional refinement or support. ",(Number of human interventions / Total AI decision instances) x 100,Percentage (%) of AI decisions requiring human intervention. ,3,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3b1aff0c-5c94-445c-a106-c94ab1f780ca,Social-technical decision quality,"This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective: 
- To evaluate the effectiveness of the AI system in operating autonomously. 
- To provide a performance benchmark for minimizing human interventions across various domains. 
- To identify areas where the AI may require additional refinement or support. ",bbacedca-d4da-4c9b-afba-590bc1d9919e,Benchmark score (SUM of test scores),SUM,KPI-HS-003,KPI-HS-003: Human intervention frequency (Power Grid),"The Human Intervention Frequency KPI measures the proportion of instances in which a human operator intervenes in an automated decision-making process. While this KPI was initially developed for railway traffic control scenarios, it has been generalized to assess the reliability and autonomy of any AI-assisted system. It reflects the trust placed in the AI by quantifying how often human corrections are required during routine operations. ",76092ae5-1e53-4fe9-b071-9810f3e97c94,Test score (SUM of scenario scores),SUM,e43ad806-85da-4271-9d61-1d7567124d13,"Scenario 1 - The Human Intervention Frequency KPI measures the proportion of instances in which a human operator intervenes in an automated decision-making process. While this KPI was initially developed for railway traffic control scenarios, it has been generalized to assess the reliability and autonomy of any AI-assisted system. It reflects the trust placed in the AI by quantifying how often human corrections are required during routine operations. ","This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective: 
- To evaluate the effectiveness of the AI system in operating autonomously. 
- To provide a performance benchmark for minimizing human interventions across various domains. 
- To identify areas where the AI may require additional refinement or support. ",72e114c8-f18c-46ab-bfea-f472e3179bf6,Scenario score (raw values)
8,KPI-HS-003,Human intervention frequency,Task 4.3,Social-technical decision quality,O3,ATM,['Recommendation module'],semi-automated evaluation,"The Human Intervention Frequency KPI measures the proportion of instances in which a human operator intervenes in an automated decision-making process. While this KPI was initially developed for railway traffic control scenarios, it has been generalized to assess the reliability and autonomy of any AI-assisted system. It reflects the trust placed in the AI by quantifying how often human corrections are required during routine operations. ","This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective: 
- To evaluate the effectiveness of the AI system in operating autonomously. 
- To provide a performance benchmark for minimizing human interventions across various domains. 
- To identify areas where the AI may require additional refinement or support. ",(Number of human interventions / Total AI decision instances) x 100,Percentage (%) of AI decisions requiring human intervention. ,3,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3b1aff0c-5c94-445c-a106-c94ab1f780ca,Social-technical decision quality,"This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective: 
- To evaluate the effectiveness of the AI system in operating autonomously. 
- To provide a performance benchmark for minimizing human interventions across various domains. 
- To identify areas where the AI may require additional refinement or support. ",bbacedca-d4da-4c9b-afba-590bc1d9919e,Benchmark score (SUM of test scores),SUM,KPI-HS-003,KPI-HS-003: Human intervention frequency (ATM),"The Human Intervention Frequency KPI measures the proportion of instances in which a human operator intervenes in an automated decision-making process. While this KPI was initially developed for railway traffic control scenarios, it has been generalized to assess the reliability and autonomy of any AI-assisted system. It reflects the trust placed in the AI by quantifying how often human corrections are required during routine operations. ",76092ae5-1e53-4fe9-b071-9810f3e97c94,Test score (SUM of scenario scores),SUM,4648d31a-ffd9-4b0e-91e9-8fa7d12cee40,"Scenario 1 - The Human Intervention Frequency KPI measures the proportion of instances in which a human operator intervenes in an automated decision-making process. While this KPI was initially developed for railway traffic control scenarios, it has been generalized to assess the reliability and autonomy of any AI-assisted system. It reflects the trust placed in the AI by quantifying how often human corrections are required during routine operations. ","This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective: 
- To evaluate the effectiveness of the AI system in operating autonomously. 
- To provide a performance benchmark for minimizing human interventions across various domains. 
- To identify areas where the AI may require additional refinement or support. ",23f381a2-e850-4f5f-819e-cb739e145d30,Scenario score (raw values)
9,KPI-AS-005,Agreement score,Task 4.3,"AI acceptability, trust and trustworthiness",O2,Railway,['(non applicable)'],special evaluation setup,This KPI represents human operators’ self-reported agreement with individual AI-generated solutions/decisions on a scale of 0–100. ,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Agreement score”.  ",Self-reported agreement with specific solutions on a scale of 0–100. ,Interval scale response ,4,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3767d0c2-279b-4c59-92d9-5734312b22fb,"AI acceptability, trust and trustworthiness","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Agreement score”.  ",f16b1221-ed01-4e64-8d5c-7d673fc20829,Benchmark score (SUM of test scores),SUM,KPI-AS-005,KPI-AS-005: Agreement score (Railway),This KPI represents human operators’ self-reported agreement with individual AI-generated solutions/decisions on a scale of 0–100. ,5812a1c2-c00b-4958-ae37-5d4f541b8131,Test score (SUM of scenario scores),SUM,feb9b668-32ed-420d-aafa-4af4b5641596,Scenario 1 - This KPI represents human operators’ self-reported agreement with individual AI-generated solutions/decisions on a scale of 0–100. ,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Agreement score”.  ",c05f8d61-fd3b-4ffe-b2a2-308696e2e099,Scenario score (raw values)
10,KPI-AS-005,Agreement score,Task 4.3,"AI acceptability, trust and trustworthiness",O2,Power Grid,['(non applicable)'],special evaluation setup,This KPI represents human operators’ self-reported agreement with individual AI-generated solutions/decisions on a scale of 0–100. ,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Agreement score”.  ",Self-reported agreement with specific solutions on a scale of 0–100. ,Interval scale response ,4,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3767d0c2-279b-4c59-92d9-5734312b22fb,"AI acceptability, trust and trustworthiness","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Agreement score”.  ",f16b1221-ed01-4e64-8d5c-7d673fc20829,Benchmark score (SUM of test scores),SUM,KPI-AS-005,KPI-AS-005: Agreement score (Power Grid),This KPI represents human operators’ self-reported agreement with individual AI-generated solutions/decisions on a scale of 0–100. ,5812a1c2-c00b-4958-ae37-5d4f541b8131,Test score (SUM of scenario scores),SUM,e69d14f2-4318-48d4-98c2-d9e03c6d2535,Scenario 1 - This KPI represents human operators’ self-reported agreement with individual AI-generated solutions/decisions on a scale of 0–100. ,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Agreement score”.  ",129455a5-3786-4003-a170-397ac226370b,Scenario score (raw values)
11,KPI-AS-005,Agreement score,Task 4.3,"AI acceptability, trust and trustworthiness",O2,ATM,['(non applicable)'],special evaluation setup,This KPI represents human operators’ self-reported agreement with individual AI-generated solutions/decisions on a scale of 0–100. ,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Agreement score”.  ",Self-reported agreement with specific solutions on a scale of 0–100. ,Interval scale response ,4,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3767d0c2-279b-4c59-92d9-5734312b22fb,"AI acceptability, trust and trustworthiness","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Agreement score”.  ",f16b1221-ed01-4e64-8d5c-7d673fc20829,Benchmark score (SUM of test scores),SUM,KPI-AS-005,KPI-AS-005: Agreement score (ATM),This KPI represents human operators’ self-reported agreement with individual AI-generated solutions/decisions on a scale of 0–100. ,5812a1c2-c00b-4958-ae37-5d4f541b8131,Test score (SUM of scenario scores),SUM,e02b4f33-67d6-4501-9b35-7b338791eaea,Scenario 1 - This KPI represents human operators’ self-reported agreement with individual AI-generated solutions/decisions on a scale of 0–100. ,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Agreement score”.  ",1689a308-8afa-4c33-80c2-c071c703b6f9,Scenario score (raw values)
12,KPI-AS-006,AI co-learning capability,Task 4.3,AI-human learning curves,O3,Railway,['(non applicable)'],special evaluation setup,This KPI represents human operators’ self-reported assessment of the AI ability to adapt to the operators’ preferences measured with a questionnaire. ,"This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “AI co-learning capability”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale),5,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a5d464fd-6ac9-4b3a-adc6-f83e83f0adad,AI-human learning curves,"This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “AI co-learning capability”.  ",a82ebe2d-b507-49fb-a6ae-96281d88d938,Benchmark score (SUM of test scores),SUM,KPI-AS-006,KPI-AS-006: AI co-learning capability (Railway),This KPI represents human operators’ self-reported assessment of the AI ability to adapt to the operators’ preferences measured with a questionnaire. ,6e5d1284-7690-41ec-bcbd-d3e74c6421a5,Test score (SUM of scenario scores),SUM,cbed0ba2-7874-428a-8ce2-bf9398f79786,Scenario 1 - This KPI represents human operators’ self-reported assessment of the AI ability to adapt to the operators’ preferences measured with a questionnaire. ,"This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “AI co-learning capability”.  ",6fbda3a6-fffd-405e-a7b6-b1d691cb1142,Scenario score (raw values)
13,KPI-AS-006,AI co-learning capability,Task 4.3,AI-human learning curves,O3,Power Grid,['(non applicable)'],special evaluation setup,This KPI represents human operators’ self-reported assessment of the AI ability to adapt to the operators’ preferences measured with a questionnaire. ,"This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “AI co-learning capability”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale),5,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a5d464fd-6ac9-4b3a-adc6-f83e83f0adad,AI-human learning curves,"This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “AI co-learning capability”.  ",a82ebe2d-b507-49fb-a6ae-96281d88d938,Benchmark score (SUM of test scores),SUM,KPI-AS-006,KPI-AS-006: AI co-learning capability (Power Grid),This KPI represents human operators’ self-reported assessment of the AI ability to adapt to the operators’ preferences measured with a questionnaire. ,6e5d1284-7690-41ec-bcbd-d3e74c6421a5,Test score (SUM of scenario scores),SUM,4de55ee4-77b0-408c-802b-6f48b5b8ff09,Scenario 1 - This KPI represents human operators’ self-reported assessment of the AI ability to adapt to the operators’ preferences measured with a questionnaire. ,"This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “AI co-learning capability”.  ",e3661d48-c379-40bc-a6d3-e5f680d69c6c,Scenario score (raw values)
14,KPI-AS-006,AI co-learning capability,Task 4.3,AI-human learning curves,O3,ATM,['(non applicable)'],special evaluation setup,This KPI represents human operators’ self-reported assessment of the AI ability to adapt to the operators’ preferences measured with a questionnaire. ,"This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “AI co-learning capability”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale),5,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a5d464fd-6ac9-4b3a-adc6-f83e83f0adad,AI-human learning curves,"This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “AI co-learning capability”.  ",a82ebe2d-b507-49fb-a6ae-96281d88d938,Benchmark score (SUM of test scores),SUM,KPI-AS-006,KPI-AS-006: AI co-learning capability (ATM),This KPI represents human operators’ self-reported assessment of the AI ability to adapt to the operators’ preferences measured with a questionnaire. ,6e5d1284-7690-41ec-bcbd-d3e74c6421a5,Test score (SUM of scenario scores),SUM,ffebbca8-5e34-4400-8d55-14ab49aa65e6,Scenario 1 - This KPI represents human operators’ self-reported assessment of the AI ability to adapt to the operators’ preferences measured with a questionnaire. ,"This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “AI co-learning capability”.  ",bde9e4e2-0d13-4a21-a60a-a14a29ea4259,Scenario score (raw values)
15,KPI-AF-008,Assistant alert accuracy,Task 4.1,Effectiveness,O2,Power Grid,"['Human machine interaction module', 'Digital environment']",fully automated evaluation,"Assistant alert accuracy is based on the number of times the AI assistant agent is right about forecasted issues ahead of time. 
Even if forecasted issues concern all events that lead to a grid state out of acceptable limits (set by operation policy), use cases of the project focus on managing overloads only: this KPI therefore only focuses on alerts related to line overloads. 
The calculation of KPI relies on simulation of 2 parallel paths (starting from the moment the alert is raised): 
- Simulation of the “do nothing” path, to assess the truth values
- Application of remedial actions to the “do nothing” path, to assess solved cases 
To calculate the KPI, all interventions by an agent or operator are fixed to a specific plan since every alert is related to a specific plan (e.g. remedial actions). 
Note: line contingencies for which alerts can be raised are the lines that can be attacked in the environment (env.alertable_line_ids in grid2Op), so this should be properly configured beforehand. ","This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ","The formula to compute the KPI is the confusion matrix (see Calculation Methodology): 
- TP, True positive cases, forecast alerts were raised by the AI assistant, and overloads did occur on the transmission grid) 
- FP, False positive cases, forecast alerts were raised by the AI assistant, but no overload occurred on the transmission grid 
- TN, True negative cases, the AI assistant raised no forecast alert, and no overload occurred on the transmission grid 
- FN, False negative cases, the AI assistant raised no forecast alert, but overloads occurred on the transmission grid 
- Starting from True positive cases, TPS, the True positive cases solved, represent the alert effectively solved by the recommendations. 
The KPI can be computed per episode, across several episodes of one scenario, or even across scenarios. ",None (counting) ,6,['Power Grid'],0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,575728ac-628b-416e-a98e-34ef7858169a,Effectiveness,"This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",8c4641d6-bf1f-415f-b8cd-3c7145487741,Benchmark score (SUM of test scores),SUM,KPI-AF-008,KPI-AF-008: Assistant alert accuracy (Power Grid),"Assistant alert accuracy is based on the number of times the AI assistant agent is right about forecasted issues ahead of time. 
Even if forecasted issues concern all events that lead to a grid state out of acceptable limits (set by operation policy), use cases of the project focus on managing overloads only: this KPI therefore only focuses on alerts related to line overloads. 
The calculation of KPI relies on simulation of 2 parallel paths (starting from the moment the alert is raised): 
- Simulation of the “do nothing” path, to assess the truth values
- Application of remedial actions to the “do nothing” path, to assess solved cases 
To calculate the KPI, all interventions by an agent or operator are fixed to a specific plan since every alert is related to a specific plan (e.g. remedial actions). 
Note: line contingencies for which alerts can be raised are the lines that can be attacked in the environment (env.alertable_line_ids in grid2Op), so this should be properly configured beforehand. ",27850b8c-74da-4461-b3ee-521c78827698,Test score (SUM of scenario scores),SUM,8f98caba-3c03-4acd-b9ef-f6c3c2836a26,"Scenario 1 - Assistant alert accuracy is based on the number of times the AI assistant agent is right about forecasted issues ahead of time. 
Even if forecasted issues concern all events that lead to a grid state out of acceptable limits (set by operation policy), use cases of the project focus on managing overloads only: this KPI therefore only focuses on alerts related to line overloads. 
The calculation of KPI relies on simulation of 2 parallel paths (starting from the moment the alert is raised): 
- Simulation of the “do nothing” path, to assess the truth values
- Application of remedial actions to the “do nothing” path, to assess solved cases 
To calculate the KPI, all interventions by an agent or operator are fixed to a specific plan since every alert is related to a specific plan (e.g. remedial actions). 
Note: line contingencies for which alerts can be raised are the lines that can be attacked in the environment (env.alertable_line_ids in grid2Op), so this should be properly configured beforehand. ","This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",af05d980-fff6-4100-8054-5881df2fe6df,Scenario score (raw values)
16,KPI-AS-009,Assistant disturbance,Task 4.3,Human user experience,O3,Railway,['(non applicable)'],special evaluation setup,Assistant disturbance KPI aims to measure if the AI assistant's notifications are disturbing the human operator's activity. ,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ","For each notification, the score ranges in [0, 5] with:  
- 0 meaning that the notification was not considered disturbing at all by the human operator 
- 5 means that the human operator considered the notification as fully disturbing 
This KPI is still under analysis on how it will be implemented. If with a single manual questionnaire or with a pop-up in the dashboard. ",None (score) ,7,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-AS-009,KPI-AS-009: Assistant disturbance (Railway),Assistant disturbance KPI aims to measure if the AI assistant's notifications are disturbing the human operator's activity. ,889d0c98-394d-411e-8a8a-e5023634ec51,Test score (SUM of scenario scores),SUM,845b6dd2-f7f9-4991-8d2e-e5913f18d332,Scenario 1 - Assistant disturbance KPI aims to measure if the AI assistant's notifications are disturbing the human operator's activity. ,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",0c4457bd-8082-4c68-a9ce-8b07057c1ee6,Scenario score (raw values)
17,KPI-AS-009,Assistant disturbance,Task 4.3,Human user experience,O3,Power Grid,['(non applicable)'],special evaluation setup,Assistant disturbance KPI aims to measure if the AI assistant's notifications are disturbing the human operator's activity. ,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ","For each notification, the score ranges in [0, 5] with:  
- 0 meaning that the notification was not considered disturbing at all by the human operator 
- 5 means that the human operator considered the notification as fully disturbing 
This KPI is still under analysis on how it will be implemented. If with a single manual questionnaire or with a pop-up in the dashboard. ",None (score) ,7,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-AS-009,KPI-AS-009: Assistant disturbance (Power Grid),Assistant disturbance KPI aims to measure if the AI assistant's notifications are disturbing the human operator's activity. ,889d0c98-394d-411e-8a8a-e5023634ec51,Test score (SUM of scenario scores),SUM,7f8bd11e-8566-47ea-a6d8-464b0b29aefb,Scenario 1 - Assistant disturbance KPI aims to measure if the AI assistant's notifications are disturbing the human operator's activity. ,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",9fa77e95-6eea-460d-8450-ee5643212513,Scenario score (raw values)
18,KPI-AS-009,Assistant disturbance,Task 4.3,Human user experience,O3,ATM,['(non applicable)'],special evaluation setup,Assistant disturbance KPI aims to measure if the AI assistant's notifications are disturbing the human operator's activity. ,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ","For each notification, the score ranges in [0, 5] with:  
- 0 meaning that the notification was not considered disturbing at all by the human operator 
- 5 means that the human operator considered the notification as fully disturbing 
This KPI is still under analysis on how it will be implemented. If with a single manual questionnaire or with a pop-up in the dashboard. ",None (score) ,7,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-AS-009,KPI-AS-009: Assistant disturbance (ATM),Assistant disturbance KPI aims to measure if the AI assistant's notifications are disturbing the human operator's activity. ,889d0c98-394d-411e-8a8a-e5023634ec51,Test score (SUM of scenario scores),SUM,448179e6-2071-4b7d-b03c-c4e4b544366a,Scenario 1 - Assistant disturbance KPI aims to measure if the AI assistant's notifications are disturbing the human operator's activity. ,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",79092009-64d5-40cd-a86d-97bc44823f5e,Scenario score (raw values)
19,KPI-CF-012,Carbon intensity,Task 4.1,Solution quality,O2,Power Grid,['Digital environment'],fully automated evaluation,"Carbon intensity selectivity estimates the overall carbon intensity of the action recommendation provided by the AI assistant to the human operator: goal of carbon intensity KPI is to measure how much the actions will directly contribute to greenhouse gases emission, by focusing on CO2 (which is unfortunately not the only greenhouse gas).  

which is calculated as the weighted averaged emission factor of generation variation, including: 

Redispatching actions, 

Curtailment actions. ","This KPI is calculated to estimate the relative performance compared to a baseline. 
The main difficulty of evaluating and assessing this KPIs lies in the difficulty to establish a proper deadline: 
- There is no history of human actions on the digital environments used for evaluation (since they are synthetic ones), 
- Comparison with KPI calculated on real grid’s operations (TenneT or RTE) is not relevant since each grid has its own generation mix, and each TSO has its own operation policies (and own redispatching offers). 
This KPI contributes to evaluating Solution quality of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",See calculation steps,"kgCO2/MWh 
",8,['Power Grid'],0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9344b9be-477f-48e5-ab4a-2726b1bb63ee,Solution quality,"This KPI is calculated to estimate the relative performance compared to a baseline. 
The main difficulty of evaluating and assessing this KPIs lies in the difficulty to establish a proper deadline: 
- There is no history of human actions on the digital environments used for evaluation (since they are synthetic ones), 
- Comparison with KPI calculated on real grid’s operations (TenneT or RTE) is not relevant since each grid has its own generation mix, and each TSO has its own operation policies (and own redispatching offers). 
This KPI contributes to evaluating Solution quality of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",ae7940c6-48e1-413a-a97e-b5eb6281e95b,Benchmark score (SUM of test scores),SUM,KPI-CF-012,KPI-CF-012: Carbon intensity (Power Grid),"Carbon intensity selectivity estimates the overall carbon intensity of the action recommendation provided by the AI assistant to the human operator: goal of carbon intensity KPI is to measure how much the actions will directly contribute to greenhouse gases emission, by focusing on CO2 (which is unfortunately not the only greenhouse gas).  

which is calculated as the weighted averaged emission factor of generation variation, including: 

Redispatching actions, 

Curtailment actions. ",f6e764eb-3167-486d-9a44-821ae0b4b4e3,Test score (SUM of scenario scores),SUM,dc7c0dda-f147-49ac-bd82-856a3473310c,"Scenario 1 - Carbon intensity selectivity estimates the overall carbon intensity of the action recommendation provided by the AI assistant to the human operator: goal of carbon intensity KPI is to measure how much the actions will directly contribute to greenhouse gases emission, by focusing on CO2 (which is unfortunately not the only greenhouse gas).  

which is calculated as the weighted averaged emission factor of generation variation, including: 

Redispatching actions, 

Curtailment actions. ","This KPI is calculated to estimate the relative performance compared to a baseline. 
The main difficulty of evaluating and assessing this KPIs lies in the difficulty to establish a proper deadline: 
- There is no history of human actions on the digital environments used for evaluation (since they are synthetic ones), 
- Comparison with KPI calculated on real grid’s operations (TenneT or RTE) is not relevant since each grid has its own generation mix, and each TSO has its own operation policies (and own redispatching offers). 
This KPI contributes to evaluating Solution quality of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",f0572279-a2b3-497e-9100-dd363c0e34d4,Scenario score (raw values)
20,KPI-CS-013,Comprehensibility,Task 4.3,"AI acceptability, trust and trustworthiness",O2,Railway,['(non applicable)'],special evaluation setup,"This KPI represents human operators’ self-reported ability to understand and thus make use of the AI-generated solution/decision, measured with a questionnaire. ","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Comprehensibility”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,9,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3767d0c2-279b-4c59-92d9-5734312b22fb,"AI acceptability, trust and trustworthiness","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Comprehensibility”.  ",f16b1221-ed01-4e64-8d5c-7d673fc20829,Benchmark score (SUM of test scores),SUM,KPI-CS-013,KPI-CS-013: Comprehensibility (Railway),"This KPI represents human operators’ self-reported ability to understand and thus make use of the AI-generated solution/decision, measured with a questionnaire. ",f9378ad8-9d62-4ea8-813a-57708d982df9,Test score (SUM of scenario scores),SUM,b2f0ea16-ddfc-470e-93d3-9603bf6dc04c,"Scenario 1 - This KPI represents human operators’ self-reported ability to understand and thus make use of the AI-generated solution/decision, measured with a questionnaire. ","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Comprehensibility”.  ",5dfc1638-b0f5-42c5-a180-ec9fd93cc1d0,Scenario score (raw values)
21,KPI-CS-013,Comprehensibility,Task 4.3,"AI acceptability, trust and trustworthiness",O2,Power Grid,['(non applicable)'],special evaluation setup,"This KPI represents human operators’ self-reported ability to understand and thus make use of the AI-generated solution/decision, measured with a questionnaire. ","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Comprehensibility”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,9,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3767d0c2-279b-4c59-92d9-5734312b22fb,"AI acceptability, trust and trustworthiness","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Comprehensibility”.  ",f16b1221-ed01-4e64-8d5c-7d673fc20829,Benchmark score (SUM of test scores),SUM,KPI-CS-013,KPI-CS-013: Comprehensibility (Power Grid),"This KPI represents human operators’ self-reported ability to understand and thus make use of the AI-generated solution/decision, measured with a questionnaire. ",f9378ad8-9d62-4ea8-813a-57708d982df9,Test score (SUM of scenario scores),SUM,3a8beb09-a027-4cb1-9170-d822507b456a,"Scenario 1 - This KPI represents human operators’ self-reported ability to understand and thus make use of the AI-generated solution/decision, measured with a questionnaire. ","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Comprehensibility”.  ",2de2a8db-cab2-4274-8eed-b3d80c49ce49,Scenario score (raw values)
22,KPI-CS-013,Comprehensibility,Task 4.3,"AI acceptability, trust and trustworthiness",O2,ATM,['(non applicable)'],special evaluation setup,"This KPI represents human operators’ self-reported ability to understand and thus make use of the AI-generated solution/decision, measured with a questionnaire. ","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Comprehensibility”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,9,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3767d0c2-279b-4c59-92d9-5734312b22fb,"AI acceptability, trust and trustworthiness","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Comprehensibility”.  ",f16b1221-ed01-4e64-8d5c-7d673fc20829,Benchmark score (SUM of test scores),SUM,KPI-CS-013,KPI-CS-013: Comprehensibility (ATM),"This KPI represents human operators’ self-reported ability to understand and thus make use of the AI-generated solution/decision, measured with a questionnaire. ",f9378ad8-9d62-4ea8-813a-57708d982df9,Test score (SUM of scenario scores),SUM,f461b7fb-2cbe-45b0-93e2-a4d074c33399,"Scenario 1 - This KPI represents human operators’ self-reported ability to understand and thus make use of the AI-generated solution/decision, measured with a questionnaire. ","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Comprehensibility”.  ",c40db74a-101e-4155-a007-1ac12ada48f3,Scenario score (raw values)
23,KPI-DS-015,Decision support satisfaction,Task 4.3,Human user experience,O3,Railway,['(non applicable)'],special evaluation setup,This KPI represents human operators’ self-reported satisfaction with the system’s support for their decision-making process when working with the AI assistant measured with a questionnaire. ,"This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Decision support satisfaction”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,10,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Decision support satisfaction”.  ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-DS-015,KPI-DS-015: Decision support satisfaction (Railway),This KPI represents human operators’ self-reported satisfaction with the system’s support for their decision-making process when working with the AI assistant measured with a questionnaire. ,63196360-6081-484f-9411-7dc9f8938463,Test score (SUM of scenario scores),SUM,c37e8e91-2aa6-47c1-a1fb-2a2016d52ef4,Scenario 1 - This KPI represents human operators’ self-reported satisfaction with the system’s support for their decision-making process when working with the AI assistant measured with a questionnaire. ,"This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Decision support satisfaction”.  ",973f7599-bd87-4f0b-a934-ca257cbe775f,Scenario score (raw values)
24,KPI-DS-015,Decision support satisfaction,Task 4.3,Human user experience,O3,Power Grid,['(non applicable)'],special evaluation setup,This KPI represents human operators’ self-reported satisfaction with the system’s support for their decision-making process when working with the AI assistant measured with a questionnaire. ,"This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Decision support satisfaction”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,10,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Decision support satisfaction”.  ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-DS-015,KPI-DS-015: Decision support satisfaction (Power Grid),This KPI represents human operators’ self-reported satisfaction with the system’s support for their decision-making process when working with the AI assistant measured with a questionnaire. ,63196360-6081-484f-9411-7dc9f8938463,Test score (SUM of scenario scores),SUM,b59aa79f-727d-4222-a1d9-13d5180821dc,Scenario 1 - This KPI represents human operators’ self-reported satisfaction with the system’s support for their decision-making process when working with the AI assistant measured with a questionnaire. ,"This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Decision support satisfaction”.  ",297fcbd9-f659-4e1c-a382-b0442c2f7800,Scenario score (raw values)
25,KPI-DS-015,Decision support satisfaction,Task 4.3,Human user experience,O3,ATM,['(non applicable)'],special evaluation setup,This KPI represents human operators’ self-reported satisfaction with the system’s support for their decision-making process when working with the AI assistant measured with a questionnaire. ,"This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Decision support satisfaction”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,10,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Decision support satisfaction”.  ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-DS-015,KPI-DS-015: Decision support satisfaction (ATM),This KPI represents human operators’ self-reported satisfaction with the system’s support for their decision-making process when working with the AI assistant measured with a questionnaire. ,63196360-6081-484f-9411-7dc9f8938463,Test score (SUM of scenario scores),SUM,2b207d42-5f4d-4b3a-bb80-128cbd06629a,Scenario 1 - This KPI represents human operators’ self-reported satisfaction with the system’s support for their decision-making process when working with the AI assistant measured with a questionnaire. ,"This KPI contributes to evaluating Human user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Decision support satisfaction”.  ",046ea274-21a1-4ba6-9280-5b44f147de89,Scenario score (raw values)
26,KPI-DF-016,Delay reduction efficiency,Task 4.1,Effectiveness,O2,Railway,['Digital environment'],fully automated evaluation,"The Delay Reduction Efficiency KPI quantifies the effectiveness of the AI-driven re-scheduling system in reducing overall train delays. By comparing delays before and after AI intervention, this metric provides insight into the system's capability to optimize train schedules and minimize disruptions. ","This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective: 
- To assess the impact of AI-based re-scheduling on reducing delays in railway operations. 
- To ensure that AI interventions lead to measurable improvements in punctuality. 
- To provide a performance benchmark for AI-driven traffic management solutions in railway networks. ",(Total delay duration before AI implementation - Total delay duration after AI implementation) / Total delay duration before AI implementation.,Percentage (%) reduction in total delay time. ,11,['Railway'],0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,575728ac-628b-416e-a98e-34ef7858169a,Effectiveness,"This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective: 
- To assess the impact of AI-based re-scheduling on reducing delays in railway operations. 
- To ensure that AI interventions lead to measurable improvements in punctuality. 
- To provide a performance benchmark for AI-driven traffic management solutions in railway networks. ",8c4641d6-bf1f-415f-b8cd-3c7145487741,Benchmark score (SUM of test scores),SUM,KPI-DF-016,KPI-DF-016: Delay reduction efficiency (Railway),"The Delay Reduction Efficiency KPI quantifies the effectiveness of the AI-driven re-scheduling system in reducing overall train delays. By comparing delays before and after AI intervention, this metric provides insight into the system's capability to optimize train schedules and minimize disruptions. ",f69ff197-bd55-4852-9dfb-8ddb6f51ffed,Test score (SUM of scenario scores),SUM,9ee5f373-d72e-4eaf-a00e-34bfee14964f,"Scenario 1 - The Delay Reduction Efficiency KPI quantifies the effectiveness of the AI-driven re-scheduling system in reducing overall train delays. By comparing delays before and after AI intervention, this metric provides insight into the system's capability to optimize train schedules and minimize disruptions. ","This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective: 
- To assess the impact of AI-based re-scheduling on reducing delays in railway operations. 
- To ensure that AI interventions lead to measurable improvements in punctuality. 
- To provide a performance benchmark for AI-driven traffic management solutions in railway networks. ",179ccc74-145c-4442-87bc-ca6ccddb4d6f,Scenario score (raw values)
27,KPI-HS-018,Human control/autonomy over the process,Task 4.3,AI-human task allocation balance,O3,Railway,['(non applicable)'],special evaluation setup,This KPI represents human operators’ perceived autonomy over the process when working with the AI assistant measured with a questionnaire. ,"This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Human Agency and Oversight”, “Human control/autonomy over the process”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,12,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,00e19f42-d9b4-48dd-a818-1197cffe81e0,AI-human task allocation balance,"This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Human Agency and Oversight”, “Human control/autonomy over the process”.  ",f12fe8b1-29e0-4dd5-83c7-41bcb2055786,Benchmark score (SUM of test scores),SUM,KPI-HS-018,KPI-HS-018: Human control/autonomy over the process (Railway),This KPI represents human operators’ perceived autonomy over the process when working with the AI assistant measured with a questionnaire. ,8b606785-0ecf-4834-ab14-911423692afe,Test score (SUM of scenario scores),SUM,a9dcd9c4-25f8-4ded-a060-83f08a07be92,Scenario 1 - This KPI represents human operators’ perceived autonomy over the process when working with the AI assistant measured with a questionnaire. ,"This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Human Agency and Oversight”, “Human control/autonomy over the process”.  ",f2aa3c38-f1a8-466d-adfd-ee9c870d79e3,Scenario score (raw values)
28,KPI-HS-018,Human control/autonomy over the process,Task 4.3,AI-human task allocation balance,O3,Power Grid,['(non applicable)'],special evaluation setup,This KPI represents human operators’ perceived autonomy over the process when working with the AI assistant measured with a questionnaire. ,"This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Human Agency and Oversight”, “Human control/autonomy over the process”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,12,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,00e19f42-d9b4-48dd-a818-1197cffe81e0,AI-human task allocation balance,"This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Human Agency and Oversight”, “Human control/autonomy over the process”.  ",f12fe8b1-29e0-4dd5-83c7-41bcb2055786,Benchmark score (SUM of test scores),SUM,KPI-HS-018,KPI-HS-018: Human control/autonomy over the process (Power Grid),This KPI represents human operators’ perceived autonomy over the process when working with the AI assistant measured with a questionnaire. ,8b606785-0ecf-4834-ab14-911423692afe,Test score (SUM of scenario scores),SUM,c005b0ce-3899-412f-950b-85dbad2a79a7,Scenario 1 - This KPI represents human operators’ perceived autonomy over the process when working with the AI assistant measured with a questionnaire. ,"This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Human Agency and Oversight”, “Human control/autonomy over the process”.  ",69d5f950-28ad-4c71-b649-ecb9aaaf4058,Scenario score (raw values)
29,KPI-HS-018,Human control/autonomy over the process,Task 4.3,AI-human task allocation balance,O3,ATM,['(non applicable)'],special evaluation setup,This KPI represents human operators’ perceived autonomy over the process when working with the AI assistant measured with a questionnaire. ,"This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Human Agency and Oversight”, “Human control/autonomy over the process”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,12,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,00e19f42-d9b4-48dd-a818-1197cffe81e0,AI-human task allocation balance,"This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Human Agency and Oversight”, “Human control/autonomy over the process”.  ",f12fe8b1-29e0-4dd5-83c7-41bcb2055786,Benchmark score (SUM of test scores),SUM,KPI-HS-018,KPI-HS-018: Human control/autonomy over the process (ATM),This KPI represents human operators’ perceived autonomy over the process when working with the AI assistant measured with a questionnaire. ,8b606785-0ecf-4834-ab14-911423692afe,Test score (SUM of scenario scores),SUM,9b97bcaf-3b4e-41b3-a82f-abcac3daff72,Scenario 1 - This KPI represents human operators’ perceived autonomy over the process when working with the AI assistant measured with a questionnaire. ,"This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Decision support for the human operator”, “Human Agency and Oversight”, “Human control/autonomy over the process”.  ",b2133336-76ab-4fbf-b21a-82573207eeac,Scenario score (raw values)
30,KPI-HS-021,Human learning,Task 4.3,AI-human learning curves,O3,Railway,['(non applicable)'],special evaluation setup,"Human learning is a complex process that leads to lasting changes in humans, influencing their perceptions of the world and their interactions with it across physical, psychological, and social dimensions. It is fundamentally shaped by the ongoing, interactive relationship between the learner's characteristics and the learning content, all situated within the specific environmental context of time and place and the continuity over time. ","This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,Lickert-Scale or similar ,13,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a5d464fd-6ac9-4b3a-adc6-f83e83f0adad,AI-human learning curves,"This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",a82ebe2d-b507-49fb-a6ae-96281d88d938,Benchmark score (SUM of test scores),SUM,KPI-HS-021,KPI-HS-021: Human learning (Railway),"Human learning is a complex process that leads to lasting changes in humans, influencing their perceptions of the world and their interactions with it across physical, psychological, and social dimensions. It is fundamentally shaped by the ongoing, interactive relationship between the learner's characteristics and the learning content, all situated within the specific environmental context of time and place and the continuity over time. ",51b153e0-86a6-4d55-84ff-48240b87fac0,Test score (SUM of scenario scores),SUM,83b5f6f7-77ea-4d89-86dc-ab22b2a00fb5,"Scenario 1 - Human learning is a complex process that leads to lasting changes in humans, influencing their perceptions of the world and their interactions with it across physical, psychological, and social dimensions. It is fundamentally shaped by the ongoing, interactive relationship between the learner's characteristics and the learning content, all situated within the specific environmental context of time and place and the continuity over time. ","This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",f71365cb-b1b8-43f5-914c-823a49d32134,Scenario score (raw values)
31,KPI-HS-021,Human learning,Task 4.3,AI-human learning curves,O3,Power Grid,['(non applicable)'],special evaluation setup,"Human learning is a complex process that leads to lasting changes in humans, influencing their perceptions of the world and their interactions with it across physical, psychological, and social dimensions. It is fundamentally shaped by the ongoing, interactive relationship between the learner's characteristics and the learning content, all situated within the specific environmental context of time and place and the continuity over time. ","This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,Lickert-Scale or similar ,13,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a5d464fd-6ac9-4b3a-adc6-f83e83f0adad,AI-human learning curves,"This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",a82ebe2d-b507-49fb-a6ae-96281d88d938,Benchmark score (SUM of test scores),SUM,KPI-HS-021,KPI-HS-021: Human learning (Power Grid),"Human learning is a complex process that leads to lasting changes in humans, influencing their perceptions of the world and their interactions with it across physical, psychological, and social dimensions. It is fundamentally shaped by the ongoing, interactive relationship between the learner's characteristics and the learning content, all situated within the specific environmental context of time and place and the continuity over time. ",51b153e0-86a6-4d55-84ff-48240b87fac0,Test score (SUM of scenario scores),SUM,19d2810c-3007-4f5d-a8e4-babb68ac59d0,"Scenario 1 - Human learning is a complex process that leads to lasting changes in humans, influencing their perceptions of the world and their interactions with it across physical, psychological, and social dimensions. It is fundamentally shaped by the ongoing, interactive relationship between the learner's characteristics and the learning content, all situated within the specific environmental context of time and place and the continuity over time. ","This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",9f627197-b8f0-4741-9134-88713d6d2fb0,Scenario score (raw values)
32,KPI-HS-021,Human learning,Task 4.3,AI-human learning curves,O3,ATM,['(non applicable)'],special evaluation setup,"Human learning is a complex process that leads to lasting changes in humans, influencing their perceptions of the world and their interactions with it across physical, psychological, and social dimensions. It is fundamentally shaped by the ongoing, interactive relationship between the learner's characteristics and the learning content, all situated within the specific environmental context of time and place and the continuity over time. ","This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,Lickert-Scale or similar ,13,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a5d464fd-6ac9-4b3a-adc6-f83e83f0adad,AI-human learning curves,"This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",a82ebe2d-b507-49fb-a6ae-96281d88d938,Benchmark score (SUM of test scores),SUM,KPI-HS-021,KPI-HS-021: Human learning (ATM),"Human learning is a complex process that leads to lasting changes in humans, influencing their perceptions of the world and their interactions with it across physical, psychological, and social dimensions. It is fundamentally shaped by the ongoing, interactive relationship between the learner's characteristics and the learning content, all situated within the specific environmental context of time and place and the continuity over time. ",51b153e0-86a6-4d55-84ff-48240b87fac0,Test score (SUM of scenario scores),SUM,4bade05f-84f9-4072-993b-95c91e057383,"Scenario 1 - Human learning is a complex process that leads to lasting changes in humans, influencing their perceptions of the world and their interactions with it across physical, psychological, and social dimensions. It is fundamentally shaped by the ongoing, interactive relationship between the learner's characteristics and the learning content, all situated within the specific environmental context of time and place and the continuity over time. ","This KPI contributes to evaluating AI-human learning curves of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",21cefafd-4fac-479f-879e-7e1334bb368d,Scenario score (raw values)
33,KPI-HS-022,Human motivation,Task 4.3,Human user experience,O3,Railway,['(non applicable)'],special evaluation setup,"“Intrinsic motivation is defined as doing an activity for its inherent satisfaction rather than for some separable consequence. When intrinsically motivated, a person is moved to act for the fun or challenge entailed rather than because of external products, pressures, or rewards” (Ryan & Deci, 2000, p. 56). ","This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,Lickert-Scale or similar ,14,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-HS-022,KPI-HS-022: Human motivation (Railway),"“Intrinsic motivation is defined as doing an activity for its inherent satisfaction rather than for some separable consequence. When intrinsically motivated, a person is moved to act for the fun or challenge entailed rather than because of external products, pressures, or rewards” (Ryan & Deci, 2000, p. 56). ",89d054fb-a978-469e-8388-6bab92eb82f7,Test score (SUM of scenario scores),SUM,87ed6a8b-d9f0-4d99-9cf5-b59a3c5e2543,"Scenario 1 - “Intrinsic motivation is defined as doing an activity for its inherent satisfaction rather than for some separable consequence. When intrinsically motivated, a person is moved to act for the fun or challenge entailed rather than because of external products, pressures, or rewards” (Ryan & Deci, 2000, p. 56). ","This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",21339422-23ce-4788-905d-99ae2b7655a7,Scenario score (raw values)
34,KPI-HS-022,Human motivation,Task 4.3,Human user experience,O3,Power Grid,['(non applicable)'],special evaluation setup,"“Intrinsic motivation is defined as doing an activity for its inherent satisfaction rather than for some separable consequence. When intrinsically motivated, a person is moved to act for the fun or challenge entailed rather than because of external products, pressures, or rewards” (Ryan & Deci, 2000, p. 56). ","This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,Lickert-Scale or similar ,14,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-HS-022,KPI-HS-022: Human motivation (Power Grid),"“Intrinsic motivation is defined as doing an activity for its inherent satisfaction rather than for some separable consequence. When intrinsically motivated, a person is moved to act for the fun or challenge entailed rather than because of external products, pressures, or rewards” (Ryan & Deci, 2000, p. 56). ",89d054fb-a978-469e-8388-6bab92eb82f7,Test score (SUM of scenario scores),SUM,dbc477c8-1c90-456d-82e3-9f72007af721,"Scenario 1 - “Intrinsic motivation is defined as doing an activity for its inherent satisfaction rather than for some separable consequence. When intrinsically motivated, a person is moved to act for the fun or challenge entailed rather than because of external products, pressures, or rewards” (Ryan & Deci, 2000, p. 56). ","This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",2c94ba82-9ba2-403d-a2d4-e9445b1524cf,Scenario score (raw values)
35,KPI-HS-022,Human motivation,Task 4.3,Human user experience,O3,ATM,['(non applicable)'],special evaluation setup,"“Intrinsic motivation is defined as doing an activity for its inherent satisfaction rather than for some separable consequence. When intrinsically motivated, a person is moved to act for the fun or challenge entailed rather than because of external products, pressures, or rewards” (Ryan & Deci, 2000, p. 56). ","This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,Lickert-Scale or similar ,14,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-HS-022,KPI-HS-022: Human motivation (ATM),"“Intrinsic motivation is defined as doing an activity for its inherent satisfaction rather than for some separable consequence. When intrinsically motivated, a person is moved to act for the fun or challenge entailed rather than because of external products, pressures, or rewards” (Ryan & Deci, 2000, p. 56). ",89d054fb-a978-469e-8388-6bab92eb82f7,Test score (SUM of scenario scores),SUM,2c8edf59-8cca-4d45-a180-3150ae59ee89,"Scenario 1 - “Intrinsic motivation is defined as doing an activity for its inherent satisfaction rather than for some separable consequence. When intrinsically motivated, a person is moved to act for the fun or challenge entailed rather than because of external products, pressures, or rewards” (Ryan & Deci, 2000, p. 56). ","This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",1399f89a-bd43-4752-a611-edce23f388d0,Scenario score (raw values)
36,KPI-HS-023,Human response time,Task 4.3,Human user experience,O3,Railway,['Human machine interaction module'],semi-automated evaluation,Human response time KPI evaluates time needed to react to AI advisory/information. ,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ","The time should be measured directly from user input and automatically by the system in background (dismiss a window when they feel satisfied after evaluating a scenario): 
- LOW less than 5 min,  
- MEDIUM 5-10 min,  
- HIGH more than 15 minutes. 
Then it is translated into % across the operator's multiple interactions with AI-generated solutions.  
This KPI is still under analysis on how it will be implemented. The objective is that it is transversal to all domains, but this means that an implementation will need to be done in each virtual environment. This implementation is still not defined and will need to be discussed with other Tasks/WPs ","LOW, MED, HIGH response time % ",15,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-HS-023,KPI-HS-023: Human response time (Railway),Human response time KPI evaluates time needed to react to AI advisory/information. ,97b46080-c3b4-4271-982b-eef9b6d2bf74,Test score (SUM of scenario scores),SUM,89fc2411-2c8d-42b7-a702-ec47ec89c0e5,Scenario 1 - Human response time KPI evaluates time needed to react to AI advisory/information. ,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",b891ce1f-59c1-45dc-92b4-cd5c6f36d05d,Scenario score (raw values)
37,KPI-HS-023,Human response time,Task 4.3,Human user experience,O3,Power Grid,['Human machine interaction module'],semi-automated evaluation,Human response time KPI evaluates time needed to react to AI advisory/information. ,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ","The time should be measured directly from user input and automatically by the system in background (dismiss a window when they feel satisfied after evaluating a scenario): 
- LOW less than 5 min,  
- MEDIUM 5-10 min,  
- HIGH more than 15 minutes. 
Then it is translated into % across the operator's multiple interactions with AI-generated solutions.  
This KPI is still under analysis on how it will be implemented. The objective is that it is transversal to all domains, but this means that an implementation will need to be done in each virtual environment. This implementation is still not defined and will need to be discussed with other Tasks/WPs ","LOW, MED, HIGH response time % ",15,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-HS-023,KPI-HS-023: Human response time (Power Grid),Human response time KPI evaluates time needed to react to AI advisory/information. ,97b46080-c3b4-4271-982b-eef9b6d2bf74,Test score (SUM of scenario scores),SUM,68a51dc8-cb6b-4c88-b4e0-3f739e4a61eb,Scenario 1 - Human response time KPI evaluates time needed to react to AI advisory/information. ,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",cb5fdf60-987f-41c4-bf23-50905e85b888,Scenario score (raw values)
38,KPI-HS-023,Human response time,Task 4.3,Human user experience,O3,ATM,['Human machine interaction module'],semi-automated evaluation,Human response time KPI evaluates time needed to react to AI advisory/information. ,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ","The time should be measured directly from user input and automatically by the system in background (dismiss a window when they feel satisfied after evaluating a scenario): 
- LOW less than 5 min,  
- MEDIUM 5-10 min,  
- HIGH more than 15 minutes. 
Then it is translated into % across the operator's multiple interactions with AI-generated solutions.  
This KPI is still under analysis on how it will be implemented. The objective is that it is transversal to all domains, but this means that an implementation will need to be done in each virtual environment. This implementation is still not defined and will need to be discussed with other Tasks/WPs ","LOW, MED, HIGH response time % ",15,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-HS-023,KPI-HS-023: Human response time (ATM),Human response time KPI evaluates time needed to react to AI advisory/information. ,97b46080-c3b4-4271-982b-eef9b6d2bf74,Test score (SUM of scenario scores),SUM,6e79ae0d-7d30-46c4-99a7-d43a9fbfe64b,Scenario 1 - Human response time KPI evaluates time needed to react to AI advisory/information. ,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",ced66c9c-45ac-45cf-a062-1cdafaf8971d,Scenario score (raw values)
39,KPI-NF-024,Network utilization,Task 4.1,Effectiveness,O2,Power Grid,['Digital environment'],fully automated evaluation,"Network utilization KPI is based on the relative line loads of the network, indicating to what extent the network and its components are utilized.","This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ","This KPI yield a vector with 6 values, that are calculated over all scenarios’ steps: 
- the maximum line’s load in N state,  
- the maximum line’s load in N-1 state,  
- the average of the maximum line’s load in N state per step,  
- the average of the maximum line’s load in N-1 state per step,  
- the share of lines where the line’s load in N state is greater than 90%, 
- the share of lines where the line’s load in N-1 state is greater than 100%. 
Line’s load is referred to as rho in Grid2Op and is defined as the observed current flow divided by the thermal limit of the line. ",Vector of 6 values expressed in percent (decimal number between 0% and 100%) ,16,['Power Grid'],0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,575728ac-628b-416e-a98e-34ef7858169a,Effectiveness,"This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",8c4641d6-bf1f-415f-b8cd-3c7145487741,Benchmark score (SUM of test scores),SUM,KPI-NF-024,KPI-NF-024: Network utilization (Power Grid),"Network utilization KPI is based on the relative line loads of the network, indicating to what extent the network and its components are utilized.",dd2b2d43-e460-442d-9658-560c36ee26d8,Test score (SUM of scenario scores),SUM,1b7bbef8-355b-4beb-8d1b-9d99a3d5ddc3,"Scenario 1 - Network utilization KPI is based on the relative line loads of the network, indicating to what extent the network and its components are utilized.","This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",68aac728-68f2-4d48-9489-222da52336fa,Scenario score (raw values)
40,KPI-PF-026,Punctuality,Task 4.1,Effectiveness,O2,Railway,['Digital environment'],fully automated evaluation,Punctuality measures the percentage of trains arriving at their destinations on time (the train doesn’t arrive after planned arrival) and the train didn’t depart before planned departure time. The goal is to maintain a high level of reliability and minimize delays for passengers and freight services. ,"This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective: 
- Improve customer satisfaction by ensuring timely arrivals 
- Guarantee maximal planned connection  
- Minimize operational disruptions caused by delays 
- Meet regulatory and stakeholder benchmarks for punctuality 
This KPI is linked with project’s Long Term Expected Impacts (LTEI) (LTEI1)KPIS-3:  
- 10% increase in punctuality in long-range traffic  
- 5% increase in punctuality in regional traffic (with realistic disturbances) 
",(Number of on-time arrivals / Total number of arrivals) x 100,Percentage (%) ,17,['Railway'],0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,575728ac-628b-416e-a98e-34ef7858169a,Effectiveness,"This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective: 
- Improve customer satisfaction by ensuring timely arrivals 
- Guarantee maximal planned connection  
- Minimize operational disruptions caused by delays 
- Meet regulatory and stakeholder benchmarks for punctuality 
This KPI is linked with project’s Long Term Expected Impacts (LTEI) (LTEI1)KPIS-3:  
- 10% increase in punctuality in long-range traffic  
- 5% increase in punctuality in regional traffic (with realistic disturbances) 
",8c4641d6-bf1f-415f-b8cd-3c7145487741,Benchmark score (SUM of test scores),SUM,KPI-PF-026,KPI-PF-026: Punctuality (Railway),Punctuality measures the percentage of trains arriving at their destinations on time (the train doesn’t arrive after planned arrival) and the train didn’t depart before planned departure time. The goal is to maintain a high level of reliability and minimize delays for passengers and freight services. ,f8118dc8-6b5a-478d-a73c-78d22c29fbd6,Test score (SUM of scenario scores),SUM,3db41010-88a7-426d-9994-c538e073efd0,Scenario 1 - Punctuality measures the percentage of trains arriving at their destinations on time (the train doesn’t arrive after planned arrival) and the train didn’t depart before planned departure time. The goal is to maintain a high level of reliability and minimize delays for passengers and freight services. ,"This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective: 
- Improve customer satisfaction by ensuring timely arrivals 
- Guarantee maximal planned connection  
- Minimize operational disruptions caused by delays 
- Meet regulatory and stakeholder benchmarks for punctuality 
This KPI is linked with project’s Long Term Expected Impacts (LTEI) (LTEI1)KPIS-3:  
- 10% increase in punctuality in long-range traffic  
- 5% increase in punctuality in regional traffic (with realistic disturbances) 
",97165c3e-d04c-4df1-83b2-37f297046c95,Scenario score (raw values)
41,KPI-RF-027,Reduction in delay,Task 4.1,Effectiveness,O2,ATM,['Digital environment'],fully automated evaluation,"The reduction in delay KPI aims to quantify the time gained overall and for each airplane, with the introduction of AI. ","This KPI aims to quantify the efficiency gains of AI integration by measuring how AI impacts execution time and delays. Specifically, it helps determine whether AI: 
- Reduces execution time deviations 
- Minimizes delays
- Enhances consistency and reliability in operations. 
By evaluating these metrics, we can assess the AI’s effectiveness in improving human decision-making, reducing intervention time, and optimizing operational workflows. 
This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. 
This KPI is linked with project’s Long Term Expected Impact (LTEI) (LTEI1)KPIS-4, 3-6% improvement in flight capacity and mile extension. ","Performance Deviation measures the percentage deviation of actual time from expected time
Delay Measurement measures the absolute delay in arrival time
These formulas will be applied to both human-only performance and human-AI collaborative performance, resulting in Human performance and Human-AI performance",Percentage and seconds ,18,['ATM'],0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,575728ac-628b-416e-a98e-34ef7858169a,Effectiveness,"This KPI aims to quantify the efficiency gains of AI integration by measuring how AI impacts execution time and delays. Specifically, it helps determine whether AI: 
- Reduces execution time deviations 
- Minimizes delays
- Enhances consistency and reliability in operations. 
By evaluating these metrics, we can assess the AI’s effectiveness in improving human decision-making, reducing intervention time, and optimizing operational workflows. 
This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. 
This KPI is linked with project’s Long Term Expected Impact (LTEI) (LTEI1)KPIS-4, 3-6% improvement in flight capacity and mile extension. ",8c4641d6-bf1f-415f-b8cd-3c7145487741,Benchmark score (SUM of test scores),SUM,KPI-RF-027,KPI-RF-027: Reduction in delay (ATM),"The reduction in delay KPI aims to quantify the time gained overall and for each airplane, with the introduction of AI. ",0cc466cf-4444-4ad0-ae35-b59555d2f672,Test score (SUM of scenario scores),SUM,57560175-82d8-4d0f-a4a0-bb89a21618e3,"Scenario 1 - The reduction in delay KPI aims to quantify the time gained overall and for each airplane, with the introduction of AI. ","This KPI aims to quantify the efficiency gains of AI integration by measuring how AI impacts execution time and delays. Specifically, it helps determine whether AI: 
- Reduces execution time deviations 
- Minimizes delays
- Enhances consistency and reliability in operations. 
By evaluating these metrics, we can assess the AI’s effectiveness in improving human decision-making, reducing intervention time, and optimizing operational workflows. 
This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. 
This KPI is linked with project’s Long Term Expected Impact (LTEI) (LTEI1)KPIS-4, 3-6% improvement in flight capacity and mile extension. ",be5186c7-f7f5-40a7-b978-b00f36446ea4,Scenario score (raw values)
42,KPI-AF-029,AI Response time,Task 4.1,Effectiveness,O2,Railway,['Digital environment'],fully automated evaluation,"The Response Time KPI measures the time taken by the AI-assisted railway re-scheduling system to generate a new operational schedule in response to a disruption. This metric evaluates how quickly the system reacts to unexpected events, ensuring minimal delays and maintaining operational efficiency. ","This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective: 
- To assess the speed of AI-assisted decision-making in railway operations. 
- To ensure rapid re-scheduling of trains in response to disturbances, minimizing the impact on passengers and freight. 
- To compare AI-assisted response times with traditional manual re-scheduling approaches. ",Average time taken from disruption detection/prediction to suggestion of adjusted schedule(s),Time (minutes or seconds)  ,19,['Railway'],0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,575728ac-628b-416e-a98e-34ef7858169a,Effectiveness,"This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective: 
- To assess the speed of AI-assisted decision-making in railway operations. 
- To ensure rapid re-scheduling of trains in response to disturbances, minimizing the impact on passengers and freight. 
- To compare AI-assisted response times with traditional manual re-scheduling approaches. ",8c4641d6-bf1f-415f-b8cd-3c7145487741,Benchmark score (SUM of test scores),SUM,KPI-AF-029,KPI-AF-029: AI Response time (Railway),"The Response Time KPI measures the time taken by the AI-assisted railway re-scheduling system to generate a new operational schedule in response to a disruption. This metric evaluates how quickly the system reacts to unexpected events, ensuring minimal delays and maintaining operational efficiency. ",21326b84-3992-46cc-b469-76b5f4c9acb3,Test score (SUM of scenario scores),SUM,f2aba6fc-141d-4263-be19-343c7dbe8702,"Scenario 1 - The Response Time KPI measures the time taken by the AI-assisted railway re-scheduling system to generate a new operational schedule in response to a disruption. This metric evaluates how quickly the system reacts to unexpected events, ensuring minimal delays and maintaining operational efficiency. ","This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective: 
- To assess the speed of AI-assisted decision-making in railway operations. 
- To ensure rapid re-scheduling of trains in response to disturbances, minimizing the impact on passengers and freight. 
- To compare AI-assisted response times with traditional manual re-scheduling approaches. ",b5e9c545-2cbf-4df5-9e29-201b20d55ea6,Scenario score (raw values)
43,KPI-SS-030,Significance of human revisions,Task 4.3,Social-technical decision quality,O3,Railway,['Human machine interaction module'],semi-automated evaluation,"This KPI represents human operators’ subjective assessment of necessary revisions for the AI-generated solutions by the human operator, self-reported by the operator with Likert-scale questions. ","This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Significance of human revisions”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,20,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3b1aff0c-5c94-445c-a106-c94ab1f780ca,Social-technical decision quality,"This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Significance of human revisions”.  ",bbacedca-d4da-4c9b-afba-590bc1d9919e,Benchmark score (SUM of test scores),SUM,KPI-SS-030,KPI-SS-030: Significance of human revisions (Railway),"This KPI represents human operators’ subjective assessment of necessary revisions for the AI-generated solutions by the human operator, self-reported by the operator with Likert-scale questions. ",9cebac58-fad8-4e5a-9601-3a68500934b9,Test score (SUM of scenario scores),SUM,b2369393-285a-4353-a16d-9a61a25a7a6e,"Scenario 1 - This KPI represents human operators’ subjective assessment of necessary revisions for the AI-generated solutions by the human operator, self-reported by the operator with Likert-scale questions. ","This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Significance of human revisions”.  ",028b144d-e457-4b0c-9e4d-5b3a090c27d3,Scenario score (raw values)
44,KPI-SS-030,Significance of human revisions,Task 4.3,Social-technical decision quality,O3,Power Grid,['Human machine interaction module'],semi-automated evaluation,"This KPI represents human operators’ subjective assessment of necessary revisions for the AI-generated solutions by the human operator, self-reported by the operator with Likert-scale questions. ","This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Significance of human revisions”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,20,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3b1aff0c-5c94-445c-a106-c94ab1f780ca,Social-technical decision quality,"This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Significance of human revisions”.  ",bbacedca-d4da-4c9b-afba-590bc1d9919e,Benchmark score (SUM of test scores),SUM,KPI-SS-030,KPI-SS-030: Significance of human revisions (Power Grid),"This KPI represents human operators’ subjective assessment of necessary revisions for the AI-generated solutions by the human operator, self-reported by the operator with Likert-scale questions. ",9cebac58-fad8-4e5a-9601-3a68500934b9,Test score (SUM of scenario scores),SUM,888b006a-ebd0-40cc-b342-da5bdedc7482,"Scenario 1 - This KPI represents human operators’ subjective assessment of necessary revisions for the AI-generated solutions by the human operator, self-reported by the operator with Likert-scale questions. ","This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Significance of human revisions”.  ",f4cf26c6-3557-48a8-830b-523d6c65f396,Scenario score (raw values)
45,KPI-SS-030,Significance of human revisions,Task 4.3,Social-technical decision quality,O3,ATM,['Human machine interaction module'],semi-automated evaluation,"This KPI represents human operators’ subjective assessment of necessary revisions for the AI-generated solutions by the human operator, self-reported by the operator with Likert-scale questions. ","This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Significance of human revisions”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,20,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3b1aff0c-5c94-445c-a106-c94ab1f780ca,Social-technical decision quality,"This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Significance of human revisions”.  ",bbacedca-d4da-4c9b-afba-590bc1d9919e,Benchmark score (SUM of test scores),SUM,KPI-SS-030,KPI-SS-030: Significance of human revisions (ATM),"This KPI represents human operators’ subjective assessment of necessary revisions for the AI-generated solutions by the human operator, self-reported by the operator with Likert-scale questions. ",9cebac58-fad8-4e5a-9601-3a68500934b9,Test score (SUM of scenario scores),SUM,96ddbfb8-58c9-420f-824d-3651f58a0c09,"Scenario 1 - This KPI represents human operators’ subjective assessment of necessary revisions for the AI-generated solutions by the human operator, self-reported by the operator with Likert-scale questions. ","This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Significance of human revisions”.  ",c2e7c70d-e18e-4b1a-9278-2d31cb7d045a,Scenario score (raw values)
46,KPI-SS-031,Situation awareness,Task 4.3,Human user experience,O3,Railway,['(non applicable)'],special evaluation setup,"“Situation Awareness is the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future” (Endsley, 1988). ","This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,Lickert-Scale or similar ,21,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-SS-031,KPI-SS-031: Situation awareness (Railway),"“Situation Awareness is the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future” (Endsley, 1988). ",b8151684-4268-4ef0-a27a-6ca345bb4f37,Test score (SUM of scenario scores),SUM,b7143ac1-68d8-4843-a8df-1673a7a957a2,"Scenario 1 - “Situation Awareness is the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future” (Endsley, 1988). ","This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",4d93aa4a-15a5-4ecd-8b46-bce230454853,Scenario score (raw values)
47,KPI-SS-031,Situation awareness,Task 4.3,Human user experience,O3,Power Grid,['(non applicable)'],special evaluation setup,"“Situation Awareness is the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future” (Endsley, 1988). ","This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,Lickert-Scale or similar ,21,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-SS-031,KPI-SS-031: Situation awareness (Power Grid),"“Situation Awareness is the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future” (Endsley, 1988). ",b8151684-4268-4ef0-a27a-6ca345bb4f37,Test score (SUM of scenario scores),SUM,45bea49e-cfbc-4561-aa1f-d1c2783a5589,"Scenario 1 - “Situation Awareness is the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future” (Endsley, 1988). ","This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",10a19e4a-b47c-4660-87f0-8a2bb09d1bd9,Scenario score (raw values)
48,KPI-SS-031,Situation awareness,Task 4.3,Human user experience,O3,ATM,['(non applicable)'],special evaluation setup,"“Situation Awareness is the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future” (Endsley, 1988). ","This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,Lickert-Scale or similar ,21,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-SS-031,KPI-SS-031: Situation awareness (ATM),"“Situation Awareness is the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future” (Endsley, 1988). ",b8151684-4268-4ef0-a27a-6ca345bb4f37,Test score (SUM of scenario scores),SUM,6235ada3-1ff8-4d6f-9dad-2d5baa83bbcd,"Scenario 1 - “Situation Awareness is the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future” (Endsley, 1988). ","This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",1e32fb42-399b-4c0d-8e0a-74b8067f43e8,Scenario score (raw values)
49,KPI-SS-032,System efficiency,Task 4.1,Effectiveness,O2,ATM,"['Human machine interaction module', 'Digital environment']",semi-automated evaluation,System efficiency measures the efficiency of the system in delivering trustworthy solutions requiring less effort and time to deliver an appropriate response by the operator.  ,"The System efficiency KPI aims to evaluate the effectiveness of the AI solution in real operational conditions, considering not just its raw response time but also the quality and usability of its assistance. This includes how the AI presents its advice, its ease of use, the accuracy of its recommendations, and how well it integrates with existing data and workflows. 
The evaluation will measure the AI-human collaboration, focusing on: 
- Response efficiency: The time taken for the AI to generate advice and for the human operator to act on it. 
- Advice clarity & usability: How well structured, coherent, and understandable the AI’s suggestions are. 
- Data integration quality: How seamlessly the AI incorporates relevant information into its recommendations. 
- Human correction factor: Whether and how often the operator needs to correct or refine the AI’s output. 
- Decision-making speed: The overall reduction in response time achieved through AI-assisted operation. 
By considering these factors, the tests aim to assess how well the AI minimizes human intervention while maximizing efficiency, accuracy, and usability in decision-making. 
This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",Number of tests where time it takes the human to compute a solution is greater than time it takes for the AI to compute a solution and the human operator to accept the solution ,Percentage (%) ,22,['ATM'],0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,575728ac-628b-416e-a98e-34ef7858169a,Effectiveness,"The System efficiency KPI aims to evaluate the effectiveness of the AI solution in real operational conditions, considering not just its raw response time but also the quality and usability of its assistance. This includes how the AI presents its advice, its ease of use, the accuracy of its recommendations, and how well it integrates with existing data and workflows. 
The evaluation will measure the AI-human collaboration, focusing on: 
- Response efficiency: The time taken for the AI to generate advice and for the human operator to act on it. 
- Advice clarity & usability: How well structured, coherent, and understandable the AI’s suggestions are. 
- Data integration quality: How seamlessly the AI incorporates relevant information into its recommendations. 
- Human correction factor: Whether and how often the operator needs to correct or refine the AI’s output. 
- Decision-making speed: The overall reduction in response time achieved through AI-assisted operation. 
By considering these factors, the tests aim to assess how well the AI minimizes human intervention while maximizing efficiency, accuracy, and usability in decision-making. 
This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",8c4641d6-bf1f-415f-b8cd-3c7145487741,Benchmark score (SUM of test scores),SUM,KPI-SS-032,KPI-SS-032: System efficiency (ATM),System efficiency measures the efficiency of the system in delivering trustworthy solutions requiring less effort and time to deliver an appropriate response by the operator.  ,4d052ec7-2c5f-43d3-98e6-f472434f5e35,Test score (SUM of scenario scores),SUM,3aaeffdc-56ff-4a38-b358-7d6c49111da4,Scenario 1 - System efficiency measures the efficiency of the system in delivering trustworthy solutions requiring less effort and time to deliver an appropriate response by the operator.  ,"The System efficiency KPI aims to evaluate the effectiveness of the AI solution in real operational conditions, considering not just its raw response time but also the quality and usability of its assistance. This includes how the AI presents its advice, its ease of use, the accuracy of its recommendations, and how well it integrates with existing data and workflows. 
The evaluation will measure the AI-human collaboration, focusing on: 
- Response efficiency: The time taken for the AI to generate advice and for the human operator to act on it. 
- Advice clarity & usability: How well structured, coherent, and understandable the AI’s suggestions are. 
- Data integration quality: How seamlessly the AI incorporates relevant information into its recommendations. 
- Human correction factor: Whether and how often the operator needs to correct or refine the AI’s output. 
- Decision-making speed: The overall reduction in response time achieved through AI-assisted operation. 
By considering these factors, the tests aim to assess how well the AI minimizes human intervention while maximizing efficiency, accuracy, and usability in decision-making. 
This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",0fca8065-8c52-4e44-96f8-333c6ab352ca,Scenario score (raw values)
50,KPI-TF-034,Topological action complexity,Task 4.1,Solution quality,O2,Power Grid,['Digital environment'],fully automated evaluation,Topological action complexity KPI quantifies the topological utilization of the grid and gives insights into how many topological actions are utilized: performing too complex or too many topology actions can indeed navigate the grid into topologies that are either unknown or hard to recover from for operators. ,"This KPI contributes to evaluating Solution quality of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ","This KPI yields a vector with 6 values, that are calculated over all scenarios’ steps: 
- The minimum, maximum and average number of topological actions performed by the AI assistant per timestamp, 
- The minimum, maximum and average share of modified buses per timestamp. ","Vector of 6 values expressed as: 
- Number (first 3 values), 
- Percent (decimal number between 0% and 100%, last 3 values). ",23,['Power Grid'],0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9344b9be-477f-48e5-ab4a-2726b1bb63ee,Solution quality,"This KPI contributes to evaluating Solution quality of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",ae7940c6-48e1-413a-a97e-b5eb6281e95b,Benchmark score (SUM of test scores),SUM,KPI-TF-034,KPI-TF-034: Topological action complexity (Power Grid),Topological action complexity KPI quantifies the topological utilization of the grid and gives insights into how many topological actions are utilized: performing too complex or too many topology actions can indeed navigate the grid into topologies that are either unknown or hard to recover from for operators. ,aecb9e43-a74d-4428-8e22-5def8785d852,Test score (SUM of scenario scores),SUM,62ab043e-b49e-4bc8-be82-9264dce095d7,Scenario 1 - Topological action complexity KPI quantifies the topological utilization of the grid and gives insights into how many topological actions are utilized: performing too complex or too many topology actions can indeed navigate the grid into topologies that are either unknown or hard to recover from for operators. ,"This KPI contributes to evaluating Solution quality of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",d54af6f3-445f-4a98-aabe-56882da7afe9,Scenario score (raw values)
51,KPI-TS-035,Total decision time,Task 4.1,Effectiveness,O2,Power Grid,"['Human machine interaction module', 'Digital environment']",semi-automated evaluation,"It is based on the overall time needed to decide, thus including the respective time taken by the AI assistant and human operator. This KPI can be detailed to specifically distinguish the time needed by the AI assistant to provide a recommendation. 
An assumption is that a Human Machine Interaction (HMI) module is available.  ","This KPI addresses the following objectives: 
1. Given an alert, how much time is left until the problem occurs? 
The longer the better since it gives more time to make a decision.   
2. Given an alert, how much time does the AI assistant take to come up with its recommendations to mitigate the issue? 
The shorter the better. 
3. Given the recommendations by the AI assistant, how much time does the human operator take to make a final decision? 
The shorter the better since it indicates that the recommendations are clear and convincing for the human operator. 
In case there is no interaction possible between the AI assistant and the human operator, this overall split is not possible. Then there is only one overall time needed to decide, starting from the alert and ending with the final decision by the human operator. 
This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",See KPI calculation methodology ,Time (minutes or seconds)  ,24,['Power Grid'],0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,575728ac-628b-416e-a98e-34ef7858169a,Effectiveness,"This KPI addresses the following objectives: 
1. Given an alert, how much time is left until the problem occurs? 
The longer the better since it gives more time to make a decision.   
2. Given an alert, how much time does the AI assistant take to come up with its recommendations to mitigate the issue? 
The shorter the better. 
3. Given the recommendations by the AI assistant, how much time does the human operator take to make a final decision? 
The shorter the better since it indicates that the recommendations are clear and convincing for the human operator. 
In case there is no interaction possible between the AI assistant and the human operator, this overall split is not possible. Then there is only one overall time needed to decide, starting from the alert and ending with the final decision by the human operator. 
This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",8c4641d6-bf1f-415f-b8cd-3c7145487741,Benchmark score (SUM of test scores),SUM,KPI-TS-035,KPI-TS-035: Total decision time (Power Grid),"It is based on the overall time needed to decide, thus including the respective time taken by the AI assistant and human operator. This KPI can be detailed to specifically distinguish the time needed by the AI assistant to provide a recommendation. 
An assumption is that a Human Machine Interaction (HMI) module is available.  ",b30a6639-dfc6-46ed-a9a9-12270785a0b7,Test score (SUM of scenario scores),SUM,d5813f31-914a-4fa0-8f5b-d3c16dcf313a,"Scenario 1 - It is based on the overall time needed to decide, thus including the respective time taken by the AI assistant and human operator. This KPI can be detailed to specifically distinguish the time needed by the AI assistant to provide a recommendation. 
An assumption is that a Human Machine Interaction (HMI) module is available.  ","This KPI addresses the following objectives: 
1. Given an alert, how much time is left until the problem occurs? 
The longer the better since it gives more time to make a decision.   
2. Given an alert, how much time does the AI assistant take to come up with its recommendations to mitigate the issue? 
The shorter the better. 
3. Given the recommendations by the AI assistant, how much time does the human operator take to make a final decision? 
The shorter the better since it indicates that the recommendations are clear and convincing for the human operator. 
In case there is no interaction possible between the AI assistant and the human operator, this overall split is not possible. Then there is only one overall time needed to decide, starting from the alert and ending with the final decision by the human operator. 
This KPI contributes to evaluating Effectiveness of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",0c638dea-4a34-499e-a47c-a537a36a698f,Scenario score (raw values)
52,KPI-OF-036,Operation score,Task 4.1,Solution quality,O2,Power Grid,['Digital environment'],fully automated evaluation,"The operation score KPI for operating a power grid includes the cost of a blackout, the cost of energy losses on the grid, and the cost of remedial actions. ","This KPI contributes to evaluating Solution quality of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. 

This KPI is linked with project’s Long Term Expected Impacts (LTEI): 
- (LTEI1)KPIS-1, 15%-20% reduction in renewable energy curtailment due to optimal exploration of network flexibility with AI (see “Sum of curtailed RES energy volumes”) 
- (LTEI1)KPIS-2, 20%-30% avoided electricity demand shedding (see “Sum of remaining energy to be supplied in case of blackout”) ","This KPI yields a vector with 8 values per episode: 
- Number topological actions performed by the AI assistant, 
- Number of redispatching actions (including but not limited to storage) performed by the AI assistant, 
- Sum of redispatched energy volumes, 
- Sum of balanced energy volumes, 
Note: this element is influenced by the actions implemented in the environment to compensate imbalances between loads and generations 
- Number of RES curtailment actions performed by the AI assistant, 
Such actions correspond to cases where the agent decreases generation from renewable energy sources (from what would be possible given the current weather) 
- Sum of curtailed RES energy volumes, 
- Sum of energy losses (estimated as difference between active power values of generations and loads), 
- Sum of remaining energy to be supplied in case of blackout. ","Vector of 8 values expressed as: 
- Number, 
- Number, 
- Energy in MWh, 
- Energy in MWh, 
- Number, 
- Energy in MWh, 
- Energy in MWh, 
- Energy in MWh. 
These values are expressed as raw values and will be possibly normalized during the evaluation to get fixed range values. ",25,['Power Grid'],0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9344b9be-477f-48e5-ab4a-2726b1bb63ee,Solution quality,"This KPI contributes to evaluating Solution quality of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. 

This KPI is linked with project’s Long Term Expected Impacts (LTEI): 
- (LTEI1)KPIS-1, 15%-20% reduction in renewable energy curtailment due to optimal exploration of network flexibility with AI (see “Sum of curtailed RES energy volumes”) 
- (LTEI1)KPIS-2, 20%-30% avoided electricity demand shedding (see “Sum of remaining energy to be supplied in case of blackout”) ",ae7940c6-48e1-413a-a97e-b5eb6281e95b,Benchmark score (SUM of test scores),SUM,KPI-OF-036,KPI-OF-036: Operation score (Power Grid),"The operation score KPI for operating a power grid includes the cost of a blackout, the cost of energy losses on the grid, and the cost of remedial actions. ",18636916-aa47-4561-a212-2d34d35217ce,Test score (SUM of scenario scores),SUM,b1d2684b-db35-4a86-85bf-3a6776c5b7f9,"Scenario 1 - The operation score KPI for operating a power grid includes the cost of a blackout, the cost of energy losses on the grid, and the cost of remedial actions. ","This KPI contributes to evaluating Solution quality of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. 

This KPI is linked with project’s Long Term Expected Impacts (LTEI): 
- (LTEI1)KPIS-1, 15%-20% reduction in renewable energy curtailment due to optimal exploration of network flexibility with AI (see “Sum of curtailed RES energy volumes”) 
- (LTEI1)KPIS-2, 20%-30% avoided electricity demand shedding (see “Sum of remaining energy to be supplied in case of blackout”) ",1798e034-43f5-4efe-82e3-3f43b8fa4f34,Scenario score (raw values)
53,KPI-TS-038,Trust in AI solutions score,Task 4.3,"AI acceptability, trust and trustworthiness",O2,Railway,['(non applicable)'],special evaluation setup,This KPI represents human operators’ self-reported trust (attitude) for individual AI-generated solutions measured with a questionnaire. ,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Trust in AI solutions score”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,26,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3767d0c2-279b-4c59-92d9-5734312b22fb,"AI acceptability, trust and trustworthiness","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Trust in AI solutions score”.  ",f16b1221-ed01-4e64-8d5c-7d673fc20829,Benchmark score (SUM of test scores),SUM,KPI-TS-038,KPI-TS-038: Trust in AI solutions score (Railway),This KPI represents human operators’ self-reported trust (attitude) for individual AI-generated solutions measured with a questionnaire. ,cfa02aac-3c2c-4ace-a35c-5956051fb5a3,Test score (SUM of scenario scores),SUM,8889ec79-ceaf-42a2-be43-4dc756fe7116,Scenario 1 - This KPI represents human operators’ self-reported trust (attitude) for individual AI-generated solutions measured with a questionnaire. ,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Trust in AI solutions score”.  ",1e25536f-37c0-47a0-8c6a-74cc54ebe7e9,Scenario score (raw values)
54,KPI-TS-038,Trust in AI solutions score,Task 4.3,"AI acceptability, trust and trustworthiness",O2,Power Grid,['(non applicable)'],special evaluation setup,This KPI represents human operators’ self-reported trust (attitude) for individual AI-generated solutions measured with a questionnaire. ,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Trust in AI solutions score”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,26,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3767d0c2-279b-4c59-92d9-5734312b22fb,"AI acceptability, trust and trustworthiness","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Trust in AI solutions score”.  ",f16b1221-ed01-4e64-8d5c-7d673fc20829,Benchmark score (SUM of test scores),SUM,KPI-TS-038,KPI-TS-038: Trust in AI solutions score (Power Grid),This KPI represents human operators’ self-reported trust (attitude) for individual AI-generated solutions measured with a questionnaire. ,cfa02aac-3c2c-4ace-a35c-5956051fb5a3,Test score (SUM of scenario scores),SUM,d72752de-c185-4d87-8854-dd0d89e2dece,Scenario 1 - This KPI represents human operators’ self-reported trust (attitude) for individual AI-generated solutions measured with a questionnaire. ,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Trust in AI solutions score”.  ",8ec10e45-3524-462f-b1cc-0bc183f81caa,Scenario score (raw values)
55,KPI-TS-038,Trust in AI solutions score,Task 4.3,"AI acceptability, trust and trustworthiness",O2,ATM,['(non applicable)'],special evaluation setup,This KPI represents human operators’ self-reported trust (attitude) for individual AI-generated solutions measured with a questionnaire. ,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Trust in AI solutions score”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,26,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3767d0c2-279b-4c59-92d9-5734312b22fb,"AI acceptability, trust and trustworthiness","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Trust in AI solutions score”.  ",f16b1221-ed01-4e64-8d5c-7d673fc20829,Benchmark score (SUM of test scores),SUM,KPI-TS-038,KPI-TS-038: Trust in AI solutions score (ATM),This KPI represents human operators’ self-reported trust (attitude) for individual AI-generated solutions measured with a questionnaire. ,cfa02aac-3c2c-4ace-a35c-5956051fb5a3,Test score (SUM of scenario scores),SUM,386fb6b7-32e2-4dc3-aa1a-d218eb8af9c9,Scenario 1 - This KPI represents human operators’ self-reported trust (attitude) for individual AI-generated solutions measured with a questionnaire. ,"This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Trust in AI solutions score”.  ",d1c7b171-1ac2-43b2-b748-1bfbe7afbb88,Scenario score (raw values)
56,KPI-TS-039,Trust towards the AI tool,Task 4.3,"AI acceptability, trust and trustworthiness",O2,Railway,['(non applicable)'],special evaluation setup,"(Dis)trust is defined here as a sentiment resulting from knowledge, beliefs, emotions, and other elements derived from lived or transmitted experience, which generates positive or negative expectations concerning the reactions of a system and the interaction with it (whether it is a question of another human being, an organization or a technology)” (Cahour & Forzy, 2009, p. 1261). ","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,Lickert-Scale or similar ,27,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3767d0c2-279b-4c59-92d9-5734312b22fb,"AI acceptability, trust and trustworthiness","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ",f16b1221-ed01-4e64-8d5c-7d673fc20829,Benchmark score (SUM of test scores),SUM,KPI-TS-039,KPI-TS-039: Trust towards the AI tool (Railway),"(Dis)trust is defined here as a sentiment resulting from knowledge, beliefs, emotions, and other elements derived from lived or transmitted experience, which generates positive or negative expectations concerning the reactions of a system and the interaction with it (whether it is a question of another human being, an organization or a technology)” (Cahour & Forzy, 2009, p. 1261). ",e0627e54-44ea-4cc1-9aef-76658ddbeebf,Test score (SUM of scenario scores),SUM,67f5dff2-5ab1-444c-9ee6-38ed0d5b4b7c,"Scenario 1 - (Dis)trust is defined here as a sentiment resulting from knowledge, beliefs, emotions, and other elements derived from lived or transmitted experience, which generates positive or negative expectations concerning the reactions of a system and the interaction with it (whether it is a question of another human being, an organization or a technology)” (Cahour & Forzy, 2009, p. 1261). ","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ",6b59d4a1-1093-49f6-a6ba-e91bd6dfecbb,Scenario score (raw values)
57,KPI-TS-039,Trust towards the AI tool,Task 4.3,"AI acceptability, trust and trustworthiness",O2,Power Grid,['(non applicable)'],special evaluation setup,"(Dis)trust is defined here as a sentiment resulting from knowledge, beliefs, emotions, and other elements derived from lived or transmitted experience, which generates positive or negative expectations concerning the reactions of a system and the interaction with it (whether it is a question of another human being, an organization or a technology)” (Cahour & Forzy, 2009, p. 1261). ","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,Lickert-Scale or similar ,27,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3767d0c2-279b-4c59-92d9-5734312b22fb,"AI acceptability, trust and trustworthiness","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ",f16b1221-ed01-4e64-8d5c-7d673fc20829,Benchmark score (SUM of test scores),SUM,KPI-TS-039,KPI-TS-039: Trust towards the AI tool (Power Grid),"(Dis)trust is defined here as a sentiment resulting from knowledge, beliefs, emotions, and other elements derived from lived or transmitted experience, which generates positive or negative expectations concerning the reactions of a system and the interaction with it (whether it is a question of another human being, an organization or a technology)” (Cahour & Forzy, 2009, p. 1261). ",e0627e54-44ea-4cc1-9aef-76658ddbeebf,Test score (SUM of scenario scores),SUM,df066353-f882-476c-bb3a-89636a9f0933,"Scenario 1 - (Dis)trust is defined here as a sentiment resulting from knowledge, beliefs, emotions, and other elements derived from lived or transmitted experience, which generates positive or negative expectations concerning the reactions of a system and the interaction with it (whether it is a question of another human being, an organization or a technology)” (Cahour & Forzy, 2009, p. 1261). ","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ",27892e9d-3a87-4cec-b579-adae071efaaf,Scenario score (raw values)
58,KPI-TS-039,Trust towards the AI tool,Task 4.3,"AI acceptability, trust and trustworthiness",O2,ATM,['(non applicable)'],special evaluation setup,"(Dis)trust is defined here as a sentiment resulting from knowledge, beliefs, emotions, and other elements derived from lived or transmitted experience, which generates positive or negative expectations concerning the reactions of a system and the interaction with it (whether it is a question of another human being, an organization or a technology)” (Cahour & Forzy, 2009, p. 1261). ","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1-5) ,Lickert-Scale or similar ,27,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3767d0c2-279b-4c59-92d9-5734312b22fb,"AI acceptability, trust and trustworthiness","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ",f16b1221-ed01-4e64-8d5c-7d673fc20829,Benchmark score (SUM of test scores),SUM,KPI-TS-039,KPI-TS-039: Trust towards the AI tool (ATM),"(Dis)trust is defined here as a sentiment resulting from knowledge, beliefs, emotions, and other elements derived from lived or transmitted experience, which generates positive or negative expectations concerning the reactions of a system and the interaction with it (whether it is a question of another human being, an organization or a technology)” (Cahour & Forzy, 2009, p. 1261). ",e0627e54-44ea-4cc1-9aef-76658ddbeebf,Test score (SUM of scenario scores),SUM,aabecd2c-9d79-41a1-b9b1-2cf7a40f82bd,"Scenario 1 - (Dis)trust is defined here as a sentiment resulting from knowledge, beliefs, emotions, and other elements derived from lived or transmitted experience, which generates positive or negative expectations concerning the reactions of a system and the interaction with it (whether it is a question of another human being, an organization or a technology)” (Cahour & Forzy, 2009, p. 1261). ","This KPI contributes to evaluating AI acceptability, trust and trustworthiness of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O2 main project objective. ",b237f5f7-22c1-4dfb-9142-93327353b066,Scenario score (raw values)
59,KPI-WS-040,Workload,Task 4.3,Human user experience,O3,Railway,['(non applicable)'],special evaluation setup,"Workload KPI is based on the workload assessment of human operators of the AI assistant. 
After each testing session using the system, the workload of human operators due to the AI assistant will be evaluated to understand in which scenarios (and depending on the AI level of support) it contributes for a higher workload. ","This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Impact on workload” KPI-IS-041. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ","It shall be determined according to the NASA-TLX methodology or similar. 
This KPI is still under analysis on how it will be implemented. If with a single manual questionnaire or with a pop-up in the dashboard. ",None (rating scale),28,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Impact on workload” KPI-IS-041. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-WS-040,KPI-WS-040: Workload (Railway),"Workload KPI is based on the workload assessment of human operators of the AI assistant. 
After each testing session using the system, the workload of human operators due to the AI assistant will be evaluated to understand in which scenarios (and depending on the AI level of support) it contributes for a higher workload. ",fe7e4123-e1db-45f2-b9aa-cc23abfa2cf8,Test score (SUM of scenario scores),SUM,06495534-c9ef-4b24-afec-715e01e33b7c,"Scenario 1 - Workload KPI is based on the workload assessment of human operators of the AI assistant. 
After each testing session using the system, the workload of human operators due to the AI assistant will be evaluated to understand in which scenarios (and depending on the AI level of support) it contributes for a higher workload. ","This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Impact on workload” KPI-IS-041. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",0c24da44-f5db-4cd7-a26b-651356c69304,Scenario score (raw values)
60,KPI-WS-040,Workload,Task 4.3,Human user experience,O3,Power Grid,['(non applicable)'],special evaluation setup,"Workload KPI is based on the workload assessment of human operators of the AI assistant. 
After each testing session using the system, the workload of human operators due to the AI assistant will be evaluated to understand in which scenarios (and depending on the AI level of support) it contributes for a higher workload. ","This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Impact on workload” KPI-IS-041. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ","It shall be determined according to the NASA-TLX methodology or similar. 
This KPI is still under analysis on how it will be implemented. If with a single manual questionnaire or with a pop-up in the dashboard. ",None (rating scale),28,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Impact on workload” KPI-IS-041. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-WS-040,KPI-WS-040: Workload (Power Grid),"Workload KPI is based on the workload assessment of human operators of the AI assistant. 
After each testing session using the system, the workload of human operators due to the AI assistant will be evaluated to understand in which scenarios (and depending on the AI level of support) it contributes for a higher workload. ",fe7e4123-e1db-45f2-b9aa-cc23abfa2cf8,Test score (SUM of scenario scores),SUM,f161097f-1574-4f6b-aa6a-b600acd4ef5b,"Scenario 1 - Workload KPI is based on the workload assessment of human operators of the AI assistant. 
After each testing session using the system, the workload of human operators due to the AI assistant will be evaluated to understand in which scenarios (and depending on the AI level of support) it contributes for a higher workload. ","This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Impact on workload” KPI-IS-041. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",064abac0-045f-4739-9266-6e69e1e86114,Scenario score (raw values)
61,KPI-WS-040,Workload,Task 4.3,Human user experience,O3,ATM,['(non applicable)'],special evaluation setup,"Workload KPI is based on the workload assessment of human operators of the AI assistant. 
After each testing session using the system, the workload of human operators due to the AI assistant will be evaluated to understand in which scenarios (and depending on the AI level of support) it contributes for a higher workload. ","This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Impact on workload” KPI-IS-041. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ","It shall be determined according to the NASA-TLX methodology or similar. 
This KPI is still under analysis on how it will be implemented. If with a single manual questionnaire or with a pop-up in the dashboard. ",None (rating scale),28,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Impact on workload” KPI-IS-041. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-WS-040,KPI-WS-040: Workload (ATM),"Workload KPI is based on the workload assessment of human operators of the AI assistant. 
After each testing session using the system, the workload of human operators due to the AI assistant will be evaluated to understand in which scenarios (and depending on the AI level of support) it contributes for a higher workload. ",fe7e4123-e1db-45f2-b9aa-cc23abfa2cf8,Test score (SUM of scenario scores),SUM,1c360326-f2d3-42d7-b429-47911b97bbe0,"Scenario 1 - Workload KPI is based on the workload assessment of human operators of the AI assistant. 
After each testing session using the system, the workload of human operators due to the AI assistant will be evaluated to understand in which scenarios (and depending on the AI level of support) it contributes for a higher workload. ","This KPI assesses whether the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Impact on workload” KPI-IS-041. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",5ba3f6d8-b88c-4071-8fd0-4894cb401ef9,Scenario score (raw values)
62,KPI-IS-041,Impact on workload,Task 4.3,AI-human task allocation balance,O3,Railway,['(non applicable)'],special evaluation setup,Impact on the workload KPI assesses operators’ perception of the system impact on their workload (either positive or negative)  ,"This KPI compares if the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Workload” KPI-WS-040. 
This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ","It is measured directly from user input using a 7-point Likert scale: 
- From 1 (Huge Increase in workload)  
- To 7 (Huge decrease of workload). 
This KPI is still under analysis on how it will be implemented. If with a single manual questionnaire or with a pop-up in the dashboard. ",Value between 1 and 7 ,29,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,00e19f42-d9b4-48dd-a818-1197cffe81e0,AI-human task allocation balance,"This KPI compares if the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Workload” KPI-WS-040. 
This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",f12fe8b1-29e0-4dd5-83c7-41bcb2055786,Benchmark score (SUM of test scores),SUM,KPI-IS-041,KPI-IS-041: Impact on workload (Railway),Impact on the workload KPI assesses operators’ perception of the system impact on their workload (either positive or negative)  ,aa8166ec-f59e-4575-a4af-c18810f282d5,Test score (SUM of scenario scores),SUM,905d61d8-6db0-4a0e-8017-bfa0469dcb6c,Scenario 1 - Impact on the workload KPI assesses operators’ perception of the system impact on their workload (either positive or negative)  ,"This KPI compares if the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Workload” KPI-WS-040. 
This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",f9140751-28fd-4fd4-8d65-3e39903b90fd,Scenario score (raw values)
63,KPI-IS-041,Impact on workload,Task 4.3,AI-human task allocation balance,O3,Power Grid,['(non applicable)'],special evaluation setup,Impact on the workload KPI assesses operators’ perception of the system impact on their workload (either positive or negative)  ,"This KPI compares if the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Workload” KPI-WS-040. 
This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ","It is measured directly from user input using a 7-point Likert scale: 
- From 1 (Huge Increase in workload)  
- To 7 (Huge decrease of workload). 
This KPI is still under analysis on how it will be implemented. If with a single manual questionnaire or with a pop-up in the dashboard. ",Value between 1 and 7 ,29,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,00e19f42-d9b4-48dd-a818-1197cffe81e0,AI-human task allocation balance,"This KPI compares if the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Workload” KPI-WS-040. 
This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",f12fe8b1-29e0-4dd5-83c7-41bcb2055786,Benchmark score (SUM of test scores),SUM,KPI-IS-041,KPI-IS-041: Impact on workload (Power Grid),Impact on the workload KPI assesses operators’ perception of the system impact on their workload (either positive or negative)  ,aa8166ec-f59e-4575-a4af-c18810f282d5,Test score (SUM of scenario scores),SUM,40e62790-fa23-4563-8e93-1d95950ebba3,Scenario 1 - Impact on the workload KPI assesses operators’ perception of the system impact on their workload (either positive or negative)  ,"This KPI compares if the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Workload” KPI-WS-040. 
This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",8299686b-5169-47f2-8605-82a33dad96a8,Scenario score (raw values)
64,KPI-IS-041,Impact on workload,Task 4.3,AI-human task allocation balance,O3,ATM,['(non applicable)'],special evaluation setup,Impact on the workload KPI assesses operators’ perception of the system impact on their workload (either positive or negative)  ,"This KPI compares if the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Workload” KPI-WS-040. 
This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ","It is measured directly from user input using a 7-point Likert scale: 
- From 1 (Huge Increase in workload)  
- To 7 (Huge decrease of workload). 
This KPI is still under analysis on how it will be implemented. If with a single manual questionnaire or with a pop-up in the dashboard. ",Value between 1 and 7 ,29,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,00e19f42-d9b4-48dd-a818-1197cffe81e0,AI-human task allocation balance,"This KPI compares if the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Workload” KPI-WS-040. 
This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",f12fe8b1-29e0-4dd5-83c7-41bcb2055786,Benchmark score (SUM of test scores),SUM,KPI-IS-041,KPI-IS-041: Impact on workload (ATM),Impact on the workload KPI assesses operators’ perception of the system impact on their workload (either positive or negative)  ,aa8166ec-f59e-4575-a4af-c18810f282d5,Test score (SUM of scenario scores),SUM,013f53fc-b9c2-4596-b48a-1abec0cdedd8,Scenario 1 - Impact on the workload KPI assesses operators’ perception of the system impact on their workload (either positive or negative)  ,"This KPI compares if the inputs of the operators are according to their real psychophysiology. This can act as a verification methodology but also support the AI to adapt. 
This KPI will be analyzed together with the “Workload” KPI-WS-040. 
This KPI contributes to evaluating AI-human task allocation balance of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",8e5c7892-8dba-4dd0-af73-38ac029b7acb,Scenario score (raw values)
65,KPI-NF-045,Network Impact Propagation,Task 4.1,Solution quality,O2,Railway,['Digital environment'],fully automated evaluation,"The Network Impact Propagation KPI measures how disruptions in one part of the railway network affect the overall system, including delay propagation and congestion spillover. This KPI helps evaluate the cascading effects of local disturbances and the efficiency of AI-assisted re-scheduling in mitigating these effects. ","This KPI contributes to evaluating Solution quality of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. 
- To assess the ripple effects of disruptions across the railway network. 
- To quantify how effectively AI-assisted re-scheduling contains and mitigates propagation of delays. 
- To support decision-making in optimizing re-scheduling strategies for network-wide efficiency. ",Number of trains affected (or Affected Network Nodes) divided by Total number of trains (or Total Network Nodes) ,Percentage (%) ,30,['Railway'],0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9344b9be-477f-48e5-ab4a-2726b1bb63ee,Solution quality,"This KPI contributes to evaluating Solution quality of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. 
- To assess the ripple effects of disruptions across the railway network. 
- To quantify how effectively AI-assisted re-scheduling contains and mitigates propagation of delays. 
- To support decision-making in optimizing re-scheduling strategies for network-wide efficiency. ",ae7940c6-48e1-413a-a97e-b5eb6281e95b,Benchmark score (SUM of test scores),SUM,KPI-NF-045,KPI-NF-045: Network Impact Propagation (Railway),"The Network Impact Propagation KPI measures how disruptions in one part of the railway network affect the overall system, including delay propagation and congestion spillover. This KPI helps evaluate the cascading effects of local disturbances and the efficiency of AI-assisted re-scheduling in mitigating these effects. ",3a0bdbcc-e73a-4041-a193-5af18c5a6d1d,Test score (SUM of scenario scores),SUM,490da2b0-9687-4baa-8d44-93dfbc6dd555,"Scenario 1 - The Network Impact Propagation KPI measures how disruptions in one part of the railway network affect the overall system, including delay propagation and congestion spillover. This KPI helps evaluate the cascading effects of local disturbances and the efficiency of AI-assisted re-scheduling in mitigating these effects. ","This KPI contributes to evaluating Solution quality of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. 
- To assess the ripple effects of disruptions across the railway network. 
- To quantify how effectively AI-assisted re-scheduling contains and mitigates propagation of delays. 
- To support decision-making in optimizing re-scheduling strategies for network-wide efficiency. ",969ec8a9-4b15-488a-8212-bb7b16f0be98,Scenario score (raw values)
66,KPI-CS-049,Cognitive Performance & Stress,Task 4.3,Human user experience,O3,Railway,['Human Assessment module'],semi-automated evaluation,Cognitive Performance & Stress KPI performs an implicit assessment of the human cognitive performance status and stress levels along the different task that will be performed. The output provides information about the operator mental status and aims to be used to integrate the AI system to contribute as a reward to better adapt decision system. ,"The computation of the metrics will be made on the Human Assessment Module and will be integrated in the system that will Tune the autonomy Level of the system. Taking this into account, the objective is to be able to tune the system autonomy level based on the implicit assessment in real time.  
For example, higher traffic or hard situations/decisions will be detected with any interference with the human operator, implicitly providing information to be used by the decision system. 
This KPI will not focus on the final results when this module is integrated, but in the calculation of personalized cognitive and stress metrics of a single human based on an individual assessment protocol. If we are not able to perform such protocol, then this module will be generic and not personalized, removing this KPIs. In the personalization we aim to achieve a 20-30% improvement on performance of the model based for a single individual data, enabling a high level of personalization. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",Performance of the model to measure cognitive status and stress of a single user.  ,Percentage (%) ,31,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"The computation of the metrics will be made on the Human Assessment Module and will be integrated in the system that will Tune the autonomy Level of the system. Taking this into account, the objective is to be able to tune the system autonomy level based on the implicit assessment in real time.  
For example, higher traffic or hard situations/decisions will be detected with any interference with the human operator, implicitly providing information to be used by the decision system. 
This KPI will not focus on the final results when this module is integrated, but in the calculation of personalized cognitive and stress metrics of a single human based on an individual assessment protocol. If we are not able to perform such protocol, then this module will be generic and not personalized, removing this KPIs. In the personalization we aim to achieve a 20-30% improvement on performance of the model based for a single individual data, enabling a high level of personalization. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-CS-049,KPI-CS-049: Cognitive Performance & Stress (Railway),Cognitive Performance & Stress KPI performs an implicit assessment of the human cognitive performance status and stress levels along the different task that will be performed. The output provides information about the operator mental status and aims to be used to integrate the AI system to contribute as a reward to better adapt decision system. ,92b34abc-9b31-4596-9a1f-4ffad49348e0,Test score (SUM of scenario scores),SUM,3f5fd1c5-ad5d-49f9-85ae-2f15db9d3d06,Scenario 1 - Cognitive Performance & Stress KPI performs an implicit assessment of the human cognitive performance status and stress levels along the different task that will be performed. The output provides information about the operator mental status and aims to be used to integrate the AI system to contribute as a reward to better adapt decision system. ,"The computation of the metrics will be made on the Human Assessment Module and will be integrated in the system that will Tune the autonomy Level of the system. Taking this into account, the objective is to be able to tune the system autonomy level based on the implicit assessment in real time.  
For example, higher traffic or hard situations/decisions will be detected with any interference with the human operator, implicitly providing information to be used by the decision system. 
This KPI will not focus on the final results when this module is integrated, but in the calculation of personalized cognitive and stress metrics of a single human based on an individual assessment protocol. If we are not able to perform such protocol, then this module will be generic and not personalized, removing this KPIs. In the personalization we aim to achieve a 20-30% improvement on performance of the model based for a single individual data, enabling a high level of personalization. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",2cfc9978-a574-4666-813a-63ae90cb01f4,Scenario score (raw values)
67,KPI-CS-049,Cognitive Performance & Stress,Task 4.3,Human user experience,O3,Power Grid,['Human Assessment module'],semi-automated evaluation,Cognitive Performance & Stress KPI performs an implicit assessment of the human cognitive performance status and stress levels along the different task that will be performed. The output provides information about the operator mental status and aims to be used to integrate the AI system to contribute as a reward to better adapt decision system. ,"The computation of the metrics will be made on the Human Assessment Module and will be integrated in the system that will Tune the autonomy Level of the system. Taking this into account, the objective is to be able to tune the system autonomy level based on the implicit assessment in real time.  
For example, higher traffic or hard situations/decisions will be detected with any interference with the human operator, implicitly providing information to be used by the decision system. 
This KPI will not focus on the final results when this module is integrated, but in the calculation of personalized cognitive and stress metrics of a single human based on an individual assessment protocol. If we are not able to perform such protocol, then this module will be generic and not personalized, removing this KPIs. In the personalization we aim to achieve a 20-30% improvement on performance of the model based for a single individual data, enabling a high level of personalization. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",Performance of the model to measure cognitive status and stress of a single user.  ,Percentage (%) ,31,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"The computation of the metrics will be made on the Human Assessment Module and will be integrated in the system that will Tune the autonomy Level of the system. Taking this into account, the objective is to be able to tune the system autonomy level based on the implicit assessment in real time.  
For example, higher traffic or hard situations/decisions will be detected with any interference with the human operator, implicitly providing information to be used by the decision system. 
This KPI will not focus on the final results when this module is integrated, but in the calculation of personalized cognitive and stress metrics of a single human based on an individual assessment protocol. If we are not able to perform such protocol, then this module will be generic and not personalized, removing this KPIs. In the personalization we aim to achieve a 20-30% improvement on performance of the model based for a single individual data, enabling a high level of personalization. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-CS-049,KPI-CS-049: Cognitive Performance & Stress (Power Grid),Cognitive Performance & Stress KPI performs an implicit assessment of the human cognitive performance status and stress levels along the different task that will be performed. The output provides information about the operator mental status and aims to be used to integrate the AI system to contribute as a reward to better adapt decision system. ,92b34abc-9b31-4596-9a1f-4ffad49348e0,Test score (SUM of scenario scores),SUM,24623e2d-2927-467d-b980-25dbb631c95e,Scenario 1 - Cognitive Performance & Stress KPI performs an implicit assessment of the human cognitive performance status and stress levels along the different task that will be performed. The output provides information about the operator mental status and aims to be used to integrate the AI system to contribute as a reward to better adapt decision system. ,"The computation of the metrics will be made on the Human Assessment Module and will be integrated in the system that will Tune the autonomy Level of the system. Taking this into account, the objective is to be able to tune the system autonomy level based on the implicit assessment in real time.  
For example, higher traffic or hard situations/decisions will be detected with any interference with the human operator, implicitly providing information to be used by the decision system. 
This KPI will not focus on the final results when this module is integrated, but in the calculation of personalized cognitive and stress metrics of a single human based on an individual assessment protocol. If we are not able to perform such protocol, then this module will be generic and not personalized, removing this KPIs. In the personalization we aim to achieve a 20-30% improvement on performance of the model based for a single individual data, enabling a high level of personalization. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",04e1ee43-bc36-4313-af63-9ab455d598d9,Scenario score (raw values)
68,KPI-CS-049,Cognitive Performance & Stress,Task 4.3,Human user experience,O3,ATM,['Human Assessment module'],semi-automated evaluation,Cognitive Performance & Stress KPI performs an implicit assessment of the human cognitive performance status and stress levels along the different task that will be performed. The output provides information about the operator mental status and aims to be used to integrate the AI system to contribute as a reward to better adapt decision system. ,"The computation of the metrics will be made on the Human Assessment Module and will be integrated in the system that will Tune the autonomy Level of the system. Taking this into account, the objective is to be able to tune the system autonomy level based on the implicit assessment in real time.  
For example, higher traffic or hard situations/decisions will be detected with any interference with the human operator, implicitly providing information to be used by the decision system. 
This KPI will not focus on the final results when this module is integrated, but in the calculation of personalized cognitive and stress metrics of a single human based on an individual assessment protocol. If we are not able to perform such protocol, then this module will be generic and not personalized, removing this KPIs. In the personalization we aim to achieve a 20-30% improvement on performance of the model based for a single individual data, enabling a high level of personalization. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",Performance of the model to measure cognitive status and stress of a single user.  ,Percentage (%) ,31,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,f0e50d4a-905b-4749-a875-bed72d49a5d7,Human user experience,"The computation of the metrics will be made on the Human Assessment Module and will be integrated in the system that will Tune the autonomy Level of the system. Taking this into account, the objective is to be able to tune the system autonomy level based on the implicit assessment in real time.  
For example, higher traffic or hard situations/decisions will be detected with any interference with the human operator, implicitly providing information to be used by the decision system. 
This KPI will not focus on the final results when this module is integrated, but in the calculation of personalized cognitive and stress metrics of a single human based on an individual assessment protocol. If we are not able to perform such protocol, then this module will be generic and not personalized, removing this KPIs. In the personalization we aim to achieve a 20-30% improvement on performance of the model based for a single individual data, enabling a high level of personalization. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",d927554a-1ac3-4a25-9b12-28d5530847be,Benchmark score (SUM of test scores),SUM,KPI-CS-049,KPI-CS-049: Cognitive Performance & Stress (ATM),Cognitive Performance & Stress KPI performs an implicit assessment of the human cognitive performance status and stress levels along the different task that will be performed. The output provides information about the operator mental status and aims to be used to integrate the AI system to contribute as a reward to better adapt decision system. ,92b34abc-9b31-4596-9a1f-4ffad49348e0,Test score (SUM of scenario scores),SUM,e7b7be0a-d75a-4567-b161-93b6a0589863,Scenario 1 - Cognitive Performance & Stress KPI performs an implicit assessment of the human cognitive performance status and stress levels along the different task that will be performed. The output provides information about the operator mental status and aims to be used to integrate the AI system to contribute as a reward to better adapt decision system. ,"The computation of the metrics will be made on the Human Assessment Module and will be integrated in the system that will Tune the autonomy Level of the system. Taking this into account, the objective is to be able to tune the system autonomy level based on the implicit assessment in real time.  
For example, higher traffic or hard situations/decisions will be detected with any interference with the human operator, implicitly providing information to be used by the decision system. 
This KPI will not focus on the final results when this module is integrated, but in the calculation of personalized cognitive and stress metrics of a single human based on an individual assessment protocol. If we are not able to perform such protocol, then this module will be generic and not personalized, removing this KPIs. In the personalization we aim to achieve a 20-30% improvement on performance of the model based for a single individual data, enabling a high level of personalization. 
This KPI contributes to evaluating Human-user experience of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",554d8868-af1a-4cea-baf1-b7b8874cdd58,Scenario score (raw values)
69,KPI-AF-050,AI-Agent Scalability Training,Task 4.1,Scalability,O2,Railway,['Prediction module'],fully automated evaluation,"AI-Agent Scalability Training measures the elapsed time required by an AI-agent to reach a predefined performance threshold. Time measured both as wallclock time (seconds) as well as steps or episodes according to the domain needs. The performance is defined by the native reward formulation defined by the digital environment or by domain experts. 
The time to threshold is measured across:  
(i) Different instance complexities; 
(ii) Different hardware availability. 
The performance threshold is set empirically and is defined by the cumulative reward formulation specific to the application domain. Note that the reward formulation used to train the agent may differ. For case (i), the type of hardware used should be logged to interpret the wallclock time measurements. ","This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ","Time taken to achieve a specific performance level during the training phase of an AI-agent, considering varying instance complexities and hardware availability ",Steps or Episodes and wall-clock time ,32,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,e84b389d-036a-4b30-8ef1-961a8e4ec74f,Scalability,"This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",3e8cea0d-adf5-4dc2-80fe-f6b34e6463ce,Benchmark score (SUM of test scores),SUM,KPI-AF-050,KPI-AF-050: AI-Agent Scalability Training (Railway),"AI-Agent Scalability Training measures the elapsed time required by an AI-agent to reach a predefined performance threshold. Time measured both as wallclock time (seconds) as well as steps or episodes according to the domain needs. The performance is defined by the native reward formulation defined by the digital environment or by domain experts. 
The time to threshold is measured across:  
(i) Different instance complexities; 
(ii) Different hardware availability. 
The performance threshold is set empirically and is defined by the cumulative reward formulation specific to the application domain. Note that the reward formulation used to train the agent may differ. For case (i), the type of hardware used should be logged to interpret the wallclock time measurements. ",c01f512f-4e7b-4edf-8acc-b64c6e6226d5,Test score (SUM of scenario scores),SUM,3784539b-d54e-43ea-91d9-1495457d794e,"Scenario 1 - AI-Agent Scalability Training measures the elapsed time required by an AI-agent to reach a predefined performance threshold. Time measured both as wallclock time (seconds) as well as steps or episodes according to the domain needs. The performance is defined by the native reward formulation defined by the digital environment or by domain experts. 
The time to threshold is measured across:  
(i) Different instance complexities; 
(ii) Different hardware availability. 
The performance threshold is set empirically and is defined by the cumulative reward formulation specific to the application domain. Note that the reward formulation used to train the agent may differ. For case (i), the type of hardware used should be logged to interpret the wallclock time measurements. ","This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",b1f6e2d7-1bac-4d3f-9135-0e5004fabc01,Scenario score (raw values)
70,KPI-AF-050,AI-Agent Scalability Training,Task 4.1,Scalability,O2,Power Grid,['Prediction module'],fully automated evaluation,"AI-Agent Scalability Training measures the elapsed time required by an AI-agent to reach a predefined performance threshold. Time measured both as wallclock time (seconds) as well as steps or episodes according to the domain needs. The performance is defined by the native reward formulation defined by the digital environment or by domain experts. 
The time to threshold is measured across:  
(i) Different instance complexities; 
(ii) Different hardware availability. 
The performance threshold is set empirically and is defined by the cumulative reward formulation specific to the application domain. Note that the reward formulation used to train the agent may differ. For case (i), the type of hardware used should be logged to interpret the wallclock time measurements. ","This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ","Time taken to achieve a specific performance level during the training phase of an AI-agent, considering varying instance complexities and hardware availability ",Steps or Episodes and wall-clock time ,32,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,e84b389d-036a-4b30-8ef1-961a8e4ec74f,Scalability,"This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",3e8cea0d-adf5-4dc2-80fe-f6b34e6463ce,Benchmark score (SUM of test scores),SUM,KPI-AF-050,KPI-AF-050: AI-Agent Scalability Training (Power Grid),"AI-Agent Scalability Training measures the elapsed time required by an AI-agent to reach a predefined performance threshold. Time measured both as wallclock time (seconds) as well as steps or episodes according to the domain needs. The performance is defined by the native reward formulation defined by the digital environment or by domain experts. 
The time to threshold is measured across:  
(i) Different instance complexities; 
(ii) Different hardware availability. 
The performance threshold is set empirically and is defined by the cumulative reward formulation specific to the application domain. Note that the reward formulation used to train the agent may differ. For case (i), the type of hardware used should be logged to interpret the wallclock time measurements. ",c01f512f-4e7b-4edf-8acc-b64c6e6226d5,Test score (SUM of scenario scores),SUM,650537b0-97e9-4665-bc92-d2123d6829d4,"Scenario 1 - AI-Agent Scalability Training measures the elapsed time required by an AI-agent to reach a predefined performance threshold. Time measured both as wallclock time (seconds) as well as steps or episodes according to the domain needs. The performance is defined by the native reward formulation defined by the digital environment or by domain experts. 
The time to threshold is measured across:  
(i) Different instance complexities; 
(ii) Different hardware availability. 
The performance threshold is set empirically and is defined by the cumulative reward formulation specific to the application domain. Note that the reward formulation used to train the agent may differ. For case (i), the type of hardware used should be logged to interpret the wallclock time measurements. ","This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",595c0ad6-d255-4bce-9140-bc9d988b6658,Scenario score (raw values)
71,KPI-AF-050,AI-Agent Scalability Training,Task 4.1,Scalability,O2,ATM,['Prediction module'],fully automated evaluation,"AI-Agent Scalability Training measures the elapsed time required by an AI-agent to reach a predefined performance threshold. Time measured both as wallclock time (seconds) as well as steps or episodes according to the domain needs. The performance is defined by the native reward formulation defined by the digital environment or by domain experts. 
The time to threshold is measured across:  
(i) Different instance complexities; 
(ii) Different hardware availability. 
The performance threshold is set empirically and is defined by the cumulative reward formulation specific to the application domain. Note that the reward formulation used to train the agent may differ. For case (i), the type of hardware used should be logged to interpret the wallclock time measurements. ","This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ","Time taken to achieve a specific performance level during the training phase of an AI-agent, considering varying instance complexities and hardware availability ",Steps or Episodes and wall-clock time ,32,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,e84b389d-036a-4b30-8ef1-961a8e4ec74f,Scalability,"This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",3e8cea0d-adf5-4dc2-80fe-f6b34e6463ce,Benchmark score (SUM of test scores),SUM,KPI-AF-050,KPI-AF-050: AI-Agent Scalability Training (ATM),"AI-Agent Scalability Training measures the elapsed time required by an AI-agent to reach a predefined performance threshold. Time measured both as wallclock time (seconds) as well as steps or episodes according to the domain needs. The performance is defined by the native reward formulation defined by the digital environment or by domain experts. 
The time to threshold is measured across:  
(i) Different instance complexities; 
(ii) Different hardware availability. 
The performance threshold is set empirically and is defined by the cumulative reward formulation specific to the application domain. Note that the reward formulation used to train the agent may differ. For case (i), the type of hardware used should be logged to interpret the wallclock time measurements. ",c01f512f-4e7b-4edf-8acc-b64c6e6226d5,Test score (SUM of scenario scores),SUM,dd04ce7b-e9a9-4cfe-985f-6facf776c5be,"Scenario 1 - AI-Agent Scalability Training measures the elapsed time required by an AI-agent to reach a predefined performance threshold. Time measured both as wallclock time (seconds) as well as steps or episodes according to the domain needs. The performance is defined by the native reward formulation defined by the digital environment or by domain experts. 
The time to threshold is measured across:  
(i) Different instance complexities; 
(ii) Different hardware availability. 
The performance threshold is set empirically and is defined by the cumulative reward formulation specific to the application domain. Note that the reward formulation used to train the agent may differ. For case (i), the type of hardware used should be logged to interpret the wallclock time measurements. ","This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",f27c386f-e60a-4ca3-ae16-c4a071692c86,Scenario score (raw values)
72,KPI-AF-051,AI-Agent Scalability Testing,Task 4.1,Scalability,O2,Railway,"['Recommendation module', 'Simulation engine']",fully automated evaluation,"Compare multiple trained agents, RL-based or not, based on the average inference time to sample one or multiple actions while increasing the complexity of the scenario analysed. Complexity is a domain-relevant concept that must be defined. ","This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",Inference time and performance of the trained AI agents as a function of instance complexity on standardized hardware.  ,"Time to be measured in seconds 
Performance to be measured using the environment native reward function or a suitably chosen use-case specific metric.  
Complexity to be defined in a use-case specific way, e.g., using a sequence of pre-defined scenarios increasing in complexity, such as increasing area, number of vehicles, nodes in the network.  ",33,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,e84b389d-036a-4b30-8ef1-961a8e4ec74f,Scalability,"This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",3e8cea0d-adf5-4dc2-80fe-f6b34e6463ce,Benchmark score (SUM of test scores),SUM,KPI-AF-051,KPI-AF-051: AI-Agent Scalability Testing (Railway),"Compare multiple trained agents, RL-based or not, based on the average inference time to sample one or multiple actions while increasing the complexity of the scenario analysed. Complexity is a domain-relevant concept that must be defined. ",837b3786-bdc8-4368-a9bf-2e33aaa91630,Test score (SUM of scenario scores),SUM,ae6d72df-be12-431b-b169-d08ccf6af20c,"Scenario 1 - Compare multiple trained agents, RL-based or not, based on the average inference time to sample one or multiple actions while increasing the complexity of the scenario analysed. Complexity is a domain-relevant concept that must be defined. ","This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",b6c84de8-83be-44ef-9d1c-4a91b07dfa12,Scenario score (raw values)
73,KPI-AF-051,AI-Agent Scalability Testing,Task 4.1,Scalability,O2,Power Grid,"['Recommendation module', 'Simulation engine']",fully automated evaluation,"Compare multiple trained agents, RL-based or not, based on the average inference time to sample one or multiple actions while increasing the complexity of the scenario analysed. Complexity is a domain-relevant concept that must be defined. ","This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",Inference time and performance of the trained AI agents as a function of instance complexity on standardized hardware.  ,"Time to be measured in seconds 
Performance to be measured using the environment native reward function or a suitably chosen use-case specific metric.  
Complexity to be defined in a use-case specific way, e.g., using a sequence of pre-defined scenarios increasing in complexity, such as increasing area, number of vehicles, nodes in the network.  ",33,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,e84b389d-036a-4b30-8ef1-961a8e4ec74f,Scalability,"This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",3e8cea0d-adf5-4dc2-80fe-f6b34e6463ce,Benchmark score (SUM of test scores),SUM,KPI-AF-051,KPI-AF-051: AI-Agent Scalability Testing (Power Grid),"Compare multiple trained agents, RL-based or not, based on the average inference time to sample one or multiple actions while increasing the complexity of the scenario analysed. Complexity is a domain-relevant concept that must be defined. ",837b3786-bdc8-4368-a9bf-2e33aaa91630,Test score (SUM of scenario scores),SUM,de21a35b-9654-43b8-ba69-2afe016a372d,"Scenario 1 - Compare multiple trained agents, RL-based or not, based on the average inference time to sample one or multiple actions while increasing the complexity of the scenario analysed. Complexity is a domain-relevant concept that must be defined. ","This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",348faafe-1050-4a59-b302-8f27eb126fce,Scenario score (raw values)
74,KPI-AF-051,AI-Agent Scalability Testing,Task 4.1,Scalability,O2,ATM,"['Recommendation module', 'Simulation engine']",fully automated evaluation,"Compare multiple trained agents, RL-based or not, based on the average inference time to sample one or multiple actions while increasing the complexity of the scenario analysed. Complexity is a domain-relevant concept that must be defined. ","This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",Inference time and performance of the trained AI agents as a function of instance complexity on standardized hardware.  ,"Time to be measured in seconds 
Performance to be measured using the environment native reward function or a suitably chosen use-case specific metric.  
Complexity to be defined in a use-case specific way, e.g., using a sequence of pre-defined scenarios increasing in complexity, such as increasing area, number of vehicles, nodes in the network.  ",33,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,e84b389d-036a-4b30-8ef1-961a8e4ec74f,Scalability,"This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",3e8cea0d-adf5-4dc2-80fe-f6b34e6463ce,Benchmark score (SUM of test scores),SUM,KPI-AF-051,KPI-AF-051: AI-Agent Scalability Testing (ATM),"Compare multiple trained agents, RL-based or not, based on the average inference time to sample one or multiple actions while increasing the complexity of the scenario analysed. Complexity is a domain-relevant concept that must be defined. ",837b3786-bdc8-4368-a9bf-2e33aaa91630,Test score (SUM of scenario scores),SUM,3cf2519f-5d8a-4d5a-bf5e-e8a1d3ece6fc,"Scenario 1 - Compare multiple trained agents, RL-based or not, based on the average inference time to sample one or multiple actions while increasing the complexity of the scenario analysed. Complexity is a domain-relevant concept that must be defined. ","This KPI contributes to evaluating Scalability of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",5a63e302-9ad1-44d4-a8cc-aa6d93cddcce,Scenario score (raw values)
75,KPI-DF-052,Domain shift adaptation time,Task 4.2,Reliability,O4,Railway,['Digital environment'],fully automated evaluation,The time or number of episodes required for the agent to regain a specific level of performance in the shifted domain after the domain shift has occurred. It can be used to evaluate how quickly an agent can adapt to new environmental conditions.  ,"Domain adaptation (DA) is a sub-field of transfer learning. DA can be defined as the capability to deploy a model trained in one or more source domains into a different target domain. We consider that the source and target domains have the same feature space. In this sense, it is important for RL based agents to have a reasonable adaptation time to a new domain which may present a slight shift from the source domain. However, the adaptation time should also consider the performance drop into its computation, as a high performance drop after the adaptation could not be tolerated.  
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",The adaptation time could be computed as the sum of episodes required for an agent to regain a specific level of performance in the shifted domain after the domain shift has occurred. ,"Time, number of time steps, number of episodes ",34,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"Domain adaptation (DA) is a sub-field of transfer learning. DA can be defined as the capability to deploy a model trained in one or more source domains into a different target domain. We consider that the source and target domains have the same feature space. In this sense, it is important for RL based agents to have a reasonable adaptation time to a new domain which may present a slight shift from the source domain. However, the adaptation time should also consider the performance drop into its computation, as a high performance drop after the adaptation could not be tolerated.  
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-052,KPI-DF-052: Domain shift adaptation time (Railway),The time or number of episodes required for the agent to regain a specific level of performance in the shifted domain after the domain shift has occurred. It can be used to evaluate how quickly an agent can adapt to new environmental conditions.  ,e66d9e9c-3cd8-4572-8bd4-0e9296e12950,Test score (SUM of scenario scores),SUM,c3e99ac3-37e1-45de-8fe9-3c0021e97a60,Scenario 1 - The time or number of episodes required for the agent to regain a specific level of performance in the shifted domain after the domain shift has occurred. It can be used to evaluate how quickly an agent can adapt to new environmental conditions.  ,"Domain adaptation (DA) is a sub-field of transfer learning. DA can be defined as the capability to deploy a model trained in one or more source domains into a different target domain. We consider that the source and target domains have the same feature space. In this sense, it is important for RL based agents to have a reasonable adaptation time to a new domain which may present a slight shift from the source domain. However, the adaptation time should also consider the performance drop into its computation, as a high performance drop after the adaptation could not be tolerated.  
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",a2d47484-3b2b-4a7a-880a-682e9df21565,Scenario score (raw values)
76,KPI-DF-052,Domain shift adaptation time,Task 4.2,Reliability,O4,Power Grid,['Digital environment'],fully automated evaluation,The time or number of episodes required for the agent to regain a specific level of performance in the shifted domain after the domain shift has occurred. It can be used to evaluate how quickly an agent can adapt to new environmental conditions.  ,"Domain adaptation (DA) is a sub-field of transfer learning. DA can be defined as the capability to deploy a model trained in one or more source domains into a different target domain. We consider that the source and target domains have the same feature space. In this sense, it is important for RL based agents to have a reasonable adaptation time to a new domain which may present a slight shift from the source domain. However, the adaptation time should also consider the performance drop into its computation, as a high performance drop after the adaptation could not be tolerated.  
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",The adaptation time could be computed as the sum of episodes required for an agent to regain a specific level of performance in the shifted domain after the domain shift has occurred. ,"Time, number of time steps, number of episodes ",34,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"Domain adaptation (DA) is a sub-field of transfer learning. DA can be defined as the capability to deploy a model trained in one or more source domains into a different target domain. We consider that the source and target domains have the same feature space. In this sense, it is important for RL based agents to have a reasonable adaptation time to a new domain which may present a slight shift from the source domain. However, the adaptation time should also consider the performance drop into its computation, as a high performance drop after the adaptation could not be tolerated.  
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-052,KPI-DF-052: Domain shift adaptation time (Power Grid),The time or number of episodes required for the agent to regain a specific level of performance in the shifted domain after the domain shift has occurred. It can be used to evaluate how quickly an agent can adapt to new environmental conditions.  ,e66d9e9c-3cd8-4572-8bd4-0e9296e12950,Test score (SUM of scenario scores),SUM,4f6b5365-f257-492c-b1fe-57f96df37bbd,Scenario 1 - The time or number of episodes required for the agent to regain a specific level of performance in the shifted domain after the domain shift has occurred. It can be used to evaluate how quickly an agent can adapt to new environmental conditions.  ,"Domain adaptation (DA) is a sub-field of transfer learning. DA can be defined as the capability to deploy a model trained in one or more source domains into a different target domain. We consider that the source and target domains have the same feature space. In this sense, it is important for RL based agents to have a reasonable adaptation time to a new domain which may present a slight shift from the source domain. However, the adaptation time should also consider the performance drop into its computation, as a high performance drop after the adaptation could not be tolerated.  
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",a0cc9752-4547-4fe4-88d1-61dab912d981,Scenario score (raw values)
77,KPI-DF-052,Domain shift adaptation time,Task 4.2,Reliability,O4,ATM,['Digital environment'],fully automated evaluation,The time or number of episodes required for the agent to regain a specific level of performance in the shifted domain after the domain shift has occurred. It can be used to evaluate how quickly an agent can adapt to new environmental conditions.  ,"Domain adaptation (DA) is a sub-field of transfer learning. DA can be defined as the capability to deploy a model trained in one or more source domains into a different target domain. We consider that the source and target domains have the same feature space. In this sense, it is important for RL based agents to have a reasonable adaptation time to a new domain which may present a slight shift from the source domain. However, the adaptation time should also consider the performance drop into its computation, as a high performance drop after the adaptation could not be tolerated.  
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",The adaptation time could be computed as the sum of episodes required for an agent to regain a specific level of performance in the shifted domain after the domain shift has occurred. ,"Time, number of time steps, number of episodes ",34,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"Domain adaptation (DA) is a sub-field of transfer learning. DA can be defined as the capability to deploy a model trained in one or more source domains into a different target domain. We consider that the source and target domains have the same feature space. In this sense, it is important for RL based agents to have a reasonable adaptation time to a new domain which may present a slight shift from the source domain. However, the adaptation time should also consider the performance drop into its computation, as a high performance drop after the adaptation could not be tolerated.  
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-052,KPI-DF-052: Domain shift adaptation time (ATM),The time or number of episodes required for the agent to regain a specific level of performance in the shifted domain after the domain shift has occurred. It can be used to evaluate how quickly an agent can adapt to new environmental conditions.  ,e66d9e9c-3cd8-4572-8bd4-0e9296e12950,Test score (SUM of scenario scores),SUM,86a68d1a-e3b5-40a4-9bb4-5f06f097f01b,Scenario 1 - The time or number of episodes required for the agent to regain a specific level of performance in the shifted domain after the domain shift has occurred. It can be used to evaluate how quickly an agent can adapt to new environmental conditions.  ,"Domain adaptation (DA) is a sub-field of transfer learning. DA can be defined as the capability to deploy a model trained in one or more source domains into a different target domain. We consider that the source and target domains have the same feature space. In this sense, it is important for RL based agents to have a reasonable adaptation time to a new domain which may present a slight shift from the source domain. However, the adaptation time should also consider the performance drop into its computation, as a high performance drop after the adaptation could not be tolerated.  
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",9539d6dd-f292-4fb3-9370-e5179e816f03,Scenario score (raw values)
78,KPI-DF-053,Domain shift generalization gap,Task 4.2,Reliability,O4,Railway,['Digital environment'],fully automated evaluation,"Domain shift – generalization gap evaluates the absolute difference between the performance (e.g., rewards) in the training domain and the shifted domain. This metrics quantifies the extent of performance loss due to domain shift. ","The objective is to verify to which extent the AI-based assistant performance deteriorates when the target domain presents some changes in comparison to the source domain. If an agent can retain the same performance expectations in shifted domain, it will be qualified as reliable. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Absolute difference between the rewards) in the training domain and the shifted domain,No units,35,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"The objective is to verify to which extent the AI-based assistant performance deteriorates when the target domain presents some changes in comparison to the source domain. If an agent can retain the same performance expectations in shifted domain, it will be qualified as reliable. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-053,KPI-DF-053: Domain shift generalization gap (Railway),"Domain shift – generalization gap evaluates the absolute difference between the performance (e.g., rewards) in the training domain and the shifted domain. This metrics quantifies the extent of performance loss due to domain shift. ",4d1ad4aa-37f7-47b7-92b2-1f64000f710f,Test score (SUM of scenario scores),SUM,3c29b8f6-c6d3-4733-b339-95ae5cd9f49b,"Scenario 1 - Domain shift – generalization gap evaluates the absolute difference between the performance (e.g., rewards) in the training domain and the shifted domain. This metrics quantifies the extent of performance loss due to domain shift. ","The objective is to verify to which extent the AI-based assistant performance deteriorates when the target domain presents some changes in comparison to the source domain. If an agent can retain the same performance expectations in shifted domain, it will be qualified as reliable. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",b6d0f296-7164-4979-941a-542c089b710b,Scenario score (raw values)
79,KPI-DF-053,Domain shift generalization gap,Task 4.2,Reliability,O4,Power Grid,['Digital environment'],fully automated evaluation,"Domain shift – generalization gap evaluates the absolute difference between the performance (e.g., rewards) in the training domain and the shifted domain. This metrics quantifies the extent of performance loss due to domain shift. ","The objective is to verify to which extent the AI-based assistant performance deteriorates when the target domain presents some changes in comparison to the source domain. If an agent can retain the same performance expectations in shifted domain, it will be qualified as reliable. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Absolute difference between the rewards) in the training domain and the shifted domain,No units,35,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"The objective is to verify to which extent the AI-based assistant performance deteriorates when the target domain presents some changes in comparison to the source domain. If an agent can retain the same performance expectations in shifted domain, it will be qualified as reliable. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-053,KPI-DF-053: Domain shift generalization gap (Power Grid),"Domain shift – generalization gap evaluates the absolute difference between the performance (e.g., rewards) in the training domain and the shifted domain. This metrics quantifies the extent of performance loss due to domain shift. ",4d1ad4aa-37f7-47b7-92b2-1f64000f710f,Test score (SUM of scenario scores),SUM,d09fc000-0db1-4160-a8df-6e4af7954c6e,"Scenario 1 - Domain shift – generalization gap evaluates the absolute difference between the performance (e.g., rewards) in the training domain and the shifted domain. This metrics quantifies the extent of performance loss due to domain shift. ","The objective is to verify to which extent the AI-based assistant performance deteriorates when the target domain presents some changes in comparison to the source domain. If an agent can retain the same performance expectations in shifted domain, it will be qualified as reliable. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",2c384ac0-7e21-4067-b645-adc679cbc0db,Scenario score (raw values)
80,KPI-DF-053,Domain shift generalization gap,Task 4.2,Reliability,O4,ATM,['Digital environment'],fully automated evaluation,"Domain shift – generalization gap evaluates the absolute difference between the performance (e.g., rewards) in the training domain and the shifted domain. This metrics quantifies the extent of performance loss due to domain shift. ","The objective is to verify to which extent the AI-based assistant performance deteriorates when the target domain presents some changes in comparison to the source domain. If an agent can retain the same performance expectations in shifted domain, it will be qualified as reliable. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Absolute difference between the rewards) in the training domain and the shifted domain,No units,35,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"The objective is to verify to which extent the AI-based assistant performance deteriorates when the target domain presents some changes in comparison to the source domain. If an agent can retain the same performance expectations in shifted domain, it will be qualified as reliable. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-053,KPI-DF-053: Domain shift generalization gap (ATM),"Domain shift – generalization gap evaluates the absolute difference between the performance (e.g., rewards) in the training domain and the shifted domain. This metrics quantifies the extent of performance loss due to domain shift. ",4d1ad4aa-37f7-47b7-92b2-1f64000f710f,Test score (SUM of scenario scores),SUM,a9fd4bdb-bf42-4825-a0d7-10eaa5b05a93,"Scenario 1 - Domain shift – generalization gap evaluates the absolute difference between the performance (e.g., rewards) in the training domain and the shifted domain. This metrics quantifies the extent of performance loss due to domain shift. ","The objective is to verify to which extent the AI-based assistant performance deteriorates when the target domain presents some changes in comparison to the source domain. If an agent can retain the same performance expectations in shifted domain, it will be qualified as reliable. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",1873fef1-f638-4620-bdd2-3d7744e5dac3,Scenario score (raw values)
81,KPI-DF-054,Domain shift out of domain detection accuracy,Task 4.2,Reliability,O4,Railway,['Digital environment'],fully automated evaluation,Domain shift – out of domain detection accuracy measures the accuracy with which the agent can detect whether it is operating in a domain that is different from the one it was trained on. It is useful for systems that need to switch strategies or request human intervention when a domain shift is detected. A recent paper proposed by Nasvytis et al. (2024) introduce various approaches for detection of OOD in RL. ,"It is crucial for an AI-based assistant to determine whether it can make reliable decisions in a given configuration. AI algorithms tend to be more dependable when they have been trained on similar configurations. Therefore, if the AI assistant can accurately detect out-of-domain configurations, it can seek human feedback to reduce uncertainty, leading to more adapted and reliable decisions in future scenarios. This KPI allows to determine if AI-based system could detect the shift before decision making. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","If a detection algorithm or tool is used, the accuracy of OOD detection is given by: 
(TP+TN)/( TP+TN+FP+FN)
This formula provides a measure of how well the agent can detect domain shifts, balancing both the correct identification of OOD and ID scenarios. It is essential for systems that need to adapt their strategies or seek human intervention when a domain shift is detected. 
Otherwise, compute a distribution-based distance (e.g. Wasserstein) between source and target domains and if this distance is greater than a predefined threshold, we can validate the hypothesis that there is a shift in the data.  ",Percentage (%) of correctly identified OOD cases ,36,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"It is crucial for an AI-based assistant to determine whether it can make reliable decisions in a given configuration. AI algorithms tend to be more dependable when they have been trained on similar configurations. Therefore, if the AI assistant can accurately detect out-of-domain configurations, it can seek human feedback to reduce uncertainty, leading to more adapted and reliable decisions in future scenarios. This KPI allows to determine if AI-based system could detect the shift before decision making. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-054,KPI-DF-054: Domain shift out of domain detection accuracy (Railway),Domain shift – out of domain detection accuracy measures the accuracy with which the agent can detect whether it is operating in a domain that is different from the one it was trained on. It is useful for systems that need to switch strategies or request human intervention when a domain shift is detected. A recent paper proposed by Nasvytis et al. (2024) introduce various approaches for detection of OOD in RL. ,9fa2c4be-86b9-41d3-ac93-6d2a22694214,Test score (SUM of scenario scores),SUM,2a02f508-ac87-416b-96f8-04613cd42251,Scenario 1 - Domain shift – out of domain detection accuracy measures the accuracy with which the agent can detect whether it is operating in a domain that is different from the one it was trained on. It is useful for systems that need to switch strategies or request human intervention when a domain shift is detected. A recent paper proposed by Nasvytis et al. (2024) introduce various approaches for detection of OOD in RL. ,"It is crucial for an AI-based assistant to determine whether it can make reliable decisions in a given configuration. AI algorithms tend to be more dependable when they have been trained on similar configurations. Therefore, if the AI assistant can accurately detect out-of-domain configurations, it can seek human feedback to reduce uncertainty, leading to more adapted and reliable decisions in future scenarios. This KPI allows to determine if AI-based system could detect the shift before decision making. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",dc2bd632-c27e-4661-b8f0-461158fdb5fb,Scenario score (raw values)
82,KPI-DF-054,Domain shift out of domain detection accuracy,Task 4.2,Reliability,O4,Power Grid,['Digital environment'],fully automated evaluation,Domain shift – out of domain detection accuracy measures the accuracy with which the agent can detect whether it is operating in a domain that is different from the one it was trained on. It is useful for systems that need to switch strategies or request human intervention when a domain shift is detected. A recent paper proposed by Nasvytis et al. (2024) introduce various approaches for detection of OOD in RL. ,"It is crucial for an AI-based assistant to determine whether it can make reliable decisions in a given configuration. AI algorithms tend to be more dependable when they have been trained on similar configurations. Therefore, if the AI assistant can accurately detect out-of-domain configurations, it can seek human feedback to reduce uncertainty, leading to more adapted and reliable decisions in future scenarios. This KPI allows to determine if AI-based system could detect the shift before decision making. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","If a detection algorithm or tool is used, the accuracy of OOD detection is given by: 
(TP+TN)/( TP+TN+FP+FN)
This formula provides a measure of how well the agent can detect domain shifts, balancing both the correct identification of OOD and ID scenarios. It is essential for systems that need to adapt their strategies or seek human intervention when a domain shift is detected. 
Otherwise, compute a distribution-based distance (e.g. Wasserstein) between source and target domains and if this distance is greater than a predefined threshold, we can validate the hypothesis that there is a shift in the data.  ",Percentage (%) of correctly identified OOD cases ,36,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"It is crucial for an AI-based assistant to determine whether it can make reliable decisions in a given configuration. AI algorithms tend to be more dependable when they have been trained on similar configurations. Therefore, if the AI assistant can accurately detect out-of-domain configurations, it can seek human feedback to reduce uncertainty, leading to more adapted and reliable decisions in future scenarios. This KPI allows to determine if AI-based system could detect the shift before decision making. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-054,KPI-DF-054: Domain shift out of domain detection accuracy (Power Grid),Domain shift – out of domain detection accuracy measures the accuracy with which the agent can detect whether it is operating in a domain that is different from the one it was trained on. It is useful for systems that need to switch strategies or request human intervention when a domain shift is detected. A recent paper proposed by Nasvytis et al. (2024) introduce various approaches for detection of OOD in RL. ,9fa2c4be-86b9-41d3-ac93-6d2a22694214,Test score (SUM of scenario scores),SUM,810f7353-32e7-4512-aa78-fc701b29ec03,Scenario 1 - Domain shift – out of domain detection accuracy measures the accuracy with which the agent can detect whether it is operating in a domain that is different from the one it was trained on. It is useful for systems that need to switch strategies or request human intervention when a domain shift is detected. A recent paper proposed by Nasvytis et al. (2024) introduce various approaches for detection of OOD in RL. ,"It is crucial for an AI-based assistant to determine whether it can make reliable decisions in a given configuration. AI algorithms tend to be more dependable when they have been trained on similar configurations. Therefore, if the AI assistant can accurately detect out-of-domain configurations, it can seek human feedback to reduce uncertainty, leading to more adapted and reliable decisions in future scenarios. This KPI allows to determine if AI-based system could detect the shift before decision making. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",4ba83eae-1210-4f94-856c-979ad6dc3cc6,Scenario score (raw values)
83,KPI-DF-054,Domain shift out of domain detection accuracy,Task 4.2,Reliability,O4,ATM,['Digital environment'],fully automated evaluation,Domain shift – out of domain detection accuracy measures the accuracy with which the agent can detect whether it is operating in a domain that is different from the one it was trained on. It is useful for systems that need to switch strategies or request human intervention when a domain shift is detected. A recent paper proposed by Nasvytis et al. (2024) introduce various approaches for detection of OOD in RL. ,"It is crucial for an AI-based assistant to determine whether it can make reliable decisions in a given configuration. AI algorithms tend to be more dependable when they have been trained on similar configurations. Therefore, if the AI assistant can accurately detect out-of-domain configurations, it can seek human feedback to reduce uncertainty, leading to more adapted and reliable decisions in future scenarios. This KPI allows to determine if AI-based system could detect the shift before decision making. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","If a detection algorithm or tool is used, the accuracy of OOD detection is given by: 
(TP+TN)/( TP+TN+FP+FN)
This formula provides a measure of how well the agent can detect domain shifts, balancing both the correct identification of OOD and ID scenarios. It is essential for systems that need to adapt their strategies or seek human intervention when a domain shift is detected. 
Otherwise, compute a distribution-based distance (e.g. Wasserstein) between source and target domains and if this distance is greater than a predefined threshold, we can validate the hypothesis that there is a shift in the data.  ",Percentage (%) of correctly identified OOD cases ,36,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"It is crucial for an AI-based assistant to determine whether it can make reliable decisions in a given configuration. AI algorithms tend to be more dependable when they have been trained on similar configurations. Therefore, if the AI assistant can accurately detect out-of-domain configurations, it can seek human feedback to reduce uncertainty, leading to more adapted and reliable decisions in future scenarios. This KPI allows to determine if AI-based system could detect the shift before decision making. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-054,KPI-DF-054: Domain shift out of domain detection accuracy (ATM),Domain shift – out of domain detection accuracy measures the accuracy with which the agent can detect whether it is operating in a domain that is different from the one it was trained on. It is useful for systems that need to switch strategies or request human intervention when a domain shift is detected. A recent paper proposed by Nasvytis et al. (2024) introduce various approaches for detection of OOD in RL. ,9fa2c4be-86b9-41d3-ac93-6d2a22694214,Test score (SUM of scenario scores),SUM,2d1f2ca7-bdb7-4afe-b9db-d85006910496,Scenario 1 - Domain shift – out of domain detection accuracy measures the accuracy with which the agent can detect whether it is operating in a domain that is different from the one it was trained on. It is useful for systems that need to switch strategies or request human intervention when a domain shift is detected. A recent paper proposed by Nasvytis et al. (2024) introduce various approaches for detection of OOD in RL. ,"It is crucial for an AI-based assistant to determine whether it can make reliable decisions in a given configuration. AI algorithms tend to be more dependable when they have been trained on similar configurations. Therefore, if the AI assistant can accurately detect out-of-domain configurations, it can seek human feedback to reduce uncertainty, leading to more adapted and reliable decisions in future scenarios. This KPI allows to determine if AI-based system could detect the shift before decision making. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",7477753b-cad6-4f85-9a42-8a11ec2ac457,Scenario score (raw values)
84,KPI-DF-055,Domain shift policy robustness,Task 4.2,Reliability,O4,Railway,['Digital environment'],fully automated evaluation,"Domain shift – Policy robustness KPI calculates a ratio of the performance in the shifted domain to the performance in the original domain. A score close to 1 indicates high robustness, while a lower score indicates reduced performance due to the domain shift. It can be used to assess the generalization of a policy learned in a simulated environment when applied to a real-world scenario. ","To evaluate the robustness and generalization capability of a policy by measuring its performance ratio between a shifted domain and the original domain, ensuring that policies trained in simulated environments maintain high effectiveness when applied to real-world scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","If we present by Rshift the performance or reward obtained in shifted domain and by Roriginal the performance or reward in the source domain, the ratio is computed as: Rshift/Roriginal",,37,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"To evaluate the robustness and generalization capability of a policy by measuring its performance ratio between a shifted domain and the original domain, ensuring that policies trained in simulated environments maintain high effectiveness when applied to real-world scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-055,KPI-DF-055: Domain shift policy robustness (Railway),"Domain shift – Policy robustness KPI calculates a ratio of the performance in the shifted domain to the performance in the original domain. A score close to 1 indicates high robustness, while a lower score indicates reduced performance due to the domain shift. It can be used to assess the generalization of a policy learned in a simulated environment when applied to a real-world scenario. ",2eb18f9d-2cf9-45c4-941d-5c2a0f69dcff,Test score (SUM of scenario scores),SUM,a497974e-a61a-47c8-840c-817557940ad3,"Scenario 1 - Domain shift – Policy robustness KPI calculates a ratio of the performance in the shifted domain to the performance in the original domain. A score close to 1 indicates high robustness, while a lower score indicates reduced performance due to the domain shift. It can be used to assess the generalization of a policy learned in a simulated environment when applied to a real-world scenario. ","To evaluate the robustness and generalization capability of a policy by measuring its performance ratio between a shifted domain and the original domain, ensuring that policies trained in simulated environments maintain high effectiveness when applied to real-world scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",c1f7d31a-01a8-46c6-8272-e68ca8b9fbe1,Scenario score (raw values)
85,KPI-DF-055,Domain shift policy robustness,Task 4.2,Reliability,O4,Power Grid,['Digital environment'],fully automated evaluation,"Domain shift – Policy robustness KPI calculates a ratio of the performance in the shifted domain to the performance in the original domain. A score close to 1 indicates high robustness, while a lower score indicates reduced performance due to the domain shift. It can be used to assess the generalization of a policy learned in a simulated environment when applied to a real-world scenario. ","To evaluate the robustness and generalization capability of a policy by measuring its performance ratio between a shifted domain and the original domain, ensuring that policies trained in simulated environments maintain high effectiveness when applied to real-world scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","If we present by Rshift the performance or reward obtained in shifted domain and by Roriginal the performance or reward in the source domain, the ratio is computed as: Rshift/Roriginal",,37,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"To evaluate the robustness and generalization capability of a policy by measuring its performance ratio between a shifted domain and the original domain, ensuring that policies trained in simulated environments maintain high effectiveness when applied to real-world scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-055,KPI-DF-055: Domain shift policy robustness (Power Grid),"Domain shift – Policy robustness KPI calculates a ratio of the performance in the shifted domain to the performance in the original domain. A score close to 1 indicates high robustness, while a lower score indicates reduced performance due to the domain shift. It can be used to assess the generalization of a policy learned in a simulated environment when applied to a real-world scenario. ",2eb18f9d-2cf9-45c4-941d-5c2a0f69dcff,Test score (SUM of scenario scores),SUM,edeaa2cf-d903-4472-b8a0-18163e693148,"Scenario 1 - Domain shift – Policy robustness KPI calculates a ratio of the performance in the shifted domain to the performance in the original domain. A score close to 1 indicates high robustness, while a lower score indicates reduced performance due to the domain shift. It can be used to assess the generalization of a policy learned in a simulated environment when applied to a real-world scenario. ","To evaluate the robustness and generalization capability of a policy by measuring its performance ratio between a shifted domain and the original domain, ensuring that policies trained in simulated environments maintain high effectiveness when applied to real-world scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",69d813e7-7594-4356-9e9b-1e84601f466c,Scenario score (raw values)
86,KPI-DF-055,Domain shift policy robustness,Task 4.2,Reliability,O4,ATM,['Digital environment'],fully automated evaluation,"Domain shift – Policy robustness KPI calculates a ratio of the performance in the shifted domain to the performance in the original domain. A score close to 1 indicates high robustness, while a lower score indicates reduced performance due to the domain shift. It can be used to assess the generalization of a policy learned in a simulated environment when applied to a real-world scenario. ","To evaluate the robustness and generalization capability of a policy by measuring its performance ratio between a shifted domain and the original domain, ensuring that policies trained in simulated environments maintain high effectiveness when applied to real-world scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","If we present by Rshift the performance or reward obtained in shifted domain and by Roriginal the performance or reward in the source domain, the ratio is computed as: Rshift/Roriginal",,37,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"To evaluate the robustness and generalization capability of a policy by measuring its performance ratio between a shifted domain and the original domain, ensuring that policies trained in simulated environments maintain high effectiveness when applied to real-world scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-055,KPI-DF-055: Domain shift policy robustness (ATM),"Domain shift – Policy robustness KPI calculates a ratio of the performance in the shifted domain to the performance in the original domain. A score close to 1 indicates high robustness, while a lower score indicates reduced performance due to the domain shift. It can be used to assess the generalization of a policy learned in a simulated environment when applied to a real-world scenario. ",2eb18f9d-2cf9-45c4-941d-5c2a0f69dcff,Test score (SUM of scenario scores),SUM,7a9aab7f-d01a-41db-aa2c-8de5dd18c7b0,"Scenario 1 - Domain shift – Policy robustness KPI calculates a ratio of the performance in the shifted domain to the performance in the original domain. A score close to 1 indicates high robustness, while a lower score indicates reduced performance due to the domain shift. It can be used to assess the generalization of a policy learned in a simulated environment when applied to a real-world scenario. ","To evaluate the robustness and generalization capability of a policy by measuring its performance ratio between a shifted domain and the original domain, ensuring that policies trained in simulated environments maintain high effectiveness when applied to real-world scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",8b53b422-f1f7-4a88-bfaf-e8b7cd9e5b24,Scenario score (raw values)
87,KPI-DF-056,Domain shift robustness to domain parameters,Task 4.2,Reliability,O4,Railway,['Digital environment'],fully automated evaluation,"Robustness to domain parameters KPI evaluates the sensitivity of the agent’s performance (e.g., Reward) to changes in specific domain parameters (e.g., generators type including renewables in power grid domain). It helps to identify which environmental factors most affect the agent’s performance. ","To assess the sensitivity of the agent's performance to variations in domain parameters, identifying key environmental factors that significantly impact the agent’s effectiveness and robustness, thereby guiding improvements in adaptability and resilience across different scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","Calculating the variance or standard deviation of the rewards obtained by the agent after introducing changes in the source domain and comparing it to the standard deviation before the changes, can provide insights into the robustness of the agent's performance under varying domain parameters. 
To formalize the definition, let:  
- Rbefore​ represent the rewards obtained by the agent before introducing changes. 
- Rafter​ represent the rewards obtained after introducing changes. 
- σbefore​ be the standard deviation of Rbefore
- σafter be the standard deviation of  Rafter
- Δσ be the difference between the two standard deviations. 
The formula to quantify the change in variability due to domain changes is: 
Δσ =σafter−σbefore",,38,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"To assess the sensitivity of the agent's performance to variations in domain parameters, identifying key environmental factors that significantly impact the agent’s effectiveness and robustness, thereby guiding improvements in adaptability and resilience across different scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-056,KPI-DF-056: Domain shift robustness to domain parameters (Railway),"Robustness to domain parameters KPI evaluates the sensitivity of the agent’s performance (e.g., Reward) to changes in specific domain parameters (e.g., generators type including renewables in power grid domain). It helps to identify which environmental factors most affect the agent’s performance. ",26313136-8b07-4798-bf1a-7e212a86f2bc,Test score (SUM of scenario scores),SUM,84e84c04-25d8-4662-954a-610ff47c053d,"Scenario 1 - Robustness to domain parameters KPI evaluates the sensitivity of the agent’s performance (e.g., Reward) to changes in specific domain parameters (e.g., generators type including renewables in power grid domain). It helps to identify which environmental factors most affect the agent’s performance. ","To assess the sensitivity of the agent's performance to variations in domain parameters, identifying key environmental factors that significantly impact the agent’s effectiveness and robustness, thereby guiding improvements in adaptability and resilience across different scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",d0feec1d-4782-485d-b741-0a7e99d42f05,Scenario score (raw values)
88,KPI-DF-056,Domain shift robustness to domain parameters,Task 4.2,Reliability,O4,Power Grid,['Digital environment'],fully automated evaluation,"Robustness to domain parameters KPI evaluates the sensitivity of the agent’s performance (e.g., Reward) to changes in specific domain parameters (e.g., generators type including renewables in power grid domain). It helps to identify which environmental factors most affect the agent’s performance. ","To assess the sensitivity of the agent's performance to variations in domain parameters, identifying key environmental factors that significantly impact the agent’s effectiveness and robustness, thereby guiding improvements in adaptability and resilience across different scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","Calculating the variance or standard deviation of the rewards obtained by the agent after introducing changes in the source domain and comparing it to the standard deviation before the changes, can provide insights into the robustness of the agent's performance under varying domain parameters. 
To formalize the definition, let:  
- Rbefore​ represent the rewards obtained by the agent before introducing changes. 
- Rafter​ represent the rewards obtained after introducing changes. 
- σbefore​ be the standard deviation of Rbefore
- σafter be the standard deviation of  Rafter
- Δσ be the difference between the two standard deviations. 
The formula to quantify the change in variability due to domain changes is: 
Δσ =σafter−σbefore",,38,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"To assess the sensitivity of the agent's performance to variations in domain parameters, identifying key environmental factors that significantly impact the agent’s effectiveness and robustness, thereby guiding improvements in adaptability and resilience across different scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-056,KPI-DF-056: Domain shift robustness to domain parameters (Power Grid),"Robustness to domain parameters KPI evaluates the sensitivity of the agent’s performance (e.g., Reward) to changes in specific domain parameters (e.g., generators type including renewables in power grid domain). It helps to identify which environmental factors most affect the agent’s performance. ",26313136-8b07-4798-bf1a-7e212a86f2bc,Test score (SUM of scenario scores),SUM,9332fbfa-f179-4faf-b867-c6ebed0da5f6,"Scenario 1 - Robustness to domain parameters KPI evaluates the sensitivity of the agent’s performance (e.g., Reward) to changes in specific domain parameters (e.g., generators type including renewables in power grid domain). It helps to identify which environmental factors most affect the agent’s performance. ","To assess the sensitivity of the agent's performance to variations in domain parameters, identifying key environmental factors that significantly impact the agent’s effectiveness and robustness, thereby guiding improvements in adaptability and resilience across different scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",92cfc3d3-bb04-4e08-a0ab-e1e5375873bd,Scenario score (raw values)
89,KPI-DF-056,Domain shift robustness to domain parameters,Task 4.2,Reliability,O4,ATM,['Digital environment'],fully automated evaluation,"Robustness to domain parameters KPI evaluates the sensitivity of the agent’s performance (e.g., Reward) to changes in specific domain parameters (e.g., generators type including renewables in power grid domain). It helps to identify which environmental factors most affect the agent’s performance. ","To assess the sensitivity of the agent's performance to variations in domain parameters, identifying key environmental factors that significantly impact the agent’s effectiveness and robustness, thereby guiding improvements in adaptability and resilience across different scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","Calculating the variance or standard deviation of the rewards obtained by the agent after introducing changes in the source domain and comparing it to the standard deviation before the changes, can provide insights into the robustness of the agent's performance under varying domain parameters. 
To formalize the definition, let:  
- Rbefore​ represent the rewards obtained by the agent before introducing changes. 
- Rafter​ represent the rewards obtained after introducing changes. 
- σbefore​ be the standard deviation of Rbefore
- σafter be the standard deviation of  Rafter
- Δσ be the difference between the two standard deviations. 
The formula to quantify the change in variability due to domain changes is: 
Δσ =σafter−σbefore",,38,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"To assess the sensitivity of the agent's performance to variations in domain parameters, identifying key environmental factors that significantly impact the agent’s effectiveness and robustness, thereby guiding improvements in adaptability and resilience across different scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-056,KPI-DF-056: Domain shift robustness to domain parameters (ATM),"Robustness to domain parameters KPI evaluates the sensitivity of the agent’s performance (e.g., Reward) to changes in specific domain parameters (e.g., generators type including renewables in power grid domain). It helps to identify which environmental factors most affect the agent’s performance. ",26313136-8b07-4798-bf1a-7e212a86f2bc,Test score (SUM of scenario scores),SUM,cfeb3c08-6244-4f6e-88a6-04726dbad49e,"Scenario 1 - Robustness to domain parameters KPI evaluates the sensitivity of the agent’s performance (e.g., Reward) to changes in specific domain parameters (e.g., generators type including renewables in power grid domain). It helps to identify which environmental factors most affect the agent’s performance. ","To assess the sensitivity of the agent's performance to variations in domain parameters, identifying key environmental factors that significantly impact the agent’s effectiveness and robustness, thereby guiding improvements in adaptability and resilience across different scenarios. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",80cc8236-6537-40bd-9dc9-7f808c9d26c9,Scenario score (raw values)
90,KPI-DF-057,Domain shift success rate drop,Task 4.2,Reliability,O4,Railway,['Digital environment'],fully automated evaluation,Domain shift – success rate drop KPI measures drop in the performance of the agent after the occurrence of a shift in the source domain. ,"To quantify the decline in the agent's performance after a shift in the source domain, providing insights into the agent's ability to maintain effectiveness under altered conditions. This KPI helps in evaluating the agent's resilience, adaptability, and the robustness of its training, facilitating the identification of weaknesses and the development of strategies to improve its performance in dynamic or unpredictable environments. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","A formula to quantify the drop in performance of the agent after a domain shift could be: 
Performance drop=Roriginal−RshiftedRoriginal
Where the R could be a performance metric of the AI-based agent like the cumulated Reward. This formula yields a ratio representing the relative drop in performance, with a higher value indicating a more significant drop due to the domain shift. ",,39,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"To quantify the decline in the agent's performance after a shift in the source domain, providing insights into the agent's ability to maintain effectiveness under altered conditions. This KPI helps in evaluating the agent's resilience, adaptability, and the robustness of its training, facilitating the identification of weaknesses and the development of strategies to improve its performance in dynamic or unpredictable environments. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-057,KPI-DF-057: Domain shift success rate drop (Railway),Domain shift – success rate drop KPI measures drop in the performance of the agent after the occurrence of a shift in the source domain. ,7bf122fa-8edd-4e6f-a397-23ec6089b853,Test score (SUM of scenario scores),SUM,437aaf8a-0cf4-4a13-bcf0-a5e18ac891d3,Scenario 1 - Domain shift – success rate drop KPI measures drop in the performance of the agent after the occurrence of a shift in the source domain. ,"To quantify the decline in the agent's performance after a shift in the source domain, providing insights into the agent's ability to maintain effectiveness under altered conditions. This KPI helps in evaluating the agent's resilience, adaptability, and the robustness of its training, facilitating the identification of weaknesses and the development of strategies to improve its performance in dynamic or unpredictable environments. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",3435870f-e3cc-441d-9967-76d56f0dc750,Scenario score (raw values)
91,KPI-DF-057,Domain shift success rate drop,Task 4.2,Reliability,O4,Power Grid,['Digital environment'],fully automated evaluation,Domain shift – success rate drop KPI measures drop in the performance of the agent after the occurrence of a shift in the source domain. ,"To quantify the decline in the agent's performance after a shift in the source domain, providing insights into the agent's ability to maintain effectiveness under altered conditions. This KPI helps in evaluating the agent's resilience, adaptability, and the robustness of its training, facilitating the identification of weaknesses and the development of strategies to improve its performance in dynamic or unpredictable environments. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","A formula to quantify the drop in performance of the agent after a domain shift could be: 
Performance drop=Roriginal−RshiftedRoriginal
Where the R could be a performance metric of the AI-based agent like the cumulated Reward. This formula yields a ratio representing the relative drop in performance, with a higher value indicating a more significant drop due to the domain shift. ",,39,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"To quantify the decline in the agent's performance after a shift in the source domain, providing insights into the agent's ability to maintain effectiveness under altered conditions. This KPI helps in evaluating the agent's resilience, adaptability, and the robustness of its training, facilitating the identification of weaknesses and the development of strategies to improve its performance in dynamic or unpredictable environments. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-057,KPI-DF-057: Domain shift success rate drop (Power Grid),Domain shift – success rate drop KPI measures drop in the performance of the agent after the occurrence of a shift in the source domain. ,7bf122fa-8edd-4e6f-a397-23ec6089b853,Test score (SUM of scenario scores),SUM,39c82175-841a-4565-9ec0-8689814668fb,Scenario 1 - Domain shift – success rate drop KPI measures drop in the performance of the agent after the occurrence of a shift in the source domain. ,"To quantify the decline in the agent's performance after a shift in the source domain, providing insights into the agent's ability to maintain effectiveness under altered conditions. This KPI helps in evaluating the agent's resilience, adaptability, and the robustness of its training, facilitating the identification of weaknesses and the development of strategies to improve its performance in dynamic or unpredictable environments. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",c04131b3-1e89-4b8d-840a-0a97b149dba9,Scenario score (raw values)
92,KPI-DF-057,Domain shift success rate drop,Task 4.2,Reliability,O4,ATM,['Digital environment'],fully automated evaluation,Domain shift – success rate drop KPI measures drop in the performance of the agent after the occurrence of a shift in the source domain. ,"To quantify the decline in the agent's performance after a shift in the source domain, providing insights into the agent's ability to maintain effectiveness under altered conditions. This KPI helps in evaluating the agent's resilience, adaptability, and the robustness of its training, facilitating the identification of weaknesses and the development of strategies to improve its performance in dynamic or unpredictable environments. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","A formula to quantify the drop in performance of the agent after a domain shift could be: 
Performance drop=Roriginal−RshiftedRoriginal
Where the R could be a performance metric of the AI-based agent like the cumulated Reward. This formula yields a ratio representing the relative drop in performance, with a higher value indicating a more significant drop due to the domain shift. ",,39,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"To quantify the decline in the agent's performance after a shift in the source domain, providing insights into the agent's ability to maintain effectiveness under altered conditions. This KPI helps in evaluating the agent's resilience, adaptability, and the robustness of its training, facilitating the identification of weaknesses and the development of strategies to improve its performance in dynamic or unpredictable environments. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-057,KPI-DF-057: Domain shift success rate drop (ATM),Domain shift – success rate drop KPI measures drop in the performance of the agent after the occurrence of a shift in the source domain. ,7bf122fa-8edd-4e6f-a397-23ec6089b853,Test score (SUM of scenario scores),SUM,c5bb4095-73f1-49b4-822b-66deb530d935,Scenario 1 - Domain shift – success rate drop KPI measures drop in the performance of the agent after the occurrence of a shift in the source domain. ,"To quantify the decline in the agent's performance after a shift in the source domain, providing insights into the agent's ability to maintain effectiveness under altered conditions. This KPI helps in evaluating the agent's resilience, adaptability, and the robustness of its training, facilitating the identification of weaknesses and the development of strategies to improve its performance in dynamic or unpredictable environments. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",b6ee50d9-35fa-4a0b-8156-dcb09dfa34f1,Scenario score (raw values)
93,KPI-RS-058,Robustness to operator input,Task 4.2,Robustness,O4,Railway,"['Recommendation module', 'Simulation engine', 'Digital environment']",semi-automated evaluation,"The KPI should measure or evaluate how the trained agent behaves in terms of robustness if, during the decision-making process where a human operator makes the final decisions, a human operator occasionally intervenes and significantly overrides the autonomous decisions of the trained agent. 
For agents trained using machine learning methods, this can cause an offset between the type of states encountered in the training data and during deployment, especially for agents trained using reinforcement learning or similar methods where the agent itself decides which actions to execute. As a consequence of this offset, the agent might make poorer decisions if the human operator does not always follow the proposed actions of the agents. 
To measure how sensitive the agent is to such offsets, this KPI proposes to use a “simulated operator” that does not fully follow the course of actions suggested by the agents, and instead overwrites certain action variables set by the agents in a fraction of time steps. ","Overall, this KPI contributes to evaluating Robustness of the AI-based assistant when dealing with real-world conditions, as part of Task 4.2 evaluation objectives, and O4 main project objective. 
The KPI is related to Tasks 3.1 and 3.3. Specifically, it is related to goal (4) of Task 3.1 (“Analysis of the impact of human intervention in the decision process on AI agents developed and trained towards fully autonomous behavior”), goal (1) of Task 3.3 (“Develop and expand order-agnostic network architectures adapted to the RL setting to use human-data or human-like perturbations and ensure the system can also make good decisions in the context where actions are partially chosen by the human partner”) and goal (2) of Task 3.4 (“Detect risks early on and potentially inform human supervisors, e.g. relinquish control to a human supervisor or transition into “safety mode” when necessary”).   ","A simulated operator is defined that deviates from the agent's suggestions in a certain percentage of time steps. If agents have to set multiple variables, this deviation can concern only certain variables. The simulated operator can be based on logged data, or in the absence of such data can be a random agent. The performance of the primary AI agent (e.g., environment native reward function) is then measured in the precence of these deviations.","Environment reward, or the unit of measurement of a suitably-chosen use-case specific metric.  ",40,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"Overall, this KPI contributes to evaluating Robustness of the AI-based assistant when dealing with real-world conditions, as part of Task 4.2 evaluation objectives, and O4 main project objective. 
The KPI is related to Tasks 3.1 and 3.3. Specifically, it is related to goal (4) of Task 3.1 (“Analysis of the impact of human intervention in the decision process on AI agents developed and trained towards fully autonomous behavior”), goal (1) of Task 3.3 (“Develop and expand order-agnostic network architectures adapted to the RL setting to use human-data or human-like perturbations and ensure the system can also make good decisions in the context where actions are partially chosen by the human partner”) and goal (2) of Task 3.4 (“Detect risks early on and potentially inform human supervisors, e.g. relinquish control to a human supervisor or transition into “safety mode” when necessary”).   ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-RS-058,KPI-RS-058: Robustness to operator input (Railway),"The KPI should measure or evaluate how the trained agent behaves in terms of robustness if, during the decision-making process where a human operator makes the final decisions, a human operator occasionally intervenes and significantly overrides the autonomous decisions of the trained agent. 
For agents trained using machine learning methods, this can cause an offset between the type of states encountered in the training data and during deployment, especially for agents trained using reinforcement learning or similar methods where the agent itself decides which actions to execute. As a consequence of this offset, the agent might make poorer decisions if the human operator does not always follow the proposed actions of the agents. 
To measure how sensitive the agent is to such offsets, this KPI proposes to use a “simulated operator” that does not fully follow the course of actions suggested by the agents, and instead overwrites certain action variables set by the agents in a fraction of time steps. ",17629ed8-6b89-4c0d-97b4-174993d1bd43,Test score (SUM of scenario scores),SUM,0fe0552a-2711-40e9-8c7f-2f828a09b253,"Scenario 1 - The KPI should measure or evaluate how the trained agent behaves in terms of robustness if, during the decision-making process where a human operator makes the final decisions, a human operator occasionally intervenes and significantly overrides the autonomous decisions of the trained agent. 
For agents trained using machine learning methods, this can cause an offset between the type of states encountered in the training data and during deployment, especially for agents trained using reinforcement learning or similar methods where the agent itself decides which actions to execute. As a consequence of this offset, the agent might make poorer decisions if the human operator does not always follow the proposed actions of the agents. 
To measure how sensitive the agent is to such offsets, this KPI proposes to use a “simulated operator” that does not fully follow the course of actions suggested by the agents, and instead overwrites certain action variables set by the agents in a fraction of time steps. ","Overall, this KPI contributes to evaluating Robustness of the AI-based assistant when dealing with real-world conditions, as part of Task 4.2 evaluation objectives, and O4 main project objective. 
The KPI is related to Tasks 3.1 and 3.3. Specifically, it is related to goal (4) of Task 3.1 (“Analysis of the impact of human intervention in the decision process on AI agents developed and trained towards fully autonomous behavior”), goal (1) of Task 3.3 (“Develop and expand order-agnostic network architectures adapted to the RL setting to use human-data or human-like perturbations and ensure the system can also make good decisions in the context where actions are partially chosen by the human partner”) and goal (2) of Task 3.4 (“Detect risks early on and potentially inform human supervisors, e.g. relinquish control to a human supervisor or transition into “safety mode” when necessary”).   ",98aac2cf-6440-4684-8990-fbf11df26f4a,Scenario score (raw values)
94,KPI-RS-058,Robustness to operator input,Task 4.2,Robustness,O4,Power Grid,"['Recommendation module', 'Simulation engine', 'Digital environment']",semi-automated evaluation,"The KPI should measure or evaluate how the trained agent behaves in terms of robustness if, during the decision-making process where a human operator makes the final decisions, a human operator occasionally intervenes and significantly overrides the autonomous decisions of the trained agent. 
For agents trained using machine learning methods, this can cause an offset between the type of states encountered in the training data and during deployment, especially for agents trained using reinforcement learning or similar methods where the agent itself decides which actions to execute. As a consequence of this offset, the agent might make poorer decisions if the human operator does not always follow the proposed actions of the agents. 
To measure how sensitive the agent is to such offsets, this KPI proposes to use a “simulated operator” that does not fully follow the course of actions suggested by the agents, and instead overwrites certain action variables set by the agents in a fraction of time steps. ","Overall, this KPI contributes to evaluating Robustness of the AI-based assistant when dealing with real-world conditions, as part of Task 4.2 evaluation objectives, and O4 main project objective. 
The KPI is related to Tasks 3.1 and 3.3. Specifically, it is related to goal (4) of Task 3.1 (“Analysis of the impact of human intervention in the decision process on AI agents developed and trained towards fully autonomous behavior”), goal (1) of Task 3.3 (“Develop and expand order-agnostic network architectures adapted to the RL setting to use human-data or human-like perturbations and ensure the system can also make good decisions in the context where actions are partially chosen by the human partner”) and goal (2) of Task 3.4 (“Detect risks early on and potentially inform human supervisors, e.g. relinquish control to a human supervisor or transition into “safety mode” when necessary”).   ","A simulated operator is defined that deviates from the agent's suggestions in a certain percentage of time steps. If agents have to set multiple variables, this deviation can concern only certain variables. The simulated operator can be based on logged data, or in the absence of such data can be a random agent. The performance of the primary AI agent (e.g., environment native reward function) is then measured in the precence of these deviations.","Environment reward, or the unit of measurement of a suitably-chosen use-case specific metric.  ",40,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"Overall, this KPI contributes to evaluating Robustness of the AI-based assistant when dealing with real-world conditions, as part of Task 4.2 evaluation objectives, and O4 main project objective. 
The KPI is related to Tasks 3.1 and 3.3. Specifically, it is related to goal (4) of Task 3.1 (“Analysis of the impact of human intervention in the decision process on AI agents developed and trained towards fully autonomous behavior”), goal (1) of Task 3.3 (“Develop and expand order-agnostic network architectures adapted to the RL setting to use human-data or human-like perturbations and ensure the system can also make good decisions in the context where actions are partially chosen by the human partner”) and goal (2) of Task 3.4 (“Detect risks early on and potentially inform human supervisors, e.g. relinquish control to a human supervisor or transition into “safety mode” when necessary”).   ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-RS-058,KPI-RS-058: Robustness to operator input (Power Grid),"The KPI should measure or evaluate how the trained agent behaves in terms of robustness if, during the decision-making process where a human operator makes the final decisions, a human operator occasionally intervenes and significantly overrides the autonomous decisions of the trained agent. 
For agents trained using machine learning methods, this can cause an offset between the type of states encountered in the training data and during deployment, especially for agents trained using reinforcement learning or similar methods where the agent itself decides which actions to execute. As a consequence of this offset, the agent might make poorer decisions if the human operator does not always follow the proposed actions of the agents. 
To measure how sensitive the agent is to such offsets, this KPI proposes to use a “simulated operator” that does not fully follow the course of actions suggested by the agents, and instead overwrites certain action variables set by the agents in a fraction of time steps. ",17629ed8-6b89-4c0d-97b4-174993d1bd43,Test score (SUM of scenario scores),SUM,df5f16dc-6bdf-40fa-838d-277082153095,"Scenario 1 - The KPI should measure or evaluate how the trained agent behaves in terms of robustness if, during the decision-making process where a human operator makes the final decisions, a human operator occasionally intervenes and significantly overrides the autonomous decisions of the trained agent. 
For agents trained using machine learning methods, this can cause an offset between the type of states encountered in the training data and during deployment, especially for agents trained using reinforcement learning or similar methods where the agent itself decides which actions to execute. As a consequence of this offset, the agent might make poorer decisions if the human operator does not always follow the proposed actions of the agents. 
To measure how sensitive the agent is to such offsets, this KPI proposes to use a “simulated operator” that does not fully follow the course of actions suggested by the agents, and instead overwrites certain action variables set by the agents in a fraction of time steps. ","Overall, this KPI contributes to evaluating Robustness of the AI-based assistant when dealing with real-world conditions, as part of Task 4.2 evaluation objectives, and O4 main project objective. 
The KPI is related to Tasks 3.1 and 3.3. Specifically, it is related to goal (4) of Task 3.1 (“Analysis of the impact of human intervention in the decision process on AI agents developed and trained towards fully autonomous behavior”), goal (1) of Task 3.3 (“Develop and expand order-agnostic network architectures adapted to the RL setting to use human-data or human-like perturbations and ensure the system can also make good decisions in the context where actions are partially chosen by the human partner”) and goal (2) of Task 3.4 (“Detect risks early on and potentially inform human supervisors, e.g. relinquish control to a human supervisor or transition into “safety mode” when necessary”).   ",a6fe687d-e719-40bc-bb3a-d6d2bd13a4c6,Scenario score (raw values)
95,KPI-RS-058,Robustness to operator input,Task 4.2,Robustness,O4,ATM,"['Recommendation module', 'Simulation engine', 'Digital environment']",semi-automated evaluation,"The KPI should measure or evaluate how the trained agent behaves in terms of robustness if, during the decision-making process where a human operator makes the final decisions, a human operator occasionally intervenes and significantly overrides the autonomous decisions of the trained agent. 
For agents trained using machine learning methods, this can cause an offset between the type of states encountered in the training data and during deployment, especially for agents trained using reinforcement learning or similar methods where the agent itself decides which actions to execute. As a consequence of this offset, the agent might make poorer decisions if the human operator does not always follow the proposed actions of the agents. 
To measure how sensitive the agent is to such offsets, this KPI proposes to use a “simulated operator” that does not fully follow the course of actions suggested by the agents, and instead overwrites certain action variables set by the agents in a fraction of time steps. ","Overall, this KPI contributes to evaluating Robustness of the AI-based assistant when dealing with real-world conditions, as part of Task 4.2 evaluation objectives, and O4 main project objective. 
The KPI is related to Tasks 3.1 and 3.3. Specifically, it is related to goal (4) of Task 3.1 (“Analysis of the impact of human intervention in the decision process on AI agents developed and trained towards fully autonomous behavior”), goal (1) of Task 3.3 (“Develop and expand order-agnostic network architectures adapted to the RL setting to use human-data or human-like perturbations and ensure the system can also make good decisions in the context where actions are partially chosen by the human partner”) and goal (2) of Task 3.4 (“Detect risks early on and potentially inform human supervisors, e.g. relinquish control to a human supervisor or transition into “safety mode” when necessary”).   ","A simulated operator is defined that deviates from the agent's suggestions in a certain percentage of time steps. If agents have to set multiple variables, this deviation can concern only certain variables. The simulated operator can be based on logged data, or in the absence of such data can be a random agent. The performance of the primary AI agent (e.g., environment native reward function) is then measured in the precence of these deviations.","Environment reward, or the unit of measurement of a suitably-chosen use-case specific metric.  ",40,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"Overall, this KPI contributes to evaluating Robustness of the AI-based assistant when dealing with real-world conditions, as part of Task 4.2 evaluation objectives, and O4 main project objective. 
The KPI is related to Tasks 3.1 and 3.3. Specifically, it is related to goal (4) of Task 3.1 (“Analysis of the impact of human intervention in the decision process on AI agents developed and trained towards fully autonomous behavior”), goal (1) of Task 3.3 (“Develop and expand order-agnostic network architectures adapted to the RL setting to use human-data or human-like perturbations and ensure the system can also make good decisions in the context where actions are partially chosen by the human partner”) and goal (2) of Task 3.4 (“Detect risks early on and potentially inform human supervisors, e.g. relinquish control to a human supervisor or transition into “safety mode” when necessary”).   ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-RS-058,KPI-RS-058: Robustness to operator input (ATM),"The KPI should measure or evaluate how the trained agent behaves in terms of robustness if, during the decision-making process where a human operator makes the final decisions, a human operator occasionally intervenes and significantly overrides the autonomous decisions of the trained agent. 
For agents trained using machine learning methods, this can cause an offset between the type of states encountered in the training data and during deployment, especially for agents trained using reinforcement learning or similar methods where the agent itself decides which actions to execute. As a consequence of this offset, the agent might make poorer decisions if the human operator does not always follow the proposed actions of the agents. 
To measure how sensitive the agent is to such offsets, this KPI proposes to use a “simulated operator” that does not fully follow the course of actions suggested by the agents, and instead overwrites certain action variables set by the agents in a fraction of time steps. ",17629ed8-6b89-4c0d-97b4-174993d1bd43,Test score (SUM of scenario scores),SUM,51d389bb-02c7-4911-9573-fd4307c33173,"Scenario 1 - The KPI should measure or evaluate how the trained agent behaves in terms of robustness if, during the decision-making process where a human operator makes the final decisions, a human operator occasionally intervenes and significantly overrides the autonomous decisions of the trained agent. 
For agents trained using machine learning methods, this can cause an offset between the type of states encountered in the training data and during deployment, especially for agents trained using reinforcement learning or similar methods where the agent itself decides which actions to execute. As a consequence of this offset, the agent might make poorer decisions if the human operator does not always follow the proposed actions of the agents. 
To measure how sensitive the agent is to such offsets, this KPI proposes to use a “simulated operator” that does not fully follow the course of actions suggested by the agents, and instead overwrites certain action variables set by the agents in a fraction of time steps. ","Overall, this KPI contributes to evaluating Robustness of the AI-based assistant when dealing with real-world conditions, as part of Task 4.2 evaluation objectives, and O4 main project objective. 
The KPI is related to Tasks 3.1 and 3.3. Specifically, it is related to goal (4) of Task 3.1 (“Analysis of the impact of human intervention in the decision process on AI agents developed and trained towards fully autonomous behavior”), goal (1) of Task 3.3 (“Develop and expand order-agnostic network architectures adapted to the RL setting to use human-data or human-like perturbations and ensure the system can also make good decisions in the context where actions are partially chosen by the human partner”) and goal (2) of Task 3.4 (“Detect risks early on and potentially inform human supervisors, e.g. relinquish control to a human supervisor or transition into “safety mode” when necessary”).   ",509c1483-b174-4ac7-8cf3-a04b900c930f,Scenario score (raw values)
96,KPI-AS-068,Assistant adaptation to user preferences,Task 4.1,Solution quality,O2,Power Grid,['Recommendation module'],semi-automated evaluation,"Assistant adaptation to user preferences assesses how the AI assistant adapts to operator’s choices and preferences. 
The assistant provides several recommendations which represent different trade-offs of different objectives, and the operator eventually makes one single choice. 
This KPI assume that an estimation of epistemic uncertainty is calculated for each action recommendation, which can be used later by the human to select the action in a multi-objective setting. 
This KPIs thus aims at measuring: 
- Whether the choice that the operator makes is in the set of recommendations proposed by the assistant, 
- How is the recommendation chosen by the operator ranked compared to the other ones, 
- Whether the recommendation chosen by the operator has a high epistemic uncertainty compared to the other recommendations. ","This KPI contributes to evaluating Solution quality of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ","See calculation steps: for this KPI, raw values are given as lists to allow different possible summary calculations.","Vector with 6 values without units, for each step:
- the lowest epistemic uncertainty of recommendations 
- the highest epistemic uncertainty of recommendations
- the epistemic uncertainty of the recommendation chosen by the operator
- the rank of the recommendation chosen by the operator
- the total number of proposed recommendations
- whether the choice that the operator makes is in the set of recommendations proposed by the assistant",41,['Power Grid'],0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9344b9be-477f-48e5-ab4a-2726b1bb63ee,Solution quality,"This KPI contributes to evaluating Solution quality of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",ae7940c6-48e1-413a-a97e-b5eb6281e95b,Benchmark score (SUM of test scores),SUM,KPI-AS-068,KPI-AS-068: Assistant adaptation to user preferences (Power Grid),"Assistant adaptation to user preferences assesses how the AI assistant adapts to operator’s choices and preferences. 
The assistant provides several recommendations which represent different trade-offs of different objectives, and the operator eventually makes one single choice. 
This KPI assume that an estimation of epistemic uncertainty is calculated for each action recommendation, which can be used later by the human to select the action in a multi-objective setting. 
This KPIs thus aims at measuring: 
- Whether the choice that the operator makes is in the set of recommendations proposed by the assistant, 
- How is the recommendation chosen by the operator ranked compared to the other ones, 
- Whether the recommendation chosen by the operator has a high epistemic uncertainty compared to the other recommendations. ",6ae72957-0bbd-4921-b430-d3c0d5c280c1,Test score (SUM of scenario scores),SUM,632a1138-b54f-4b3d-99bf-320db5b2868e,"Scenario 1 - Assistant adaptation to user preferences assesses how the AI assistant adapts to operator’s choices and preferences. 
The assistant provides several recommendations which represent different trade-offs of different objectives, and the operator eventually makes one single choice. 
This KPI assume that an estimation of epistemic uncertainty is calculated for each action recommendation, which can be used later by the human to select the action in a multi-objective setting. 
This KPIs thus aims at measuring: 
- Whether the choice that the operator makes is in the set of recommendations proposed by the assistant, 
- How is the recommendation chosen by the operator ranked compared to the other ones, 
- Whether the recommendation chosen by the operator has a high epistemic uncertainty compared to the other recommendations. ","This KPI contributes to evaluating Solution quality of the AI-based assistant, as part of Task 4.1 evaluation objectives, and O2 main project objective. ",4fb5b1c9-4240-40f9-bfa6-68a45bfffa6a,Scenario score (raw values)
97,KPI-DF-069,Drop-off in reward,Task 4.2,Robustness,O4,Railway,['Digital environment'],fully automated evaluation,Drop-off in reward calculates difference in reward between situation with perfect information and imperfect information either through natural malfunctions while measuring data or through intentional perturbations by an attacker. ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Total reward obtained with perfect information - Total reward obtained with imperfect information,Same unit as reward or percentage of reward with perfect information ,42,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-DF-069,KPI-DF-069: Drop-off in reward (Railway),Drop-off in reward calculates difference in reward between situation with perfect information and imperfect information either through natural malfunctions while measuring data or through intentional perturbations by an attacker. ,ddcf645a-7bcf-4394-a1c3-967d62c2558f,Test score (SUM of scenario scores),SUM,39cddceb-ae3d-4d7f-ad9a-d325892d25b9,Scenario 1 - Drop-off in reward calculates difference in reward between situation with perfect information and imperfect information either through natural malfunctions while measuring data or through intentional perturbations by an attacker. ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",295d5a45-502d-48be-9a2f-6d8126aa2e17,Scenario score (raw values)
98,KPI-DF-069,Drop-off in reward,Task 4.2,Robustness,O4,Power Grid,['Digital environment'],fully automated evaluation,Drop-off in reward calculates difference in reward between situation with perfect information and imperfect information either through natural malfunctions while measuring data or through intentional perturbations by an attacker. ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Total reward obtained with perfect information - Total reward obtained with imperfect information,Same unit as reward or percentage of reward with perfect information ,42,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-DF-069,KPI-DF-069: Drop-off in reward (Power Grid),Drop-off in reward calculates difference in reward between situation with perfect information and imperfect information either through natural malfunctions while measuring data or through intentional perturbations by an attacker. ,ddcf645a-7bcf-4394-a1c3-967d62c2558f,Test score (SUM of scenario scores),SUM,4db7a545-4fb8-48fb-87ad-5e08ac985a27,Scenario 1 - Drop-off in reward calculates difference in reward between situation with perfect information and imperfect information either through natural malfunctions while measuring data or through intentional perturbations by an attacker. ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",0e529a9e-3525-40af-80ea-0e0e18eefb1c,Scenario score (raw values)
99,KPI-DF-069,Drop-off in reward,Task 4.2,Robustness,O4,ATM,['Digital environment'],fully automated evaluation,Drop-off in reward calculates difference in reward between situation with perfect information and imperfect information either through natural malfunctions while measuring data or through intentional perturbations by an attacker. ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Total reward obtained with perfect information - Total reward obtained with imperfect information,Same unit as reward or percentage of reward with perfect information ,42,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-DF-069,KPI-DF-069: Drop-off in reward (ATM),Drop-off in reward calculates difference in reward between situation with perfect information and imperfect information either through natural malfunctions while measuring data or through intentional perturbations by an attacker. ,ddcf645a-7bcf-4394-a1c3-967d62c2558f,Test score (SUM of scenario scores),SUM,d4a85232-cd44-4f55-8756-2e2f8a229ce6,Scenario 1 - Drop-off in reward calculates difference in reward between situation with perfect information and imperfect information either through natural malfunctions while measuring data or through intentional perturbations by an attacker. ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",3028e713-7203-4f84-a380-e373e8fdf7d0,Scenario score (raw values)
100,KPI-FF-070,Frequency changed output AI agent,Task 4.2,Robustness,O4,Railway,['Digital environment'],fully automated evaluation,Frequency changed output AI agent calculates the number of times the output of the AI agent (i.e. the action the agent chooses) is changed due to perturbations ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective.  ","While running the environment feed the AI agent both the unperturbed and perturbed input, compare the actions the agent chooses and count how many times the actions are different",None (number) ,43,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective.  ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-FF-070,KPI-FF-070: Frequency changed output AI agent (Railway),Frequency changed output AI agent calculates the number of times the output of the AI agent (i.e. the action the agent chooses) is changed due to perturbations ,1823a9eb-ef2f-4394-bc32-a6d3d09d02f3,Test score (SUM of scenario scores),SUM,7c32d230-007e-4054-a8cd-60d9e7640f55,Scenario 1 - Frequency changed output AI agent calculates the number of times the output of the AI agent (i.e. the action the agent chooses) is changed due to perturbations ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective.  ",5ebf55e2-2a64-4ce0-9e42-08ba0dc8108c,Scenario score (raw values)
101,KPI-FF-070,Frequency changed output AI agent,Task 4.2,Robustness,O4,Power Grid,['Digital environment'],fully automated evaluation,Frequency changed output AI agent calculates the number of times the output of the AI agent (i.e. the action the agent chooses) is changed due to perturbations ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective.  ","While running the environment feed the AI agent both the unperturbed and perturbed input, compare the actions the agent chooses and count how many times the actions are different",None (number) ,43,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective.  ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-FF-070,KPI-FF-070: Frequency changed output AI agent (Power Grid),Frequency changed output AI agent calculates the number of times the output of the AI agent (i.e. the action the agent chooses) is changed due to perturbations ,1823a9eb-ef2f-4394-bc32-a6d3d09d02f3,Test score (SUM of scenario scores),SUM,0237ce32-8232-4a62-8302-64f29f33714c,Scenario 1 - Frequency changed output AI agent calculates the number of times the output of the AI agent (i.e. the action the agent chooses) is changed due to perturbations ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective.  ",c54441d8-cfb1-464d-9b97-8d484f9e5ea7,Scenario score (raw values)
102,KPI-FF-070,Frequency changed output AI agent,Task 4.2,Robustness,O4,ATM,['Digital environment'],fully automated evaluation,Frequency changed output AI agent calculates the number of times the output of the AI agent (i.e. the action the agent chooses) is changed due to perturbations ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective.  ","While running the environment feed the AI agent both the unperturbed and perturbed input, compare the actions the agent chooses and count how many times the actions are different",None (number) ,43,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective.  ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-FF-070,KPI-FF-070: Frequency changed output AI agent (ATM),Frequency changed output AI agent calculates the number of times the output of the AI agent (i.e. the action the agent chooses) is changed due to perturbations ,1823a9eb-ef2f-4394-bc32-a6d3d09d02f3,Test score (SUM of scenario scores),SUM,102fb4d4-36e4-4cc3-9253-d9dc383db91a,Scenario 1 - Frequency changed output AI agent calculates the number of times the output of the AI agent (i.e. the action the agent chooses) is changed due to perturbations ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective.  ",0e489390-7e89-49e7-b4de-67c3112fc317,Scenario score (raw values)
103,KPI-SF-071,Severity of changed output AI agent,Task 4.2,Robustness,O4,Railway,['Digital environment'],fully automated evaluation,Severity of changed output AI agent KPI measures similarity of the action chosen by AI agent based on a perturbed input to the action chosen with perfect information. Average pre-defined similarity score per changed action indicating how different the new action is from the original one. ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Assign similarity score to every pair of actions the AI agent can take and sum up this score for every time the agent's action is changed by perturbations,Average similarity score per action change ,44,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-SF-071,KPI-SF-071: Severity of changed output AI agent (Railway),Severity of changed output AI agent KPI measures similarity of the action chosen by AI agent based on a perturbed input to the action chosen with perfect information. Average pre-defined similarity score per changed action indicating how different the new action is from the original one. ,368978a0-2b61-4c5c-8153-38b3a9acd42a,Test score (SUM of scenario scores),SUM,327b8fa7-701d-46dc-8cd8-ee6fb04f846d,Scenario 1 - Severity of changed output AI agent KPI measures similarity of the action chosen by AI agent based on a perturbed input to the action chosen with perfect information. Average pre-defined similarity score per changed action indicating how different the new action is from the original one. ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",f0726678-2c9a-4364-ab89-cb0fc452c6b4,Scenario score (raw values)
104,KPI-SF-071,Severity of changed output AI agent,Task 4.2,Robustness,O4,Power Grid,['Digital environment'],fully automated evaluation,Severity of changed output AI agent KPI measures similarity of the action chosen by AI agent based on a perturbed input to the action chosen with perfect information. Average pre-defined similarity score per changed action indicating how different the new action is from the original one. ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Assign similarity score to every pair of actions the AI agent can take and sum up this score for every time the agent's action is changed by perturbations,Average similarity score per action change ,44,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-SF-071,KPI-SF-071: Severity of changed output AI agent (Power Grid),Severity of changed output AI agent KPI measures similarity of the action chosen by AI agent based on a perturbed input to the action chosen with perfect information. Average pre-defined similarity score per changed action indicating how different the new action is from the original one. ,368978a0-2b61-4c5c-8153-38b3a9acd42a,Test score (SUM of scenario scores),SUM,3fbf4ea0-801c-4a11-9108-a18480bff77c,Scenario 1 - Severity of changed output AI agent KPI measures similarity of the action chosen by AI agent based on a perturbed input to the action chosen with perfect information. Average pre-defined similarity score per changed action indicating how different the new action is from the original one. ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",9b538d48-829b-4d3b-a2af-23ebcbed926e,Scenario score (raw values)
105,KPI-SF-071,Severity of changed output AI agent,Task 4.2,Robustness,O4,ATM,['Digital environment'],fully automated evaluation,Severity of changed output AI agent KPI measures similarity of the action chosen by AI agent based on a perturbed input to the action chosen with perfect information. Average pre-defined similarity score per changed action indicating how different the new action is from the original one. ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Assign similarity score to every pair of actions the AI agent can take and sum up this score for every time the agent's action is changed by perturbations,Average similarity score per action change ,44,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-SF-071,KPI-SF-071: Severity of changed output AI agent (ATM),Severity of changed output AI agent KPI measures similarity of the action chosen by AI agent based on a perturbed input to the action chosen with perfect information. Average pre-defined similarity score per changed action indicating how different the new action is from the original one. ,368978a0-2b61-4c5c-8153-38b3a9acd42a,Test score (SUM of scenario scores),SUM,5423bf12-2476-4c6d-b5e1-e6f16939f8a9,Scenario 1 - Severity of changed output AI agent KPI measures similarity of the action chosen by AI agent based on a perturbed input to the action chosen with perfect information. Average pre-defined similarity score per changed action indicating how different the new action is from the original one. ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",e03dfaab-5321-4bfb-92fd-7a9b1d4db812,Scenario score (raw values)
106,KPI-SF-072,Steps survived with perturbations,Task 4.2,Robustness,O4,Railway,['Digital environment'],fully automated evaluation,Steps survived with perturbations KPI calculates the number of steps the AI agent is able to survive in environment with perturbation agent ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Count number of steps before a game over in the environment when including a perturbation agent,Number of steps ,45,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-SF-072,KPI-SF-072: Steps survived with perturbations (Railway),Steps survived with perturbations KPI calculates the number of steps the AI agent is able to survive in environment with perturbation agent ,15a704a1-6fc5-471b-88ce-c1002190a376,Test score (SUM of scenario scores),SUM,bcb2fdf6-ebd8-4862-b866-0ca005062be5,Scenario 1 - Steps survived with perturbations KPI calculates the number of steps the AI agent is able to survive in environment with perturbation agent ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",e56a4bf9-9a4a-4ac7-9d2d-43fb49882709,Scenario score (raw values)
107,KPI-SF-072,Steps survived with perturbations,Task 4.2,Robustness,O4,Power Grid,['Digital environment'],fully automated evaluation,Steps survived with perturbations KPI calculates the number of steps the AI agent is able to survive in environment with perturbation agent ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Count number of steps before a game over in the environment when including a perturbation agent,Number of steps ,45,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-SF-072,KPI-SF-072: Steps survived with perturbations (Power Grid),Steps survived with perturbations KPI calculates the number of steps the AI agent is able to survive in environment with perturbation agent ,15a704a1-6fc5-471b-88ce-c1002190a376,Test score (SUM of scenario scores),SUM,502f8c22-5298-4e7f-a59f-41211e4f78c1,Scenario 1 - Steps survived with perturbations KPI calculates the number of steps the AI agent is able to survive in environment with perturbation agent ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",a150b80b-1b54-477f-bdc3-339b376e2138,Scenario score (raw values)
108,KPI-SF-072,Steps survived with perturbations,Task 4.2,Robustness,O4,ATM,['Digital environment'],fully automated evaluation,Steps survived with perturbations KPI calculates the number of steps the AI agent is able to survive in environment with perturbation agent ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Count number of steps before a game over in the environment when including a perturbation agent,Number of steps ,45,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-SF-072,KPI-SF-072: Steps survived with perturbations (ATM),Steps survived with perturbations KPI calculates the number of steps the AI agent is able to survive in environment with perturbation agent ,15a704a1-6fc5-471b-88ce-c1002190a376,Test score (SUM of scenario scores),SUM,fcfff6b8-ce7b-4744-b9ed-ef6bfeea0b1c,Scenario 1 - Steps survived with perturbations KPI calculates the number of steps the AI agent is able to survive in environment with perturbation agent ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",ba37cc69-17ce-4a0c-9f82-887ebfb149f3,Scenario score (raw values)
109,KPI-VF-073,Vulnerability to perturbation,Task 4.2,Robustness,O4,Railway,['Digital environment'],fully automated evaluation,"Vulnerability to perturbation KPI measures vulnerability of specific value in observed state to perturbations, i.e. how likely it is that perturbing the value will result in a change in action chosen by the AI agent ","This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","For value x1 in observed state, count how many times x1 is perturbed significantly during the episode and count how many times it is perturbed when the AI agent's chosen action is changed by the perturbations and divide the latter by the former",Proportion of times perturbing the value resulted in a changed action ,46,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-VF-073,KPI-VF-073: Vulnerability to perturbation (Railway),"Vulnerability to perturbation KPI measures vulnerability of specific value in observed state to perturbations, i.e. how likely it is that perturbing the value will result in a change in action chosen by the AI agent ",17851b69-7c44-41f7-a90a-f531f79ac21e,Test score (SUM of scenario scores),SUM,be42f670-0cc6-44de-bf5e-8e6eb979ade4,"Scenario 1 - Vulnerability to perturbation KPI measures vulnerability of specific value in observed state to perturbations, i.e. how likely it is that perturbing the value will result in a change in action chosen by the AI agent ","This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",31be4ee2-9556-42d6-9566-b2dbc70b761a,Scenario score (raw values)
110,KPI-VF-073,Vulnerability to perturbation,Task 4.2,Robustness,O4,Power Grid,['Digital environment'],fully automated evaluation,"Vulnerability to perturbation KPI measures vulnerability of specific value in observed state to perturbations, i.e. how likely it is that perturbing the value will result in a change in action chosen by the AI agent ","This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","For value x1 in observed state, count how many times x1 is perturbed significantly during the episode and count how many times it is perturbed when the AI agent's chosen action is changed by the perturbations and divide the latter by the former",Proportion of times perturbing the value resulted in a changed action ,46,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-VF-073,KPI-VF-073: Vulnerability to perturbation (Power Grid),"Vulnerability to perturbation KPI measures vulnerability of specific value in observed state to perturbations, i.e. how likely it is that perturbing the value will result in a change in action chosen by the AI agent ",17851b69-7c44-41f7-a90a-f531f79ac21e,Test score (SUM of scenario scores),SUM,553762b1-fb83-4686-8dc0-25fe8e0d6484,"Scenario 1 - Vulnerability to perturbation KPI measures vulnerability of specific value in observed state to perturbations, i.e. how likely it is that perturbing the value will result in a change in action chosen by the AI agent ","This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",cad0d68a-e039-4153-83a7-eb701fdb3571,Scenario score (raw values)
111,KPI-VF-073,Vulnerability to perturbation,Task 4.2,Robustness,O4,ATM,['Digital environment'],fully automated evaluation,"Vulnerability to perturbation KPI measures vulnerability of specific value in observed state to perturbations, i.e. how likely it is that perturbing the value will result in a change in action chosen by the AI agent ","This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","For value x1 in observed state, count how many times x1 is perturbed significantly during the episode and count how many times it is perturbed when the AI agent's chosen action is changed by the perturbations and divide the latter by the former",Proportion of times perturbing the value resulted in a changed action ,46,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-VF-073,KPI-VF-073: Vulnerability to perturbation (ATM),"Vulnerability to perturbation KPI measures vulnerability of specific value in observed state to perturbations, i.e. how likely it is that perturbing the value will result in a change in action chosen by the AI agent ",17851b69-7c44-41f7-a90a-f531f79ac21e,Test score (SUM of scenario scores),SUM,121b5889-7c51-422f-a640-5ba316d3ed49,"Scenario 1 - Vulnerability to perturbation KPI measures vulnerability of specific value in observed state to perturbations, i.e. how likely it is that perturbing the value will result in a change in action chosen by the AI agent ","This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",7a6db169-58df-4857-9771-df36dea6ec53,Scenario score (raw values)
112,KPI-AF-074,Area between reward curves,Task 4.2,Resilience,O4,Railway,['Digital environment'],fully automated evaluation,Area between reward curves calculates area between the curve corresponding to the reward obtained in each step in an environment where the AI agent has perfect information and the curve for an environment where the agent's input is perturbed ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Use the trapezoidal rule for numerical integration to compute the area underneath the two curves and subtract ,None (cumulative reward) ,47,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,2291b692-58c6-4444-b01c-759dbfa42fe4,Resilience,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",6d19a774-6ca1-4b30-b817-8d9533d24818,Benchmark score (SUM of test scores),SUM,KPI-AF-074,KPI-AF-074: Area between reward curves (Railway),Area between reward curves calculates area between the curve corresponding to the reward obtained in each step in an environment where the AI agent has perfect information and the curve for an environment where the agent's input is perturbed ,06f0dea2-825d-4073-b7d8-e14d9c4625cb,Test score (SUM of scenario scores),SUM,654afe37-a639-4a62-91c1-964fd79e73eb,Scenario 1 - Area between reward curves calculates area between the curve corresponding to the reward obtained in each step in an environment where the AI agent has perfect information and the curve for an environment where the agent's input is perturbed ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",e500b9f6-cc4b-4324-90d1-f59f0942bb01,Scenario score (raw values)
113,KPI-AF-074,Area between reward curves,Task 4.2,Resilience,O4,Power Grid,['Digital environment'],fully automated evaluation,Area between reward curves calculates area between the curve corresponding to the reward obtained in each step in an environment where the AI agent has perfect information and the curve for an environment where the agent's input is perturbed ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Use the trapezoidal rule for numerical integration to compute the area underneath the two curves and subtract ,None (cumulative reward) ,47,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,2291b692-58c6-4444-b01c-759dbfa42fe4,Resilience,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",6d19a774-6ca1-4b30-b817-8d9533d24818,Benchmark score (SUM of test scores),SUM,KPI-AF-074,KPI-AF-074: Area between reward curves (Power Grid),Area between reward curves calculates area between the curve corresponding to the reward obtained in each step in an environment where the AI agent has perfect information and the curve for an environment where the agent's input is perturbed ,06f0dea2-825d-4073-b7d8-e14d9c4625cb,Test score (SUM of scenario scores),SUM,519db711-726f-4b38-ba22-1258feaec1f1,Scenario 1 - Area between reward curves calculates area between the curve corresponding to the reward obtained in each step in an environment where the AI agent has perfect information and the curve for an environment where the agent's input is perturbed ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",dc5ab5e2-9446-4a05-9f59-472ed656f11a,Scenario score (raw values)
114,KPI-AF-074,Area between reward curves,Task 4.2,Resilience,O4,ATM,['Digital environment'],fully automated evaluation,Area between reward curves calculates area between the curve corresponding to the reward obtained in each step in an environment where the AI agent has perfect information and the curve for an environment where the agent's input is perturbed ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Use the trapezoidal rule for numerical integration to compute the area underneath the two curves and subtract ,None (cumulative reward) ,47,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,2291b692-58c6-4444-b01c-759dbfa42fe4,Resilience,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",6d19a774-6ca1-4b30-b817-8d9533d24818,Benchmark score (SUM of test scores),SUM,KPI-AF-074,KPI-AF-074: Area between reward curves (ATM),Area between reward curves calculates area between the curve corresponding to the reward obtained in each step in an environment where the AI agent has perfect information and the curve for an environment where the agent's input is perturbed ,06f0dea2-825d-4073-b7d8-e14d9c4625cb,Test score (SUM of scenario scores),SUM,a90e293a-2669-408c-9b45-f3e7b67abb1e,Scenario 1 - Area between reward curves calculates area between the curve corresponding to the reward obtained in each step in an environment where the AI agent has perfect information and the curve for an environment where the agent's input is perturbed ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",187bcf55-dc63-4600-9e61-e0fe5a0b7c4a,Scenario score (raw values)
115,KPI-DF-075,Degradation time,Task 4.2,Resilience,O4,Railway,['Digital environment'],fully automated evaluation,Number of steps/episodes until reward reaches its lowest point after introducing perturbations to the input of the AI agent ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Find step/episode where reward is the lowest and get the difference to the step/episode the perturbations were introduced,Number of steps/episodes ,48,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,2291b692-58c6-4444-b01c-759dbfa42fe4,Resilience,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",6d19a774-6ca1-4b30-b817-8d9533d24818,Benchmark score (SUM of test scores),SUM,KPI-DF-075,KPI-DF-075: Degradation time (Railway),Number of steps/episodes until reward reaches its lowest point after introducing perturbations to the input of the AI agent ,0759f523-38e9-4bc2-9311-d82e8a8565c7,Test score (SUM of scenario scores),SUM,3acc7c29-c364-4c58-bba9-aa6bd23b7762,Scenario 1 - Number of steps/episodes until reward reaches its lowest point after introducing perturbations to the input of the AI agent ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",d41c074e-5044-4b05-9fdd-7711a03f7787,Scenario score (raw values)
116,KPI-DF-075,Degradation time,Task 4.2,Resilience,O4,Power Grid,['Digital environment'],fully automated evaluation,Number of steps/episodes until reward reaches its lowest point after introducing perturbations to the input of the AI agent ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Find step/episode where reward is the lowest and get the difference to the step/episode the perturbations were introduced,Number of steps/episodes ,48,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,2291b692-58c6-4444-b01c-759dbfa42fe4,Resilience,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",6d19a774-6ca1-4b30-b817-8d9533d24818,Benchmark score (SUM of test scores),SUM,KPI-DF-075,KPI-DF-075: Degradation time (Power Grid),Number of steps/episodes until reward reaches its lowest point after introducing perturbations to the input of the AI agent ,0759f523-38e9-4bc2-9311-d82e8a8565c7,Test score (SUM of scenario scores),SUM,0061d975-78e3-41ee-8231-839a9f645a58,Scenario 1 - Number of steps/episodes until reward reaches its lowest point after introducing perturbations to the input of the AI agent ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",e577e1be-6ab4-4d4f-8c01-ba0625abda0c,Scenario score (raw values)
117,KPI-DF-075,Degradation time,Task 4.2,Resilience,O4,ATM,['Digital environment'],fully automated evaluation,Number of steps/episodes until reward reaches its lowest point after introducing perturbations to the input of the AI agent ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Find step/episode where reward is the lowest and get the difference to the step/episode the perturbations were introduced,Number of steps/episodes ,48,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,2291b692-58c6-4444-b01c-759dbfa42fe4,Resilience,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",6d19a774-6ca1-4b30-b817-8d9533d24818,Benchmark score (SUM of test scores),SUM,KPI-DF-075,KPI-DF-075: Degradation time (ATM),Number of steps/episodes until reward reaches its lowest point after introducing perturbations to the input of the AI agent ,0759f523-38e9-4bc2-9311-d82e8a8565c7,Test score (SUM of scenario scores),SUM,2706885e-da3a-42e6-b24b-fdc6ff05575f,Scenario 1 - Number of steps/episodes until reward reaches its lowest point after introducing perturbations to the input of the AI agent ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",864a83e3-bca5-4e92-9d41-476329c88ab7,Scenario score (raw values)
118,KPI-RF-076,Restorative time,Task 4.2,Resilience,O4,Railway,['Digital environment'],fully automated evaluation,Number of steps/episodes until reward recovers to its highest point after reaching the lowest point after introducing perturbations to the input of the AI agent ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Find step/episode where reward is the highest and get the difference to the step/episode with the lowest reward from KPI-DF-075,Number of steps/episodes ,49,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,2291b692-58c6-4444-b01c-759dbfa42fe4,Resilience,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",6d19a774-6ca1-4b30-b817-8d9533d24818,Benchmark score (SUM of test scores),SUM,KPI-RF-076,KPI-RF-076: Restorative time (Railway),Number of steps/episodes until reward recovers to its highest point after reaching the lowest point after introducing perturbations to the input of the AI agent ,1ce16902-296c-434f-b9fc-1acb24964643,Test score (SUM of scenario scores),SUM,c87b66bf-1dec-4fc2-bdc0-73decdaff3b1,Scenario 1 - Number of steps/episodes until reward recovers to its highest point after reaching the lowest point after introducing perturbations to the input of the AI agent ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",62243c4b-19b5-4e2b-a9ef-523b2423d43b,Scenario score (raw values)
119,KPI-RF-076,Restorative time,Task 4.2,Resilience,O4,Power Grid,['Digital environment'],fully automated evaluation,Number of steps/episodes until reward recovers to its highest point after reaching the lowest point after introducing perturbations to the input of the AI agent ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Find step/episode where reward is the highest and get the difference to the step/episode with the lowest reward from KPI-DF-075,Number of steps/episodes ,49,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,2291b692-58c6-4444-b01c-759dbfa42fe4,Resilience,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",6d19a774-6ca1-4b30-b817-8d9533d24818,Benchmark score (SUM of test scores),SUM,KPI-RF-076,KPI-RF-076: Restorative time (Power Grid),Number of steps/episodes until reward recovers to its highest point after reaching the lowest point after introducing perturbations to the input of the AI agent ,1ce16902-296c-434f-b9fc-1acb24964643,Test score (SUM of scenario scores),SUM,a4252ee8-4f6e-496f-9909-d02ffb6b15cf,Scenario 1 - Number of steps/episodes until reward recovers to its highest point after reaching the lowest point after introducing perturbations to the input of the AI agent ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",ddfe7745-a449-4b08-abb1-9ef71d9468d2,Scenario score (raw values)
120,KPI-RF-076,Restorative time,Task 4.2,Resilience,O4,ATM,['Digital environment'],fully automated evaluation,Number of steps/episodes until reward recovers to its highest point after reaching the lowest point after introducing perturbations to the input of the AI agent ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Find step/episode where reward is the highest and get the difference to the step/episode with the lowest reward from KPI-DF-075,Number of steps/episodes ,49,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,2291b692-58c6-4444-b01c-759dbfa42fe4,Resilience,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",6d19a774-6ca1-4b30-b817-8d9533d24818,Benchmark score (SUM of test scores),SUM,KPI-RF-076,KPI-RF-076: Restorative time (ATM),Number of steps/episodes until reward recovers to its highest point after reaching the lowest point after introducing perturbations to the input of the AI agent ,1ce16902-296c-434f-b9fc-1acb24964643,Test score (SUM of scenario scores),SUM,f1f41de5-a702-414c-91ec-ec1ad3ce8f1f,Scenario 1 - Number of steps/episodes until reward recovers to its highest point after reaching the lowest point after introducing perturbations to the input of the AI agent ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",7f099c55-a7e8-4467-9eca-d9be5f05c5d8,Scenario score (raw values)
121,KPI-SF-077,Similarity state to unperturbed situation,Task 4.2,Resilience,O4,Railway,['Digital environment'],fully automated evaluation,Similarity state to unperturbed situation KPI measures similarity of the state in an environment where AI agent's input is perturbed to the state in the same context of an environment with perfect information ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","Choose a metric to measure the similarity between states, e.g. cosine similarity, Euclidean distance, etc., and compute similarity between the state in each step of environment with perfect information and one with perturbed input. Plot curve of similarity in each step and evaluate using KPI-AF-074, KPI-DF-075 and KPI-RF-076",,50,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,2291b692-58c6-4444-b01c-759dbfa42fe4,Resilience,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",6d19a774-6ca1-4b30-b817-8d9533d24818,Benchmark score (SUM of test scores),SUM,KPI-SF-077,KPI-SF-077: Similarity state to unperturbed situation (Railway),Similarity state to unperturbed situation KPI measures similarity of the state in an environment where AI agent's input is perturbed to the state in the same context of an environment with perfect information ,3da20342-32d0-41ef-a563-b251dfb8f9b5,Test score (SUM of scenario scores),SUM,a736254a-afba-44b0-91eb-0a668f5c393b,Scenario 1 - Similarity state to unperturbed situation KPI measures similarity of the state in an environment where AI agent's input is perturbed to the state in the same context of an environment with perfect information ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",ba698c1b-224e-4588-bbb9-80029a7a09a9,Scenario score (raw values)
122,KPI-SF-077,Similarity state to unperturbed situation,Task 4.2,Resilience,O4,Power Grid,['Digital environment'],fully automated evaluation,Similarity state to unperturbed situation KPI measures similarity of the state in an environment where AI agent's input is perturbed to the state in the same context of an environment with perfect information ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","Choose a metric to measure the similarity between states, e.g. cosine similarity, Euclidean distance, etc., and compute similarity between the state in each step of environment with perfect information and one with perturbed input. Plot curve of similarity in each step and evaluate using KPI-AF-074, KPI-DF-075 and KPI-RF-076",,50,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,2291b692-58c6-4444-b01c-759dbfa42fe4,Resilience,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",6d19a774-6ca1-4b30-b817-8d9533d24818,Benchmark score (SUM of test scores),SUM,KPI-SF-077,KPI-SF-077: Similarity state to unperturbed situation (Power Grid),Similarity state to unperturbed situation KPI measures similarity of the state in an environment where AI agent's input is perturbed to the state in the same context of an environment with perfect information ,3da20342-32d0-41ef-a563-b251dfb8f9b5,Test score (SUM of scenario scores),SUM,d8ebf7d4-29fb-4c18-8c14-e909f2a7913e,Scenario 1 - Similarity state to unperturbed situation KPI measures similarity of the state in an environment where AI agent's input is perturbed to the state in the same context of an environment with perfect information ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",52ff3cef-e257-4138-a33f-57de515627e7,Scenario score (raw values)
123,KPI-SF-077,Similarity state to unperturbed situation,Task 4.2,Resilience,O4,ATM,['Digital environment'],fully automated evaluation,Similarity state to unperturbed situation KPI measures similarity of the state in an environment where AI agent's input is perturbed to the state in the same context of an environment with perfect information ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","Choose a metric to measure the similarity between states, e.g. cosine similarity, Euclidean distance, etc., and compute similarity between the state in each step of environment with perfect information and one with perturbed input. Plot curve of similarity in each step and evaluate using KPI-AF-074, KPI-DF-075 and KPI-RF-076",,50,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,2291b692-58c6-4444-b01c-759dbfa42fe4,Resilience,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",6d19a774-6ca1-4b30-b817-8d9533d24818,Benchmark score (SUM of test scores),SUM,KPI-SF-077,KPI-SF-077: Similarity state to unperturbed situation (ATM),Similarity state to unperturbed situation KPI measures similarity of the state in an environment where AI agent's input is perturbed to the state in the same context of an environment with perfect information ,3da20342-32d0-41ef-a563-b251dfb8f9b5,Test score (SUM of scenario scores),SUM,dfac6b42-7875-4746-ae4d-a66a30f8119b,Scenario 1 - Similarity state to unperturbed situation KPI measures similarity of the state in an environment where AI agent's input is perturbed to the state in the same context of an environment with perfect information ,"This KPI contributes to evaluating Resilience of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",d774b8d2-afc8-4f13-90ee-591a5b12cb8a,Scenario score (raw values)
124,KPI-RF-078,Reward per action,Task 4.2,Robustness,O4,Railway,['Digital environment'],fully automated evaluation,Reward per action KPI calculates average reward obtained for each action performed by the AI agent ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Total reward obtained / Number of actions performed,Same unit as reward ,51,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-RF-078,KPI-RF-078: Reward per action (Railway),Reward per action KPI calculates average reward obtained for each action performed by the AI agent ,2a919d71-ed35-4e98-a3b4-4b2c1a29707d,Test score (SUM of scenario scores),SUM,41e3b2c0-b726-44ad-8f61-9f50046bf095,Scenario 1 - Reward per action KPI calculates average reward obtained for each action performed by the AI agent ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",28a8857f-bbcf-4963-8118-f4d29b969e10,Scenario score (raw values)
125,KPI-RF-078,Reward per action,Task 4.2,Robustness,O4,Power Grid,['Digital environment'],fully automated evaluation,Reward per action KPI calculates average reward obtained for each action performed by the AI agent ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Total reward obtained / Number of actions performed,Same unit as reward ,51,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-RF-078,KPI-RF-078: Reward per action (Power Grid),Reward per action KPI calculates average reward obtained for each action performed by the AI agent ,2a919d71-ed35-4e98-a3b4-4b2c1a29707d,Test score (SUM of scenario scores),SUM,003a7672-f2cd-4bb3-8489-f96f523f57aa,Scenario 1 - Reward per action KPI calculates average reward obtained for each action performed by the AI agent ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",ef465ada-6cbb-4cbd-8cd1-e94880571dfa,Scenario score (raw values)
126,KPI-RF-078,Reward per action,Task 4.2,Robustness,O4,ATM,['Digital environment'],fully automated evaluation,Reward per action KPI calculates average reward obtained for each action performed by the AI agent ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",Total reward obtained / Number of actions performed,Same unit as reward ,51,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-RF-078,KPI-RF-078: Reward per action (ATM),Reward per action KPI calculates average reward obtained for each action performed by the AI agent ,2a919d71-ed35-4e98-a3b4-4b2c1a29707d,Test score (SUM of scenario scores),SUM,8e5b2298-ced2-4992-ae4d-08debfe5d3d5,Scenario 1 - Reward per action KPI calculates average reward obtained for each action performed by the AI agent ,"This KPI contributes to evaluating Robustness of the AI-based assistant, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",6fb23a9a-6ce6-497c-81af-cf8ea1ab3ea8,Scenario score (raw values)
127,KPI-EF-086,Explainability Robustness,Task 4.2,Robustness,O4,Power Grid,"['AI agent', 'Digital environment']",fully automated evaluation,"The Explainability Robustness KPI evaluates the stability of explanations against small input perturbations, assuming the model’s output remains relatively unchanged. A robust explanation should not fluctuate significantly when the input is slightly modified. The Average Sensitivity Metric quantifies this stability by applying small perturbations to the input data and measuring how much the explanation changes. Since computing sensitivity over all possible perturbations is impractical, Monte Carlo sampling is used to estimate these variations efficiently. ","This KPI ensures that AI-driven explanations remain reliable and aligned with the actual decision-making process of the model. It helps evaluate interpretability methods in AI systems used in critical applications. 
This KPI contributes to evaluating AI trustworthiness, acceptability and trust of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O4 main project objective.","Average of explanation differences computed over multiple runs
Explanation differences measure the sensitivity estimate for each sample with p-norm (e.g., L1 or L2 distance) applied to the difference between:
- the explanation for the original input 
- the explanation for the perturbed input ","Change in explanation values (e.g., L1 or L2 norm difference), Normalized score indicating robustness ",52,['Power Grid'],0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI ensures that AI-driven explanations remain reliable and aligned with the actual decision-making process of the model. It helps evaluate interpretability methods in AI systems used in critical applications. 
This KPI contributes to evaluating AI trustworthiness, acceptability and trust of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O4 main project objective.",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-EF-086,KPI-EF-086: Explainability Robustness (Power Grid),"The Explainability Robustness KPI evaluates the stability of explanations against small input perturbations, assuming the model’s output remains relatively unchanged. A robust explanation should not fluctuate significantly when the input is slightly modified. The Average Sensitivity Metric quantifies this stability by applying small perturbations to the input data and measuring how much the explanation changes. Since computing sensitivity over all possible perturbations is impractical, Monte Carlo sampling is used to estimate these variations efficiently. ",ee98e0d0-24b9-460e-aaa7-27e814c7d4a0,Test score (SUM of scenario scores),SUM,970c0d2c-8172-44b6-bd12-f56b2f6b2eb1,"Scenario 1 - The Explainability Robustness KPI evaluates the stability of explanations against small input perturbations, assuming the model’s output remains relatively unchanged. A robust explanation should not fluctuate significantly when the input is slightly modified. The Average Sensitivity Metric quantifies this stability by applying small perturbations to the input data and measuring how much the explanation changes. Since computing sensitivity over all possible perturbations is impractical, Monte Carlo sampling is used to estimate these variations efficiently. ","This KPI ensures that AI-driven explanations remain reliable and aligned with the actual decision-making process of the model. It helps evaluate interpretability methods in AI systems used in critical applications. 
This KPI contributes to evaluating AI trustworthiness, acceptability and trust of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O4 main project objective.",ff3d72f6-0c51-48ed-9cb0-957e88fcea88,Scenario score (raw values)
128,KPI-EF-087,Explainability Faithfulness,Task 4.2,Robustness,O4,Power Grid,"['AI agent', 'Digital environment']",fully automated evaluation,"The Faithfulness KPI assesses whether the feature importance scores provided by an explanation method accurately reflect the model’s decision-making process. It systematically removes or alters features and measures the impact on the model’s predictions. The assumption is that if a feature is truly important, removing or altering it should significantly affect the model’s output. ","This KPI ensures that AI-driven explanations remain reliable and aligned with the actual decision-making process of the model. It helps evaluate interpretability methods in AI systems used in critical applications. 

This KPI contributes to evaluating AI trustworthiness, acceptability and trust of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O4 main project objective. ","Sum of the absolute difference between the model prediction for the original input and the model prediction when a feature  is removed, masked, or replaced, over the total number of evaluated samples ","Change in model confidence score (e.g., probability difference), Normalized score indicating faithfulness ",53,['Power Grid'],0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,9a859ae9-695f-40b7-83aa-8e2dd37ba38f,Robustness,"This KPI ensures that AI-driven explanations remain reliable and aligned with the actual decision-making process of the model. It helps evaluate interpretability methods in AI systems used in critical applications. 

This KPI contributes to evaluating AI trustworthiness, acceptability and trust of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O4 main project objective. ",69fc414a-fd99-4c46-9798-5034ef9db8b0,Benchmark score (SUM of test scores),SUM,KPI-EF-087,KPI-EF-087: Explainability Faithfulness (Power Grid),"The Faithfulness KPI assesses whether the feature importance scores provided by an explanation method accurately reflect the model’s decision-making process. It systematically removes or alters features and measures the impact on the model’s predictions. The assumption is that if a feature is truly important, removing or altering it should significantly affect the model’s output. ",37f9adc7-b906-4a8c-ba46-ca3f11944ac1,Test score (SUM of scenario scores),SUM,d150f9f9-3377-4ae1-a789-96cab198ac4e,"Scenario 1 - The Faithfulness KPI assesses whether the feature importance scores provided by an explanation method accurately reflect the model’s decision-making process. It systematically removes or alters features and measures the impact on the model’s predictions. The assumption is that if a feature is truly important, removing or altering it should significantly affect the model’s output. ","This KPI ensures that AI-driven explanations remain reliable and aligned with the actual decision-making process of the model. It helps evaluate interpretability methods in AI systems used in critical applications. 

This KPI contributes to evaluating AI trustworthiness, acceptability and trust of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O4 main project objective. ",8bca46d2-d2fa-4971-9f84-95fc3e0932e7,Scenario score (raw values)
129,KPI-PS-089,Perceived decision novelty,Task 4.3,Social-technical decision quality,O3,Railway,['(non applicable)'],special evaluation setup,This KPI represents human operators’ self-reported subjective assessment of nontriviality for the AI-generated solutions measured with a questionnaire. ,"This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,54,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3b1aff0c-5c94-445c-a106-c94ab1f780ca,Social-technical decision quality,"This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",bbacedca-d4da-4c9b-afba-590bc1d9919e,Benchmark score (SUM of test scores),SUM,KPI-PS-089,KPI-PS-089: Perceived decision novelty (Railway),This KPI represents human operators’ self-reported subjective assessment of nontriviality for the AI-generated solutions measured with a questionnaire. ,c4772746-6b83-4da2-8ab6-15c65485d797,Test score (SUM of scenario scores),SUM,a372bba6-3358-43e7-b8b6-9cd2f4eaafed,Scenario 1 - This KPI represents human operators’ self-reported subjective assessment of nontriviality for the AI-generated solutions measured with a questionnaire. ,"This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",ae868b6a-620a-4fa5-917d-ffbb9ac74235,Scenario score (raw values)
130,KPI-PS-089,Perceived decision novelty,Task 4.3,Social-technical decision quality,O3,Power Grid,['(non applicable)'],special evaluation setup,This KPI represents human operators’ self-reported subjective assessment of nontriviality for the AI-generated solutions measured with a questionnaire. ,"This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,54,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3b1aff0c-5c94-445c-a106-c94ab1f780ca,Social-technical decision quality,"This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",bbacedca-d4da-4c9b-afba-590bc1d9919e,Benchmark score (SUM of test scores),SUM,KPI-PS-089,KPI-PS-089: Perceived decision novelty (Power Grid),This KPI represents human operators’ self-reported subjective assessment of nontriviality for the AI-generated solutions measured with a questionnaire. ,c4772746-6b83-4da2-8ab6-15c65485d797,Test score (SUM of scenario scores),SUM,d15bf1df-0150-42f6-b9bf-82612416c890,Scenario 1 - This KPI represents human operators’ self-reported subjective assessment of nontriviality for the AI-generated solutions measured with a questionnaire. ,"This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",4deec6fb-64ee-4896-a680-1c2181d76d09,Scenario score (raw values)
131,KPI-PS-089,Perceived decision novelty,Task 4.3,Social-technical decision quality,O3,ATM,['(non applicable)'],special evaluation setup,This KPI represents human operators’ self-reported subjective assessment of nontriviality for the AI-generated solutions measured with a questionnaire. ,"This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,54,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,3b1aff0c-5c94-445c-a106-c94ab1f780ca,Social-technical decision quality,"This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",bbacedca-d4da-4c9b-afba-590bc1d9919e,Benchmark score (SUM of test scores),SUM,KPI-PS-089,KPI-PS-089: Perceived decision novelty (ATM),This KPI represents human operators’ self-reported subjective assessment of nontriviality for the AI-generated solutions measured with a questionnaire. ,c4772746-6b83-4da2-8ab6-15c65485d797,Test score (SUM of scenario scores),SUM,63402cc1-7613-4e51-ab5c-2135bb7ddc84,Scenario 1 - This KPI represents human operators’ self-reported subjective assessment of nontriviality for the AI-generated solutions measured with a questionnaire. ,"This KPI contributes to evaluating Social-technical decision quality of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. ",7943bff4-8b97-4cf6-90a8-2dabfc209ee4,Scenario score (raw values)
132,KPI-DF-090,Domain shift forgetting rate,Task 4.2,Reliability,O4,Railway,['Digital environment'],fully automated evaluation,The rate at which an agent forgets its performance in the original domain after being exposed to a shifted domain. It helps to measure the extent to which learning in the new domain negatively impacts the agent’s ability to perform in the original domain. ,"The objective of computing the Forgetting Rate in Domain Shift is to quantify the decline in an agent's performance on the original domain after being trained or exposed to a shifted domain. This metric helps assess the extent of negative transfer, ensuring that adaptation to the new domain does not excessively degrade prior knowledge. A higher forgetting rate indicates a more significant loss of previously learned knowledge due to domain shift. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","Let:  
- P[init/orig] be the agent’s performance (e.g., accuracy, reward, or another metric) in the original domain before exposure to the new domain. 
- P[post/orig] be the agent’s performance in the original domain after training in the shifted domain. 
The forgetting rate (FR) can be computed as: 
FR=( P[init/orig]− P[post/orig])/P[init/orig]",Proportion or Percentage,55,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"The objective of computing the Forgetting Rate in Domain Shift is to quantify the decline in an agent's performance on the original domain after being trained or exposed to a shifted domain. This metric helps assess the extent of negative transfer, ensuring that adaptation to the new domain does not excessively degrade prior knowledge. A higher forgetting rate indicates a more significant loss of previously learned knowledge due to domain shift. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-090,KPI-DF-090: Domain shift forgetting rate (Railway),The rate at which an agent forgets its performance in the original domain after being exposed to a shifted domain. It helps to measure the extent to which learning in the new domain negatively impacts the agent’s ability to perform in the original domain. ,3bf19330-0a07-4872-b563-f6aab3f45930,Test score (SUM of scenario scores),SUM,b2118540-8b85-4b28-a432-29bd1a6ad56b,Scenario 1 - The rate at which an agent forgets its performance in the original domain after being exposed to a shifted domain. It helps to measure the extent to which learning in the new domain negatively impacts the agent’s ability to perform in the original domain. ,"The objective of computing the Forgetting Rate in Domain Shift is to quantify the decline in an agent's performance on the original domain after being trained or exposed to a shifted domain. This metric helps assess the extent of negative transfer, ensuring that adaptation to the new domain does not excessively degrade prior knowledge. A higher forgetting rate indicates a more significant loss of previously learned knowledge due to domain shift. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",1774c4c0-4b14-4973-9124-c022ddb577f1,Scenario score (raw values)
133,KPI-DF-090,Domain shift forgetting rate,Task 4.2,Reliability,O4,Power Grid,['Digital environment'],fully automated evaluation,The rate at which an agent forgets its performance in the original domain after being exposed to a shifted domain. It helps to measure the extent to which learning in the new domain negatively impacts the agent’s ability to perform in the original domain. ,"The objective of computing the Forgetting Rate in Domain Shift is to quantify the decline in an agent's performance on the original domain after being trained or exposed to a shifted domain. This metric helps assess the extent of negative transfer, ensuring that adaptation to the new domain does not excessively degrade prior knowledge. A higher forgetting rate indicates a more significant loss of previously learned knowledge due to domain shift. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","Let:  
- P[init/orig] be the agent’s performance (e.g., accuracy, reward, or another metric) in the original domain before exposure to the new domain. 
- P[post/orig] be the agent’s performance in the original domain after training in the shifted domain. 
The forgetting rate (FR) can be computed as: 
FR=( P[init/orig]− P[post/orig])/P[init/orig]",Proportion or Percentage,55,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"The objective of computing the Forgetting Rate in Domain Shift is to quantify the decline in an agent's performance on the original domain after being trained or exposed to a shifted domain. This metric helps assess the extent of negative transfer, ensuring that adaptation to the new domain does not excessively degrade prior knowledge. A higher forgetting rate indicates a more significant loss of previously learned knowledge due to domain shift. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-090,KPI-DF-090: Domain shift forgetting rate (Power Grid),The rate at which an agent forgets its performance in the original domain after being exposed to a shifted domain. It helps to measure the extent to which learning in the new domain negatively impacts the agent’s ability to perform in the original domain. ,3bf19330-0a07-4872-b563-f6aab3f45930,Test score (SUM of scenario scores),SUM,940a68c1-07b9-4345-92e0-c498271f0e32,Scenario 1 - The rate at which an agent forgets its performance in the original domain after being exposed to a shifted domain. It helps to measure the extent to which learning in the new domain negatively impacts the agent’s ability to perform in the original domain. ,"The objective of computing the Forgetting Rate in Domain Shift is to quantify the decline in an agent's performance on the original domain after being trained or exposed to a shifted domain. This metric helps assess the extent of negative transfer, ensuring that adaptation to the new domain does not excessively degrade prior knowledge. A higher forgetting rate indicates a more significant loss of previously learned knowledge due to domain shift. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",415d2e8c-69e0-4da6-bae5-2845e3a06aaa,Scenario score (raw values)
134,KPI-DF-090,Domain shift forgetting rate,Task 4.2,Reliability,O4,ATM,['Digital environment'],fully automated evaluation,The rate at which an agent forgets its performance in the original domain after being exposed to a shifted domain. It helps to measure the extent to which learning in the new domain negatively impacts the agent’s ability to perform in the original domain. ,"The objective of computing the Forgetting Rate in Domain Shift is to quantify the decline in an agent's performance on the original domain after being trained or exposed to a shifted domain. This metric helps assess the extent of negative transfer, ensuring that adaptation to the new domain does not excessively degrade prior knowledge. A higher forgetting rate indicates a more significant loss of previously learned knowledge due to domain shift. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ","Let:  
- P[init/orig] be the agent’s performance (e.g., accuracy, reward, or another metric) in the original domain before exposure to the new domain. 
- P[post/orig] be the agent’s performance in the original domain after training in the shifted domain. 
The forgetting rate (FR) can be computed as: 
FR=( P[init/orig]− P[post/orig])/P[init/orig]",Proportion or Percentage,55,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,a4f11ae2-6f4b-486a-b1d6-e8df8618eb25,Reliability,"The objective of computing the Forgetting Rate in Domain Shift is to quantify the decline in an agent's performance on the original domain after being trained or exposed to a shifted domain. This metric helps assess the extent of negative transfer, ensuring that adaptation to the new domain does not excessively degrade prior knowledge. A higher forgetting rate indicates a more significant loss of previously learned knowledge due to domain shift. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",78e0b7d9-17f8-43b9-8a8d-8affda76a0f0,Benchmark score (SUM of test scores),SUM,KPI-DF-090,KPI-DF-090: Domain shift forgetting rate (ATM),The rate at which an agent forgets its performance in the original domain after being exposed to a shifted domain. It helps to measure the extent to which learning in the new domain negatively impacts the agent’s ability to perform in the original domain. ,3bf19330-0a07-4872-b563-f6aab3f45930,Test score (SUM of scenario scores),SUM,9b1d05d6-d155-4f99-97e8-ebbed29fd0a3,Scenario 1 - The rate at which an agent forgets its performance in the original domain after being exposed to a shifted domain. It helps to measure the extent to which learning in the new domain negatively impacts the agent’s ability to perform in the original domain. ,"The objective of computing the Forgetting Rate in Domain Shift is to quantify the decline in an agent's performance on the original domain after being trained or exposed to a shifted domain. This metric helps assess the extent of negative transfer, ensuring that adaptation to the new domain does not excessively degrade prior knowledge. A higher forgetting rate indicates a more significant loss of previously learned knowledge due to domain shift. 
This KPI contributes to evaluating Reliability of the AI-based assistant when dealing with real-world conditions which may be slightly different from source domain, as part of Task 4.2 evaluation objectives, and O4 main project objective. ",3e8548be-a4e5-4574-b46e-1ef6b75538d2,Scenario score (raw values)
135,KPI-RS-091,Reflection on operator trust ,Task 4.3,Long-term consequences of AI assistants,O3,Railway,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of the changes in their trust for the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Human Agency and Oversight”, “Credibility and Intimacy”. Furthermore, it is also relevant to the overall project KPI-ET-7 ""% of acceptance of human operators regarding AI4REALNET solutions"". ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7).  ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,56,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Human Agency and Oversight”, “Credibility and Intimacy”. Furthermore, it is also relevant to the overall project KPI-ET-7 ""% of acceptance of human operators regarding AI4REALNET solutions"". ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-091,KPI-RS-091: Reflection on operator trust  (Railway),This KPI represents self-reported human operators’ perception of the changes in their trust for the AI assistant over time (increased/decreased) on a Likert scale. ,1e2a5f02-8623-4ae6-999b-6eaf233d40c8,Test score (SUM of scenario scores),SUM,6a33cdfb-87b7-4299-8c7e-aa3525da76d7,Scenario 1 - This KPI represents self-reported human operators’ perception of the changes in their trust for the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Human Agency and Oversight”, “Credibility and Intimacy”. Furthermore, it is also relevant to the overall project KPI-ET-7 ""% of acceptance of human operators regarding AI4REALNET solutions"". ",73903939-7af2-4ca2-929c-173b40cf9213,Scenario score (raw values)
136,KPI-RS-091,Reflection on operator trust ,Task 4.3,Long-term consequences of AI assistants,O3,Power Grid,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of the changes in their trust for the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Human Agency and Oversight”, “Credibility and Intimacy”. Furthermore, it is also relevant to the overall project KPI-ET-7 ""% of acceptance of human operators regarding AI4REALNET solutions"". ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7).  ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,56,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Human Agency and Oversight”, “Credibility and Intimacy”. Furthermore, it is also relevant to the overall project KPI-ET-7 ""% of acceptance of human operators regarding AI4REALNET solutions"". ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-091,KPI-RS-091: Reflection on operator trust  (Power Grid),This KPI represents self-reported human operators’ perception of the changes in their trust for the AI assistant over time (increased/decreased) on a Likert scale. ,1e2a5f02-8623-4ae6-999b-6eaf233d40c8,Test score (SUM of scenario scores),SUM,e66c3653-fffa-45a8-a00c-80f7397e0c0a,Scenario 1 - This KPI represents self-reported human operators’ perception of the changes in their trust for the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Human Agency and Oversight”, “Credibility and Intimacy”. Furthermore, it is also relevant to the overall project KPI-ET-7 ""% of acceptance of human operators regarding AI4REALNET solutions"". ",ad72fdae-4df6-435e-bf5b-516a2af0e33e,Scenario score (raw values)
137,KPI-RS-091,Reflection on operator trust ,Task 4.3,Long-term consequences of AI assistants,O3,ATM,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of the changes in their trust for the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Human Agency and Oversight”, “Credibility and Intimacy”. Furthermore, it is also relevant to the overall project KPI-ET-7 ""% of acceptance of human operators regarding AI4REALNET solutions"". ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7).  ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,56,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Human Agency and Oversight”, “Credibility and Intimacy”. Furthermore, it is also relevant to the overall project KPI-ET-7 ""% of acceptance of human operators regarding AI4REALNET solutions"". ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-091,KPI-RS-091: Reflection on operator trust  (ATM),This KPI represents self-reported human operators’ perception of the changes in their trust for the AI assistant over time (increased/decreased) on a Likert scale. ,1e2a5f02-8623-4ae6-999b-6eaf233d40c8,Test score (SUM of scenario scores),SUM,c81cda9c-d9e2-4f25-9f65-17ffc6fec529,Scenario 1 - This KPI represents self-reported human operators’ perception of the changes in their trust for the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Human Agency and Oversight”, “Credibility and Intimacy”. Furthermore, it is also relevant to the overall project KPI-ET-7 ""% of acceptance of human operators regarding AI4REALNET solutions"". ",93697829-6f07-4970-8035-f44aa76af4f3,Scenario score (raw values)
138,KPI-RS-092,Reflection on operator agency ,Task 4.3,Long-term consequences of AI assistants,O3,Railway,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of the changes in their agency working with the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Decision support for the human operator”, “Human Agency and Oversight”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,57,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Decision support for the human operator”, “Human Agency and Oversight”.  ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-092,KPI-RS-092: Reflection on operator agency  (Railway),This KPI represents self-reported human operators’ perception of the changes in their agency working with the AI assistant over time (increased/decreased) on a Likert scale. ,980f4f20-9d4a-4a79-9d28-60922f9b825c,Test score (SUM of scenario scores),SUM,6766c2ed-923b-490c-9f88-14199e917699,Scenario 1 - This KPI represents self-reported human operators’ perception of the changes in their agency working with the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Decision support for the human operator”, “Human Agency and Oversight”.  ",f852993a-247c-4166-9f79-eaaed73e8c02,Scenario score (raw values)
139,KPI-RS-092,Reflection on operator agency ,Task 4.3,Long-term consequences of AI assistants,O3,Power Grid,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of the changes in their agency working with the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Decision support for the human operator”, “Human Agency and Oversight”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,57,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Decision support for the human operator”, “Human Agency and Oversight”.  ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-092,KPI-RS-092: Reflection on operator agency  (Power Grid),This KPI represents self-reported human operators’ perception of the changes in their agency working with the AI assistant over time (increased/decreased) on a Likert scale. ,980f4f20-9d4a-4a79-9d28-60922f9b825c,Test score (SUM of scenario scores),SUM,a8505b82-7606-42b5-a1be-8332890c0ba0,Scenario 1 - This KPI represents self-reported human operators’ perception of the changes in their agency working with the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Decision support for the human operator”, “Human Agency and Oversight”.  ",fc271b00-35e5-450a-8745-fdf709afd4cc,Scenario score (raw values)
140,KPI-RS-092,Reflection on operator agency ,Task 4.3,Long-term consequences of AI assistants,O3,ATM,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of the changes in their agency working with the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Decision support for the human operator”, “Human Agency and Oversight”.  ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,57,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Decision support for the human operator”, “Human Agency and Oversight”.  ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-092,KPI-RS-092: Reflection on operator agency  (ATM),This KPI represents self-reported human operators’ perception of the changes in their agency working with the AI assistant over time (increased/decreased) on a Likert scale. ,980f4f20-9d4a-4a79-9d28-60922f9b825c,Test score (SUM of scenario scores),SUM,4c6f5a14-9c3e-407a-863a-c6ed3e940551,Scenario 1 - This KPI represents self-reported human operators’ perception of the changes in their agency working with the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective.  
It is also relevant to protocols and concepts defined in D1.1 such as “Transparency”, “Decision support for the human operator”, “Human Agency and Oversight”.  ",c953aac9-b28b-4f09-8c47-217c3488866f,Scenario score (raw values)
141,KPI-RS-093,Reflection on operator de-skilling ,Task 4.3,Long-term consequences of AI assistants,O3,Railway,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of the changes in their own skills working with the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate de-skilling in the human operators”. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,58,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate de-skilling in the human operators”. ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-093,KPI-RS-093: Reflection on operator de-skilling  (Railway),This KPI represents self-reported human operators’ perception of the changes in their own skills working with the AI assistant over time (increased/decreased) on a Likert scale. ,21e591e8-c2ba-4244-b631-2d9ca3f8da0e,Test score (SUM of scenario scores),SUM,0ad95d5a-2af0-4c87-bc4b-a13ab28260ab,Scenario 1 - This KPI represents self-reported human operators’ perception of the changes in their own skills working with the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate de-skilling in the human operators”. ",60795455-1ce1-4267-83ab-0b869e5ad6a6,Scenario score (raw values)
142,KPI-RS-093,Reflection on operator de-skilling ,Task 4.3,Long-term consequences of AI assistants,O3,Power Grid,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of the changes in their own skills working with the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate de-skilling in the human operators”. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,58,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate de-skilling in the human operators”. ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-093,KPI-RS-093: Reflection on operator de-skilling  (Power Grid),This KPI represents self-reported human operators’ perception of the changes in their own skills working with the AI assistant over time (increased/decreased) on a Likert scale. ,21e591e8-c2ba-4244-b631-2d9ca3f8da0e,Test score (SUM of scenario scores),SUM,4e162c0c-a29c-4823-a8ba-9d4fe796a0fe,Scenario 1 - This KPI represents self-reported human operators’ perception of the changes in their own skills working with the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate de-skilling in the human operators”. ",4057e67e-d917-4cf7-aa97-779ed59b1c89,Scenario score (raw values)
143,KPI-RS-093,Reflection on operator de-skilling ,Task 4.3,Long-term consequences of AI assistants,O3,ATM,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of the changes in their own skills working with the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate de-skilling in the human operators”. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,58,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate de-skilling in the human operators”. ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-093,KPI-RS-093: Reflection on operator de-skilling  (ATM),This KPI represents self-reported human operators’ perception of the changes in their own skills working with the AI assistant over time (increased/decreased) on a Likert scale. ,21e591e8-c2ba-4244-b631-2d9ca3f8da0e,Test score (SUM of scenario scores),SUM,8b8fae63-2977-46a5-b929-a22f4b1574b2,Scenario 1 - This KPI represents self-reported human operators’ perception of the changes in their own skills working with the AI assistant over time (increased/decreased) on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate de-skilling in the human operators”. ",3a287f9f-a211-4ba3-a483-e51ca7451407,Scenario score (raw values)
144,KPI-RS-094,Reflection on over-reliance ,Task 4.3,Long-term consequences of AI assistants,O3,Railway,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of their potential over-reliance on the AI assistant on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate addictive behavior from humans”. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7).  ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,59,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate addictive behavior from humans”. ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-094,KPI-RS-094: Reflection on over-reliance  (Railway),This KPI represents self-reported human operators’ perception of their potential over-reliance on the AI assistant on a Likert scale. ,09cf843e-54b1-4c5e-900e-cd6706295192,Test score (SUM of scenario scores),SUM,93ac655d-c8d2-4e80-93f7-bc7bacf95069,Scenario 1 - This KPI represents self-reported human operators’ perception of their potential over-reliance on the AI assistant on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate addictive behavior from humans”. ",b173df9a-2b44-4230-9f5a-e8225d79ed26,Scenario score (raw values)
145,KPI-RS-094,Reflection on over-reliance ,Task 4.3,Long-term consequences of AI assistants,O3,Power Grid,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of their potential over-reliance on the AI assistant on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate addictive behavior from humans”. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7).  ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,59,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate addictive behavior from humans”. ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-094,KPI-RS-094: Reflection on over-reliance  (Power Grid),This KPI represents self-reported human operators’ perception of their potential over-reliance on the AI assistant on a Likert scale. ,09cf843e-54b1-4c5e-900e-cd6706295192,Test score (SUM of scenario scores),SUM,5705b846-25eb-48af-bc84-b355fcf29bdd,Scenario 1 - This KPI represents self-reported human operators’ perception of their potential over-reliance on the AI assistant on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate addictive behavior from humans”. ",da157855-f4e8-475b-ab78-f1db0b7732a7,Scenario score (raw values)
146,KPI-RS-094,Reflection on over-reliance ,Task 4.3,Long-term consequences of AI assistants,O3,ATM,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of their potential over-reliance on the AI assistant on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate addictive behavior from humans”. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7).  ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,59,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate addictive behavior from humans”. ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-094,KPI-RS-094: Reflection on over-reliance  (ATM),This KPI represents self-reported human operators’ perception of their potential over-reliance on the AI assistant on a Likert scale. ,09cf843e-54b1-4c5e-900e-cd6706295192,Test score (SUM of scenario scores),SUM,97bc68f9-4781-4240-8baf-752aa16763ba,Scenario 1 - This KPI represents self-reported human operators’ perception of their potential over-reliance on the AI assistant on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Mitigate addictive behavior from humans”. ",fc51fbb8-e8f2-40f0-a26a-76e7e455fd97,Scenario score (raw values)
147,KPI-RS-095,Reflection on additional training ,Task 4.3,Long-term consequences of AI assistants,O3,Railway,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of the additional training necessary to adopt the AI assistant on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Additional training about AI for human operators” and “Societal and Environmental Well-being”. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,60,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Additional training about AI for human operators” and “Societal and Environmental Well-being”. ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-095,KPI-RS-095: Reflection on additional training  (Railway),This KPI represents self-reported human operators’ perception of the additional training necessary to adopt the AI assistant on a Likert scale. ,9136484b-de63-46d1-b471-8c7c20806de0,Test score (SUM of scenario scores),SUM,6f82f84a-4485-4ac7-81bb-de8d9327258d,Scenario 1 - This KPI represents self-reported human operators’ perception of the additional training necessary to adopt the AI assistant on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Additional training about AI for human operators” and “Societal and Environmental Well-being”. ",eb064367-c43f-46e6-8ab4-d1c6166d8698,Scenario score (raw values)
148,KPI-RS-095,Reflection on additional training ,Task 4.3,Long-term consequences of AI assistants,O3,Power Grid,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of the additional training necessary to adopt the AI assistant on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Additional training about AI for human operators” and “Societal and Environmental Well-being”. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,60,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Additional training about AI for human operators” and “Societal and Environmental Well-being”. ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-095,KPI-RS-095: Reflection on additional training  (Power Grid),This KPI represents self-reported human operators’ perception of the additional training necessary to adopt the AI assistant on a Likert scale. ,9136484b-de63-46d1-b471-8c7c20806de0,Test score (SUM of scenario scores),SUM,7df79b82-3024-4d39-a90d-9a9db4873215,Scenario 1 - This KPI represents self-reported human operators’ perception of the additional training necessary to adopt the AI assistant on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Additional training about AI for human operators” and “Societal and Environmental Well-being”. ",5dd5321a-cd0f-4627-b755-224f5ab0aeb5,Scenario score (raw values)
149,KPI-RS-095,Reflection on additional training ,Task 4.3,Long-term consequences of AI assistants,O3,ATM,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of the additional training necessary to adopt the AI assistant on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Additional training about AI for human operators” and “Societal and Environmental Well-being”. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,60,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Additional training about AI for human operators” and “Societal and Environmental Well-being”. ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-095,KPI-RS-095: Reflection on additional training  (ATM),This KPI represents self-reported human operators’ perception of the additional training necessary to adopt the AI assistant on a Likert scale. ,9136484b-de63-46d1-b471-8c7c20806de0,Test score (SUM of scenario scores),SUM,44e686c1-6f5a-455d-9260-10888ef3a087,Scenario 1 - This KPI represents self-reported human operators’ perception of the additional training necessary to adopt the AI assistant on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Additional training about AI for human operators” and “Societal and Environmental Well-being”. ",a4376d6e-a7a2-4e23-82bc-c4ca787c7c81,Scenario score (raw values)
150,KPI-RS-096,Reflection on biases ,Task 4.3,Long-term consequences of AI assistants,O3,Railway,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of biased decisions potentially produced by the AI assistant with respect to gender/ethnicity/age or commercial interests on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Diversity, Non-discrimination, and Fairness”. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,61,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Diversity, Non-discrimination, and Fairness”. ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-096,KPI-RS-096: Reflection on biases  (Railway),This KPI represents self-reported human operators’ perception of biased decisions potentially produced by the AI assistant with respect to gender/ethnicity/age or commercial interests on a Likert scale. ,a3de8c92-00c3-4971-bdb2-667c8c80285c,Test score (SUM of scenario scores),SUM,6570ddc4-488b-4cce-b67b-375ddedf377c,Scenario 1 - This KPI represents self-reported human operators’ perception of biased decisions potentially produced by the AI assistant with respect to gender/ethnicity/age or commercial interests on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Diversity, Non-discrimination, and Fairness”. ",e394e404-8c49-4b56-bedf-59ec2685292f,Scenario score (raw values)
151,KPI-RS-096,Reflection on biases ,Task 4.3,Long-term consequences of AI assistants,O3,Power Grid,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of biased decisions potentially produced by the AI assistant with respect to gender/ethnicity/age or commercial interests on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Diversity, Non-discrimination, and Fairness”. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,61,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Diversity, Non-discrimination, and Fairness”. ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-096,KPI-RS-096: Reflection on biases  (Power Grid),This KPI represents self-reported human operators’ perception of biased decisions potentially produced by the AI assistant with respect to gender/ethnicity/age or commercial interests on a Likert scale. ,a3de8c92-00c3-4971-bdb2-667c8c80285c,Test score (SUM of scenario scores),SUM,b1053c40-6032-4d52-9c8f-4879eb134a1f,Scenario 1 - This KPI represents self-reported human operators’ perception of biased decisions potentially produced by the AI assistant with respect to gender/ethnicity/age or commercial interests on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Diversity, Non-discrimination, and Fairness”. ",99ba51e6-c174-4f4e-9994-78deb416b533,Scenario score (raw values)
152,KPI-RS-096,Reflection on biases ,Task 4.3,Long-term consequences of AI assistants,O3,ATM,['(non applicable)'],special evaluation setup,This KPI represents self-reported human operators’ perception of biased decisions potentially produced by the AI assistant with respect to gender/ethnicity/age or commercial interests on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Diversity, Non-discrimination, and Fairness”. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,61,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Diversity, Non-discrimination, and Fairness”. ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-RS-096,KPI-RS-096: Reflection on biases  (ATM),This KPI represents self-reported human operators’ perception of biased decisions potentially produced by the AI assistant with respect to gender/ethnicity/age or commercial interests on a Likert scale. ,a3de8c92-00c3-4971-bdb2-667c8c80285c,Test score (SUM of scenario scores),SUM,32dccf39-be88-475b-9e8e-67bf5131dced,Scenario 1 - This KPI represents self-reported human operators’ perception of biased decisions potentially produced by the AI assistant with respect to gender/ethnicity/age or commercial interests on a Likert scale. ,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Diversity, Non-discrimination, and Fairness”. ",b616ec40-6dba-4da8-a2f1-c1154287b8da,Scenario score (raw values)
153,KPI-PS-097,Predicted long-term adoption ,Task 4.3,Long-term consequences of AI assistants,O3,Railway,['(non applicable)'],special evaluation setup,"This KPI represents predicted adoption of the AI assistant by users, stakeholders, or experts on a Likert scale. ","This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Human Agency and Oversight”, “Societal and Environmental Well-being”. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,62,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Human Agency and Oversight”, “Societal and Environmental Well-being”. ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-PS-097,KPI-PS-097: Predicted long-term adoption  (Railway),"This KPI represents predicted adoption of the AI assistant by users, stakeholders, or experts on a Likert scale. ",61bf594f-944f-4bb9-8bb4-a5d083e75414,Test score (SUM of scenario scores),SUM,b72897a4-f7c2-4bcb-8b29-f2a13e8bc442,"Scenario 1 - This KPI represents predicted adoption of the AI assistant by users, stakeholders, or experts on a Likert scale. ","This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Human Agency and Oversight”, “Societal and Environmental Well-being”. ",180b2388-281e-45cb-9510-11edfd5f2e45,Scenario score (raw values)
154,KPI-PS-097,Predicted long-term adoption ,Task 4.3,Long-term consequences of AI assistants,O3,Power Grid,['(non applicable)'],special evaluation setup,"This KPI represents predicted adoption of the AI assistant by users, stakeholders, or experts on a Likert scale. ","This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Human Agency and Oversight”, “Societal and Environmental Well-being”. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,62,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Human Agency and Oversight”, “Societal and Environmental Well-being”. ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-PS-097,KPI-PS-097: Predicted long-term adoption  (Power Grid),"This KPI represents predicted adoption of the AI assistant by users, stakeholders, or experts on a Likert scale. ",61bf594f-944f-4bb9-8bb4-a5d083e75414,Test score (SUM of scenario scores),SUM,4116f6dd-7be0-46f9-9d03-f748352a176d,"Scenario 1 - This KPI represents predicted adoption of the AI assistant by users, stakeholders, or experts on a Likert scale. ","This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Human Agency and Oversight”, “Societal and Environmental Well-being”. ",b9e3086f-6b75-46c2-9dec-1e8ef5d3cf72,Scenario score (raw values)
155,KPI-PS-097,Predicted long-term adoption ,Task 4.3,Long-term consequences of AI assistants,O3,ATM,['(non applicable)'],special evaluation setup,"This KPI represents predicted adoption of the AI assistant by users, stakeholders, or experts on a Likert scale. ","This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Human Agency and Oversight”, “Societal and Environmental Well-being”. ",As operationalized by the questionnaire (normally Likert-scales with several items that are rated on a scale of e.g. 1–5 or 1–7). ,Ordinal data response on a Likert scale (or potentially a similar response on an interval scale) ,62,"['Railway', 'Power Grid', 'ATM']",0ca46887-897a-463f-bf83-c6cd6269a976,Beta Validation Campaign,The beta validation campaign runs until 30.11.2025,70862817-0593-4adc-8ba8-34db7ddbb1f0,Long-term consequences of AI assistants,"This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Human Agency and Oversight”, “Societal and Environmental Well-being”. ",42e637d9-8655-4230-837f-c13d95f4c21b,Benchmark score (SUM of test scores),SUM,KPI-PS-097,KPI-PS-097: Predicted long-term adoption  (ATM),"This KPI represents predicted adoption of the AI assistant by users, stakeholders, or experts on a Likert scale. ",61bf594f-944f-4bb9-8bb4-a5d083e75414,Test score (SUM of scenario scores),SUM,61e33f9f-75aa-4c82-b217-b4835256887d,"Scenario 1 - This KPI represents predicted adoption of the AI assistant by users, stakeholders, or experts on a Likert scale. ","This KPI contributes to evaluating Long-term consequences of AI assistants of the AI-based assistant, as part of Task 4.3 evaluation objectives, and O3 main project objective. 
It is also relevant to protocols and concepts defined in D1.1 such as “Human Agency and Oversight”, “Societal and Environmental Well-being”. ",963a078d-f2b1-4f22-a514-ef1dba2aac05,Scenario score (raw values)
